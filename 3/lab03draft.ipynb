{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lab 2: Bayesian PCA\n",
      "\n",
      "### Machine Learning: Principles and Methods, November 2013\n",
      "\n",
      "* The lab exercises should be made in groups of three people, or at least two people.\n",
      "* The deadline is Wednesday, 11 December, 23:59.\n",
      "* Assignment should be sent to T.S.Cohen at uva dot nl (Taco Cohen). The subject line of your email should be \"[MLPM2013] lab#_lastname1\\_lastname2\\_lastname3\". \n",
      "* Put your and your teammates' names in the body of the email\n",
      "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"[MLPM2013] lab01\\_Kingma\\_Hu\", the attached file should be \"lab01\\_Kingma\\_Hu.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
      "\n",
      "Notes on implementation:\n",
      "\n",
      "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact us.\n",
      "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
      "* NOTE: test your code and make sure we can run your notebook / scripts!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction\n",
      "\n",
      "In this lab assignment, we will implement a variational algorithm for Bayesian PCA. Unlike regular PCA based on maximization of retained variance or minimization of projection error (see Bishop, 12.1.1 and 12.1.2), probabilistic PCA defines a proper density model over observed and latent variables. We will work with a fully Bayesian model this time, which is to say that we will put priors on our parameters and will be interested in learning the posterior over those parameters. Bayesian methods are very elegant, but require a shift in mindset: we are no longer looking for a point estimate of the parameters (as in maximum likelihood or MAP), but for a full posterior distribution.\n",
      "\n",
      "The integrals involved in a Bayesian analysis are usually analytically intractable, so that we must resort to approximations. In this lab assignment, we will implement the variational method described in Bishop99. Chapters 10 and 12 of the PRML book contain additional material that may be useful when doing this exercise.\n",
      "\n",
      "* [Bishop99] Variational Principal Components, C. Bishop, ICANN 1999 - http://research.microsoft.com/pubs/67241/bishop-vpca-icann-99.pdf\n",
      "\n",
      "Below, you will find some code to get you started."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import scipy.special as sp\n",
      "\n",
      "class BayesianPCA(object):\n",
      "    \n",
      "    def __init__(self, d, N, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        self.d = d # number of dimensions\n",
      "        self.N = N # number of data points\n",
      "        \n",
      "        # Hyperparameters\n",
      "        self.a_alpha = a_alpha\n",
      "        self.b_alpha = b_alpha\n",
      "        self.a_tau = a_tau\n",
      "        self.b_tau = b_tau\n",
      "        self.beta = beta\n",
      "\n",
      "        # Variational parameters\n",
      "        self.means_z = np.random.randn(d, N) # called x in bishop99\n",
      "        self.sigma_z = np.random.randn(d, d)\n",
      "        self.mean_mu = np.random.randn(d, 1)\n",
      "        self.sigma_mu = np.random.randn(d, d)\n",
      "        self.means_w = np.random.randn(d, d)\n",
      "        self.sigma_w = np.random.randn(d, d)\n",
      "        self.a_alpha_tilde = self.a_alpha + d/2.0\n",
      "        self.bs_alpha_tilde = np.abs(np.random.randn(d, 1))\n",
      "        self.a_tau_tilde = self.a_tau + (N*d)/2.0\n",
      "        self.b_tau_tilde = np.abs(np.random.randn(1))\n",
      "        \n",
      "        #self.means_z = np.zeros((d, N)) + 1 # called x in bishop99\n",
      "        #self.sigma_z = np.zeros((d, d)) + 1\n",
      "        #self.mean_mu = np.zeros((d, 1)) + 1\n",
      "        #self.sigma_mu = np.zeros((d, d)) + 1\n",
      "        #self.means_w = np.zeros((d, d)) + 1\n",
      "        #self.sigma_w = np.zeros((d, d)) + 1\n",
      "        #self.a_alpha_tilde = self.a_alpha + d/2.0\n",
      "        #self.bs_alpha_tilde = np.abs(np.zeros((d, 1))) + 1\n",
      "        #self.a_tau_tilde = self.a_tau + (N*d)/2.0 \n",
      "        #self.b_tau_tilde = np.abs(np.zeros(1)) + 1\n",
      "    \n",
      "    \n",
      "    def update_z(self, X):\n",
      "        #update sigma of z\n",
      "        exp_tau = self.a_tau_tilde / float(self.b_tau_tilde)\n",
      "        exp_WtW = np.zeros((d,d))\n",
      "        for i in range(d):\n",
      "            exp_WtW[:,i] = np.sum(self.sigma_w,1) + np.dot(self.means_w, self.means_w[:,i])\n",
      "        self.sigma_z = numpy.linalg.inv(numpy.identity(d)+np.multiply(exp_tau, exp_WtW))\n",
      "        \n",
      "        \n",
      "        #print 'sigma z:\\n', self.sigma_z\n",
      "        \n",
      "        #Update means of z        \n",
      "        exp_Wt = np.transpose(self.means_w)\n",
      "        exp_mu = self.mean_mu\n",
      "        \n",
      "        #print 'shape exp Wt', exp_Wt.shape, 'shape exp_tau', exp_tau.shape\n",
      "        #print 'shape sigma z', self.sigma_z.shape, 'shape exp mu', exp_mu.shape\n",
      "        \n",
      "        self.means_z = np.dot( np.dot( np.multiply(exp_tau, self.sigma_z), exp_Wt), (X-exp_mu) )\n",
      "                \n",
      "        #print 'means z:\\n', self.means_z\n",
      "    \n",
      "    def update_mu(self, X):\n",
      "        # Update sigma of mu        \n",
      "        exp_tau = self.a_tau_tilde / float(self.b_tau_tilde)\n",
      "        self.sigma_mu = np.multiply(1/float(self.beta+N*exp_tau), numpy.identity(d)) \n",
      "        \n",
      "        #print 'sigma mu:\\n', self.sigma_mu\n",
      "        \n",
      "        # Update means of mu        \n",
      "        exp_W = self.means_w\n",
      "        exp_z = self.means_z\n",
      "        temp = numpy.sum(X-np.dot(exp_W, exp_z), 1)\n",
      "        self.mean_mu[:,0] = np.dot(np.multiply(exp_tau, self.sigma_mu), temp)\n",
      "        \n",
      "        #print 'mean mu:\\n', self.mean_mu\n",
      "            \n",
      "    def update_w(self, X):\n",
      "        # Update sigma of W\n",
      "        exp_tau = self.a_tau_tilde / float(self.b_tau_tilde)\n",
      "        sum_exp_zzt = np.zeros((d,d))\n",
      "        for i in range(d):\n",
      "            exp_znznt = np.dot(self.means_z[i,:], np.transpose(self.means_z[i,:]))\n",
      "            sum_exp_zzt = np.add(sum_exp_zzt, exp_znznt)\n",
      "        exp_alpha = np.multiply(1/float(self.a_alpha_tilde), self.bs_alpha_tilde)\n",
      "        exp_alpha = np.array(exp_alpha.conj().T).squeeze()        \n",
      "        self.sigma_w = np.add(np.diag(exp_alpha), np.multiply(exp_tau, sum_exp_zzt))\n",
      "        \n",
      "        #print 'sigma w:\\n', self.sigma_w#\n",
      "        \n",
      "        # Update mu of W\n",
      "        # w(k) is a column\n",
      "        for i in range(d):\n",
      "            temp = X[i,:]-self.mean_mu[i] \n",
      "            repmat = np.tile(temp, (d,1))\n",
      "            summation1 = np.sum(np.multiply(self.means_z, repmat),1)\n",
      "            summation = np.tile(summation1, (1,1)).T\n",
      "            if i == 0:\n",
      "                print 'temp:\\n', temp\n",
      "                print 'summation:\\n', summation1\n",
      "            self.means_w[:,i] = (exp_tau*np.dot(self.sigma_w, summation))[:,0]\n",
      "        \n",
      "        #print 'means w:\\n', self.means_w\n",
      "        \n",
      "    def update_alpha(self):\n",
      "        # a_alpha_tilde is a constant: a_alpha + d/2 so set it in the initialization and do not change anymore\n",
      "        \n",
      "        b_temp = np.diag((np.trace(self.sigma_w)+np.dot(self.means_w.T, self.means_w))/2.0) + self.b_alpha\n",
      "        self.bs_alpha_tilde = np.tile(b_temp, (1,1)).T\n",
      "        \n",
      "        #print 'a alpha tilde and bs alpha tilde:\\n', self.a_alpha_tilde, '\\n', self.bs_alpha_tilde\n",
      "        \n",
      "        #for i in range(d):\n",
      "        #    print i\n",
      "        #    temp = (np.trace(self.sigma_w)+np.dot(self.means_w[:,i].T, self.means_w[:,i]))/2.0\n",
      "        #    self.bs_alpha_tilde[i] = self.b_alpha + temp\n",
      "           \n",
      "        \n",
      "    def update_tau(self, X):\n",
      "        # a_tau is a constant so set in the initialization\n",
      "        \n",
      "        exp_WtW = np.zeros((d,d))\n",
      "        for i in range(d):\n",
      "            exp_WtW[:,i] = np.sum(self.sigma_w,1) + np.dot(self.means_w, self.means_w[:,i])\n",
      "        exp_mutmu = np.dot(self.mean_mu.T, self.mean_mu)        \n",
      "        sum = 0.0\n",
      "        \n",
      "        for i in range(N):\n",
      "            x_col = X[:,i]\n",
      "            sum += np.dot(x_col.T, x_col) + exp_mutmu + np.trace(np.dot(exp_WtW, np.dot(x_col, x_col.T)))\n",
      "            sum += 2.0 * np.dot(np.dot(self.mean_mu.T, self.means_w), self.means_z[:,i])\n",
      "            sum -= 2.0 * np.dot(np.dot(x_col.T, self.means_w),self.means_z[:,i])\n",
      "            sum -= 2.0 * np.dot(x_col.T, self.mean_mu)\n",
      "                \n",
      "        self.b_tau_tilde = self.b_tau + 0.5* sum\n",
      "        \n",
      "        #print 'a tau tilde and b tau tilde:\\n', self.a_tau_tilde, '\\n', self.b_tau_tilde\n",
      "        \n",
      "    def L(self, X):\n",
      "        L = 0.0\n",
      "        return L\n",
      "    \n",
      "    def fit(self, X, max_it):\n",
      "        for i in range(max_it):\n",
      "            if i%10 == 0:\n",
      "                print i,\n",
      "        \n",
      "            self.update_alpha()\n",
      "            self.update_tau(X)\n",
      "            self.update_w(X)\n",
      "            self.update_z(X)\n",
      "            self.update_mu(X)\n",
      "            \n",
      "            print '\\n w at',i, ':\\n', self.means_w\n",
      "            \n",
      "            #if i%200==0:\n",
      "            #    print '\\n w at',i, ':\\n', self.means_w\n",
      "                \n",
      "        print '\\n w final:\\n', self.means_w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = 6\n",
      "N = 100\n",
      "\n",
      "pca = BayesianPCA(d, N)\n",
      "X = np.random.randn(d, N)\n",
      "for i in range(1):\n",
      "    print '\\n\\n', i    \n",
      "    pca.update_alpha()\n",
      "    pca.update_tau(X)\n",
      "    pca.update_w(X)\n",
      "    pca.update_z(X)\n",
      "    pca.update_mu(X)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "0\n",
        "temp:\n",
        "[ 1.11576048  1.50452863  1.68487278  0.46222112  1.22590641  0.38153278\n",
        " -0.07480945  0.44800124 -0.38754798 -0.11770637 -0.98941156  0.52717622\n",
        " -1.26450459 -1.43584487 -1.45164099 -0.13284561  0.89778903 -0.26702482\n",
        "  0.77986256  0.35575377 -0.12632195 -0.85535357 -1.36324086  1.39078555\n",
        " -0.95127739 -0.26611376  1.01866051 -0.62233276  1.36846154 -0.43462194\n",
        " -0.28307908  0.74707338  1.60250193  0.98183146 -0.13283623  0.97605641\n",
        "  1.07255588  0.42835248  0.44401836  1.04393324  0.24305546  2.20478501\n",
        " -0.16513772 -0.13279313 -0.05323832  2.1365562   1.20669213  1.08748673\n",
        "  0.42410122  1.21104761  0.30515753  0.85293158 -0.87506514  1.44017462\n",
        "  0.25454446  1.30855452 -0.8020798   0.62814312  0.31810117  0.40209659\n",
        " -1.20120761 -0.45583286  0.51760196 -1.07267155  0.37443524  0.2735714\n",
        "  0.79152145  1.25442961  0.3894146   2.08367371 -0.74831003 -2.37703898\n",
        " -0.64517708 -1.28343124  1.641439   -1.20032134 -0.72489931 -1.37795114\n",
        "  0.47696954  0.57560551 -0.19350091 -0.50913592 -1.14553754 -1.75707785\n",
        "  0.65759088 -0.37062599 -0.65781812  0.95469579 -1.25612517  1.01461561\n",
        "  0.44553823  2.13615176 -0.24336123  0.28373237  0.80961399 -0.28072912\n",
        "  2.98022192 -0.40829457 -1.56316063  0.61807821]\n",
        "summation:\n",
        "[-13.80018135  -1.06209243  -4.08531696   1.99584045  -0.58043816\n",
        "   9.68223642]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import display, Math, Latex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1. The Q-distribution (5 points)\n",
      "\n",
      "In variational Bayes, we introduce a distribution $Q(\\Theta)$ over parameters / latent variables in order to make inference tractable. We can think of $Q$ as being an approximation of a certain distribution. What function does $Q$ approximate, $p(D|\\Theta)$, $p(\\Theta|D)$, $p(D, \\Theta)$, $p(\\Theta)$, or $p(D)$, and how do you see that?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function $Q$ approximates the function that is the posterior distribution, $p(\\Theta | D) $.\n",
      "\n",
      "The posterior $P(\\Theta | D)$ is given by multiplying the likelihood by the prior and then normalizing:\n",
      "\n",
      "\\begin{align}\n",
      "P(\\Theta | D) &= \\dfrac{P(D | \\Theta)P( \\Theta)}{P(D)} \\\\\n",
      "              &= \\dfrac{P(D | \\Theta)P( \\Theta)}{\\int P(D | \\Theta)P(\\Theta) \\hspace{1mm} d\\Theta}\n",
      "\\end{align}\n",
      "\n",
      "Why is this useful? \n",
      "For Maximum a Posteriori (MAP) learning, we could use the posterior to get a point estimate of the parameters:\n",
      "\n",
      "\\begin{align}\n",
      "\\widetilde{\\Theta}_{\\text{MAP}}(D)&= \\arg\\max_{\\Theta} P(\\Theta | D) \\\\\n",
      "            & \\propto {P(D | \\Theta)P( \\Theta)}\n",
      "\\end{align}\n",
      "\n",
      "So for MAP learning the normalization (and therefore integration) is not needed. Like Maximum likelihood estimation (MLE) learning, MAP learning learns a point estimate of our parameters.\n",
      "\n",
      "In the Bayesian framework no fixed parameter setting is chosen. The posterior is used to obtain a predictive density (also called the posterior predictive distribution) for a new data point $t^{*}$ with:\n",
      "\n",
      "\\begin{align}\n",
      "P(t^{*} | D) &= \\int P(t^{*} | \\Theta) P(\\Theta | D) \\hspace{2mm} d\\Theta\n",
      "\\end{align}\n",
      "\n",
      "This means that in the Bayesian learning framework we consider all possible parametrizations of our models by incorporating the posterior this way. Each of these is weighted by the evidence for them in $D$.\n",
      "(Note that when predicting, we also return a probability distribution as prediction for $t^{*}$. \n",
      "Retaining all possible parametrizations is in this case of Principal Component Analysis) particularly advantageous for automatically determining an adequate number of principal components given $D$.\n",
      "\n",
      "Note that we need to integrate over all possible parametrizations for this. In practice this can quickly become infeasible for models with many parameters; when using a mixture of models; or when the prior distribution is not of the same functional form as the likelihood distribution. Therefore we use a ''simpler'' distribution $Q(\\Theta)$ to approximate $P(\\Theta|D)$. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2. The mean-field approximation (15 points)\n",
      "\n",
      "Equation 13 from [Bishop99] is a very powerful result: assuming only that $Q(\\Theta)$ factorizes in a certain way (no assumptions on the functional form of the factors $Q_i$!), we get a set of coupled equations for the $Q_i$.\n",
      "\n",
      "However, the expression given in eq. 13 for Q_i contains a small mistake. Starting with the expression for the lower bound $\\mathcal{L}(Q)$, derive the correct expression (and include your derivation). You can proceed as follows: first, substitute the factorization of $Q$ (eq. 12) into the definition of $\\mathcal{L}(Q)$ and separate $\\mathcal{L}(Q)$ into $Q_i$-dependent and $Q_i$-independent terms. At this point, you should be able to spot the expectations $\\langle\\cdot\\rangle_{k \\neq i}$ over the other $Q$-distributions that appear in Bishop's solution (eq. 13). Now, keeping all $Q_k, k \\neq i$ fixed, maximize the expression with respect to $Q_i$. You should be able to spot the form of the optimal $ln Q_i$, from which $Q_i$ can easily be obtained."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is given that the lower bound is \n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q(\\Theta) \\log \\dfrac{P(D, \\Theta)}{Q(\\Theta)}\n",
      "                \\hspace{3mm} d \\Theta  \\\\\n",
      "\\end{align} \n",
      "$$\n",
      "\n",
      "And the factorization is\n",
      "\n",
      "\\begin{equation}\n",
      "Q(\\Theta)= \\prod_{k} Q_k(\\Theta_k)\n",
      "\\end{equation}\n",
      "\n",
      "Plugging in the factorization into the lower bound, we get\n",
      "\n",
      "\\begin{align}                \n",
      "\\mathcal{L}(Q) =&  \\int \n",
      "                     \\prod_{k} Q_k( \\Theta_k) \n",
      "                     \\left[\n",
      "                         \\log P(D,\\Theta) - \\log \\left( \\prod_k Q_k(\\Theta_k) \\right)\n",
      "                     \\right]\n",
      "                 \\hspace{3mm} d \\Theta  \\\\\n",
      "              =& \\int\n",
      "              \\prod_{k} Q_k( \\Theta_k) \n",
      "                     \\left[\n",
      "                         \\log P(D,\\Theta) - \\sum_k \\log Q_k(\\Theta_k)\n",
      "                     \\right]\n",
      "                  \\hspace{3mm} d \\Theta\n",
      "\\end{align}  \n",
      "\n",
      "By denoting a specific factor $Q_i$ separately we arrive at\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q_i( \\Theta_i) \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "                \\left[ \\log P(D, \\Theta) - \\log \\left( \\prod_{j \\neq i}  Q_j(\\Theta_j) \\right) - \\log Q_i (\\Theta_i) \\right] \n",
      "                \\hspace{3mm} d \\Theta  \n",
      "\\end{align}                \n",
      "\n",
      "By taking the integral of $Q_i$ apart from those for $\\{Q_1, \\dots ,Q_{i-1},Q_{i+1}, \\dots ,Q_M \\}$, we get\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "    \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "            \\log \\left( \\prod_{j \\neq i} Q_j(\\Theta_j) \\right)\n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{3mm} - \\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[ \n",
      "            \\int\n",
      "             \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Now, when we keep the $Q_k \\neq Q_{i}$ fixed the following happens, the integral over $\\Theta_{\\setminus i}$  in the third row of the equation for $\\mathcal{L}(Q)$ will become 1, because all $Q_n(\\Theta_n)$ are probability distributions. \n",
      "\n",
      "\\begin{align}\n",
      "\\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[ \n",
      "            \\int\n",
      "             \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "        =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[\n",
      "        \\prod_{k \\neq i}         \n",
      "            \\int\n",
      "              Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{k}\n",
      "        \\right]        \n",
      "        \\hspace{3mm} d \\Theta_{i} \\\\\n",
      "     =&\n",
      "      \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[\n",
      "        \\prod_{k \\neq i}         \n",
      "            1\n",
      "        \\right]        \n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "\\end{align}\n",
      "\n",
      "So we can rewrite the equation as\n",
      "\n",
      "\\begin{align}       \n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "    \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "            \\log \\left( \\prod_{j \\neq i} Q_j(\\Theta_j) \\right)\n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{3mm} - \\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}         \n",
      "\\end{align}\n",
      "\n",
      "\n",
      "Furthermore, when we keep the $Q_k \\neq Q_{i}$ fixed, the $\\Theta_{\\setminus i}$ integral in the second row of the equation for  $\\mathcal{L}(Q)$ will become a constant. We know that $\\int Q_i(\\Theta_i)  d\\Theta_{i}$ should be 1 because $Q_i$ is a probability distribution. So the whole second row can be viewed as a constant. \n",
      "\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{1mm} + \\text{const}        \n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the integral over $\\Theta_{\\setminus i}$  in the  first line in $\\mathcal{L}(Q)$ could be rewritten as an expectation :\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \\cdot\n",
      "        \\mathbb{E}_{[k \\neq i]}[ \\log P(D, \\Theta)]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{1mm} + \\text{const}        \n",
      "\\end{align}\n",
      "\n",
      "Where $\\mathbb{E}_{[k \\neq i]}$ denotes this expected value is with respect to the $Q$ distributions over all variables $\\Theta_k \\neq \\Theta_i$.\n",
      "\n",
      "\n",
      "We can define $\\mathbb{E}_{[k \\neq i]}[ \\log P(D, \\Theta)]$ as $\\log \\widetilde{P}(D, \\Theta_i)$.\n",
      "\n",
      "Intuitively it makes sense to see  $\\log \\widetilde{P}(D, \\Theta_i)$, the joint probability of that parameter and the data, as the expectation of $\\log P(D, \\Theta)$ under all $\\Theta_k$ that are not $\\Theta_i$ , i.e., to not marginalize out  $\\Theta_i$ in $\\log P(D, \\Theta)$.\n",
      "It is here particularly useful to be able to rewrite the above equation as:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =&  \\int Q_i(\\Theta_i) \\log \\widetilde{P}(D, \\Theta_i) -  \\int Q_i(\\Theta_i) \\log Q_i(\\Theta_i) \\\\\n",
      "                =& \\int Q_i(\\Theta_i) \\log \\dfrac{\\widetilde{P}(D , \\Theta_i)}{ Q_i(\\Theta_i)} \\\\\n",
      "                =& - \\text{KL} \\left(Q_i(\\Theta_i) || \\widetilde{P}(D, \\Theta_i) \\right)\n",
      "\\end{align}\n",
      "\n",
      "Recall we wanted to maximize the lower bound function $\\mathcal{L}(Q)$. We see that this negative KL-divergence will be maximal when \n",
      "\n",
      "\\begin{align}\n",
      " Q_i(\\Theta_i) &= \\widetilde{P}(D, \\Theta_i)  \\\\  \n",
      " \\log  Q_i(\\Theta_i) &= \\log \\widetilde{P}(D, \\Theta_i)  \\\\\n",
      "                     &= \\mathbb{E}_{[k \\neq i]}[\\log P(D, \\Theta)]\\\\\n",
      " Q_i(\\Theta_i)      &= \\exp \\left( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] \\right)    \n",
      "\\end{align}\n",
      "\n",
      "So in line with Bishop (except a subscript mismatch on his side), accounting for normalization (marginalizing over all $\\Theta_{\\setminus i}$) we get that the optimal value for $Q_i$ is \n",
      "\n",
      "\\begin{align}\n",
      " Q_i^{*}(\\Theta_i)      &= \\dfrac{\\exp ( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] )   }\n",
      "                      {\\int \\exp ( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] ) \\hspace{2mm} d\\Theta_i  }\n",
      "\\end{align}\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3. The log-probability (10 points)\n",
      "\n",
      "Write down the log-prob of data and parameters, $\\ln p(\\mathbf{X}, \\mathbf{Z}, \\mathbf{W}, \\mathbf{\\alpha}, \\tau, \\mathbf{\\mu})$, in full detail (where $\\mathbf{X}$ are observed, $\\mathbf{Z}$ is latent; this is different from [Bishop99] who uses $\\mathbf{T}$ and $\\mathbf{X}$ respectively, but $\\mathbf{X}$ and $\\mathbf{Z}$ are consistent with the PRML book and are more common nowadays). Could we use this to assess the convergence of the variational Bayesian PCA algorithm? If yes, how? If no, why not?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The log-probability of the data and parameters is defined as:\n",
      "$$\n",
      "\\begin{align}\n",
      "\\ln p ( X, Z, W, \\alpha, \\tau, \\mu) =& \n",
      "\\ln \\left(  \\prod_{n=1}^N p(x_n| z_n, W, \\mu, \\tau)p(Z)p(W|\\alpha)p(\\alpha)p(\\mu)p(\\tau) \\right) \\\\\n",
      "=& \\ln \\left( \\prod_{n=1}^N \\left[ \\left( \\frac{\\tau}{2 \\pi} \\right) ^{D/2} \\exp \\left( - \\frac{\\tau}{2} \\parallel x_t-Wz_n-\\mu \\parallel ^2  \\right) \\right]\n",
      "\\prod_{n=1}^N \\left[  \\left( \\frac{1}{2 \\pi} \\right)^{q/2} \\exp \\left( - \\frac{1}{2} z_n^\\top z_n \\right) \\right] \\\\\n",
      "\\prod_{i=1}^q \\left[ \\left( \\frac{\\alpha_i}{2 \\pi} \\right)^{D/2} \\exp \\left( -\\frac{\\alpha_i}{2}  w_i^\\top w_i \\right) \\right]\n",
      "\\prod_{i=1}^q \\left[ \\frac{1}{\\Gamma(a_\\alpha)} b_\\alpha^{a_\\alpha} \\alpha_i^{a_\\alpha-1} e^{-b_\\alpha \\: \\alpha_i} \\right]\\\\ \n",
      "\\left[ \\left(\\frac{ \\beta}{2\\pi} \\right)^{D/2} \\exp \\left( - \\frac{\\beta}{2} \\mu^\\top \\mu \\right) \\right]\n",
      "\\left[ \\frac{1}{\\Gamma(c_\\tau)} d_\\tau^{c_\\tau} \\tau^{c_\\tau-1} e^{-d_\\tau \\: \\tau} \\right] \\right) \n",
      "\\\\\n",
      "=& \\frac{ND}{2} \\ln \\frac{\\tau}{2 \\pi} + \\sum_{n=1}^N \\left[ - \\frac{\\tau}{2} \\parallel x_n-Wz_n-\\mu \\parallel ^2 \\right]\n",
      "+ \\frac{Nq}{2} \\ln \\frac{1}{2 \\pi} + \\sum_{n=1}^N \\left[ - \\frac{1}{2} z_n^\\top z_n \\right] \\\\\n",
      "&+ \\sum_{i=1}^q \\left[ \\frac{D}{2} \\ln \\frac{\\alpha_i}{2 \\pi} \\right] + \\sum_{i=1}^q \\left[ - \\frac{\\alpha_i}{2} w_i^\\top w_i \\right]\n",
      "+ \\: q \\ln \\frac{ b_\\alpha^{a_\\alpha}}{\\Gamma(a_\\alpha)} + \\sum_{i=1}^q \\left[ (a_\\alpha -1) \\ln \\alpha_i - b_\\alpha \\alpha_i \\right] \\\\\n",
      "&+ \\frac{D}{2} \\ln \\frac{\\beta}{2 \\pi} - \\frac{\\beta}{2} \\mu^\\top\\mu - \\ln \\Gamma(c_\\tau) + \\ln d_\\tau^{c_\\tau} + (c_\\tau-1)\\ln \\tau - d_\\tau \\tau\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bayesian learning results in a posterior distribution over the parameters. If we wish to use $\\ln P(\\Theta,D)$ to assess convergence we have no choice but to use some point estimate of the parameters from the approximated posterior $Q$ to evaluate  $\\ln P(\\Theta,D)$ (we cannot plug in the distribution). However, we consider convergence as $Q$ being as similar to the real posterior $P(\\Theta|D)$ as possible. There is no guarantee that if $Q$ becomes more similar to $P(\\Theta|D)$, that the used point estimate of $Q$ results in a higher value when evaluating $\\ln P(\\Theta,D)$. Therefore $\\ln P(\\Theta,D)$ is not a suitable option to assess convergence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4. The lower bound $\\mathcal{L}(Q)$ (25 points)\n",
      "\n",
      "Derive an expression for the lower bound $\\mathcal{L}(Q)$ of the log-prob $\\ln p(X)$ for Bayesian PCA, making use of the factorization (eq. 12) and the form of the Q-distributions (eq. 16-20) as listed in [Bishop99]. Show your steps. Implement this function.\n",
      "\n",
      "The following result may be useful:\n",
      "\n",
      "For $x \\sim \\Gamma(a,b)$, we have $\\langle \\ln x\\rangle = \\ln b + \\psi(a)$, where $\\psi(a) = \\frac{\\Gamma'(a)}{\\Gamma(a)}$ is the digamma function (which is implemented in numpy.special)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q(\\Theta) \\ln \\dfrac{P(D,\\Theta)}{Q(\\Theta)} \\hspace{2mm} d \\Theta\\\\\n",
      "                =& \\int Q(\\Theta) \\ln P(D,\\Theta) \\hspace{2mm} d \\Theta  - \\int Q(\\Theta) \\ln Q(\\Theta)  \\hspace{2mm} d \\Theta\\\\\n",
      "                =& \\mathbb{E}_Q[ \\ln P(D,\\Theta)] - \\mathbb{E}_q[\\ln Q(\\Theta)]\\\\\n",
      "               =& \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N} (x_n | m_x^{(n)} \\Sigma_x) \\right) \\right] \n",
      "                + \\mathbb{E}_Q \\left[ \\ln P(X) \\right ] + \\mathbb{E}_Q \\left[\\ln  P(W | \\alpha ) \\right] \n",
      "                + \\mathbb{E}_Q \\left[ \\ln P(\\alpha) \\right] + \\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] \n",
      "                + \\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln Q(X) \\right] - \\mathbb{E}_Q \\left[ \\ln Q(W) \\right] - \\mathbb{E}_Q \\left[ Q(\\alpha) \\right] - \\mathbb{E}_Q \\left[ \\ln Q(\\mu) \\right] - \\mathbb{E}_Q \\left[ \\ln \\left( Q(\\tau) \\right) \\right]\\\\\n",
      "                =& \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}({t}|{W} {x}_i + \\mu , \\sigma^2 I_d) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x|0, I_q) \\right) \\right]              \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} (\\dfrac{\\alpha_i}{2 \\pi})^{d/2}  \\exp( - \\dfrac{1}{2} \\alpha_i ||w_i||^2 ) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma ( \\alpha_i| a_{\\alpha}, b_{\\alpha}) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu|0, \\beta^{-1} I) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau|c_\\tau, d_\\tau ) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n| m_x^{(n)}, \\Sigma_x) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{k=1}^{d}  \\mathcal{N}(\\widetilde{w}_k | m_w^{(k)}, \\Sigma_w) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma(\\alpha_i| \\widetilde{a}_\\alpha, \\widetilde{b}_{\\alpha i} ) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu | m_\\mu, \\Sigma_\\mu) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau | \\widetilde{a}_\\tau, \\widetilde{b}_\\tau ) \\right) \\right]\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 5. Optimize variational parameters (50 points)\n",
      "Implement the update equations for the Q-distributions, in the __update_XXX methods. Each update function should re-estimate the variational parameters of the Q-distribution corresponding to one group of variables (i.e. either $Z$, $\\mu$, $W$, $\\alpha$ or $\\tau$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6. Learning algorithm (10 points)\n",
      "Implement the learning algorithm described in [Bishop99], i.e. iteratively optimize each of the Q-distributions holding the others fixed.\n",
      "\n",
      "What would be a good way to track convergence of the algorithm? Implement your suggestion.\n",
      "\n",
      "Test the algorithm on some test data drawn from a Gaussian with different variances in orthogonal directions. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "def plot_data(X):\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111, projection='3d')\n",
      "    ax.scatter(X[0,:], X[1,:], zs=X[2,:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We created a set of orthonormal basis vectors by using Matlab code from <a href=\"http://www.mathworks.com/matlabcentral/fileexchange/18973-create-orthonormal-vectors\">[Price 2008]</a>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# matrix with orthonormal vectors\n",
      "A = np.array([ [0.4246,-0.1753,-0.1466,-0.0956,0.7864,0.3741]\n",
      "                ,[0.5199,0.3483,0.2280,-0.4751,-0.4102,0.4031]\n",
      "                ,[-0.2212,-0.7183,-0.2838,-0.2656,-0.3263,0.4213]\n",
      "                ,[-0.5600,0.0724,0.6716,-0.1030,0.2397,0.4025]\n",
      "                ,[0.2692,-0.1393,0.2114,0.8147,-0.2223,0.3876]\n",
      "                ,[-0.3382,0.5544,-0.5918,0.1422,-0.0037,0.4558]])\n",
      "\n",
      "# eigenvalues on a diagonal matrix representing the magnitude of the variance\n",
      "L = np.diag( [7,6,5,4,1,1])\n",
      "\n",
      "# Covariance matrix generated based on A and L\n",
      "covariance = np.dot(np.dot(A, L) , A.T)\n",
      "means = [2,2,2,2,2,2]\n",
      "\n",
      "dims = 6\n",
      "N = 100\n",
      "\n",
      "data = numpy.random.multivariate_normal(means, covariance, (N)).T\n",
      "\n",
      "#print 'Plot of the first three dimensions of the data'\n",
      "#plot_data(data)\n",
      "\n",
      "bay_pca = BayesianPCA(dims,N)\n",
      "bay_pca.fit(data, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 temp:\n",
        "[ 3.22441598  3.09608595  4.40322049  2.33528051 -0.9589544  -0.60213922\n",
        "  3.11920839  1.9683722   3.26870131  2.94313343  1.67600896  2.23049547\n",
        "  3.02697613  2.61934379  4.29306665  2.90592999  2.74824565  1.45177561\n",
        "  4.60678757  3.85040737  1.51778723  4.52970506  4.23538806  1.36746587\n",
        " -2.14943734  3.32307426  2.66854529  2.06575314  6.38445935  2.70108559\n",
        "  2.73339318  3.86862694  2.28972953  0.27946922  4.04785362  0.15738197\n",
        "  1.33536582  4.91415767  2.93357656  3.48325353  2.89813937  4.72953703\n",
        "  3.43448417  3.75604848  1.89746371  2.19991724 -0.04429706  3.19927918\n",
        "  4.48710351  3.88632707  5.58452481  4.05772433  0.43040083  6.24286455\n",
        "  3.70715246  0.23723846  1.44689038  4.3020496   2.38377852  4.90098587\n",
        "  1.08416278  8.43352609  3.80226008  4.0528716   6.00005643  3.63988404\n",
        "  0.91549616 -0.80942044  3.03669525  4.7269964   7.13758901 -0.24101983\n",
        "  2.75945731  2.17904786  3.21012778  8.50604333  2.10088412  3.42230233\n",
        "  4.03551578  4.59249104  2.1223314   4.21420915  4.49003166  5.20263829\n",
        "  1.25114292  2.94639076  5.78774596  1.30567751  3.91941403  2.92616668\n",
        "  4.99944322  0.20345543  3.39140548  3.22436597  4.65891387  1.25780183\n",
        "  1.47124365  3.02603971  1.16562144  1.21171499]\n",
        "summation:\n",
        "[-33.19054975  74.28127577 -14.53996136  15.37213006 -18.42729653\n",
        "  -7.74515063]\n",
        "\n",
        " w at 0 :\n",
        "[[  9.66886962  47.75181716 -36.11507867 -15.54402647   9.31719962\n",
        "   90.99287415]\n",
        " [ 17.59871501  49.24783928 -32.48158938 -17.73160708  14.52751584\n",
        "   93.64140278]\n",
        " [ 11.83394235  48.84560449 -37.41904442 -15.76690545   9.73066339\n",
        "   91.85828746]\n",
        " [ 13.50431487  47.95943122 -34.04828975 -16.63587927  12.30250315\n",
        "   94.28647733]\n",
        " [ 11.26817089  45.25544644 -35.23257448 -19.32257996  15.29496953\n",
        "   92.83457569]\n",
        " [ 12.50411247  47.12508709 -33.01893393 -16.9424737   10.74197396\n",
        "   92.56204768]]\n",
        "temp:\n",
        "[ 76.86505857  76.73672854  78.04386308  75.9759231   72.68168819\n",
        "  73.03850337  76.75985098  75.6090148   76.9093439   76.58377602\n",
        "  75.31665155  75.87113806  76.66761872  76.25998639  77.93370924\n",
        "  76.54657258  76.38888824  75.0924182   78.24743016  77.49104996\n",
        "  75.15842982  78.17034765  77.87603065  75.00810846  71.49120525\n",
        "  76.96371685  76.30918788  75.70639573  80.02510194  76.34172818\n",
        "  76.37403577  77.50926953  75.93037212  73.92011181  77.68849621\n",
        "  73.79802456  74.97600841  78.55480026  76.57421915  77.12389612\n",
        "  76.53878196  78.37017962  77.07512676  77.39669107  75.5381063\n",
        "  75.84055983  73.59634553  76.83992177  78.1277461   77.52696966\n",
        "  79.2251674   77.69836692  74.07104342  79.88350714  77.34779505\n",
        "  73.87788105  75.08753297  77.94269219  76.02442111  78.54162846\n",
        "  74.72480537  82.07416868  77.44290267  77.69351419  79.64069902\n",
        "  77.28052663  74.55613875  72.83122215  76.67733784  78.36763899\n",
        "  80.7782316   73.39962276  76.4000999   75.81969045  76.85077037\n",
        "  82.14668593  75.74152671  77.06294492  77.67615837  78.23313363\n",
        "  75.76297399  77.85485174  78.13067425  78.84328088  74.89178551\n",
        "  76.58703335  79.42838855  74.9463201   77.56005662  76.56680927\n",
        "  78.64008581  73.84409802  77.03204807  76.86500856  78.29955646\n",
        "  74.89844442  75.11188624  76.6666823   74.80626403  74.85235758]\n",
        "summation:\n",
        "[ 200320.00531252 -567404.20683567 -148169.41101588 -564618.72746493\n",
        " -340744.20205011  162595.27440229]\n",
        "\n",
        " w at 1 :\n",
        "[[   196.15159092    226.41893445    239.75844275    354.71368113\n",
        "     860.19112993    535.46305558]\n",
        " [-15321.5508613  -17682.21623373 -18727.6938904  -27708.92927694\n",
        "  -67198.28539989 -41829.43363105]\n",
        " [ -2356.72689595  -2719.70562264  -2880.73199066  -4261.48469585\n",
        "  -10335.06317615  -6433.21362876]\n",
        " [ -2297.30917743  -2651.46353256  -2807.87841131  -4154.66805997\n",
        "  -10075.68900716  -6271.92255228]\n",
        " [  -928.43899789  -1071.49280513  -1134.87182782  -1679.08487139\n",
        "   -4071.92806492  -2534.78303622]\n",
        " [ 15930.54797065  18384.0493181   19472.87352385  28812.10084538\n",
        "   69872.43128444  43494.75995764]]\n",
        "temp:\n",
        "[ 11.29407181  11.16574178  12.47287632  10.40493634   7.11070143\n",
        "   7.46751661  11.18886422  10.03802803  11.33835714  11.01278926\n",
        "   9.74566479  10.3001513   11.09663196  10.68899962  12.36272248\n",
        "  10.97558582  10.81790148   9.52143144  12.6764434   11.9200632\n",
        "   9.58744306  12.59936089  12.30504389   9.4371217    5.92021849\n",
        "  11.39273008  10.73820112  10.13540897  14.45411518  10.77074142\n",
        "  10.80304901  11.93828277  10.35938536   8.34912505  12.11750945\n",
        "   8.2270378    9.40502165  12.9838135   11.00323239  11.55290936\n",
        "  10.9677952   12.79919286  11.50413999  11.82570431   9.96711954\n",
        "  10.26957306   8.02535876  11.26893501  12.55675934  11.9559829\n",
        "  13.65418064  12.12738015   8.50005666  14.31252038  11.77680829\n",
        "   8.30689429   9.5165462   12.37170543  10.45343435  12.9706417\n",
        "   9.15381861  16.50318192  11.87191591  12.12252743  14.06971226\n",
        "  11.70953987   8.98515199   7.26023539  11.10635108  12.79665223\n",
        "  15.20724483   7.828636    10.82911314  10.24870369  11.27978361\n",
        "  16.57569916  10.17053995  11.49195816  12.1051716   12.66214687\n",
        "  10.19198722  12.28386498  12.55968749  13.27229411   9.32079875\n",
        "  11.01604659  13.85740179   9.37533334  11.98906986  10.99582251\n",
        "  13.06909905   8.27311126  11.46106131  11.29402179  12.7285697\n",
        "   9.32745766   9.54089948  11.09569554   9.23527727   9.28137082]\n",
        "summation:\n",
        "[   8472.2578029   420896.74175436   77038.43795961   86878.18538804\n",
        "   88311.85626398 -414807.59335769]\n",
        "\n",
        " w at 2 :\n",
        "[[  3.86968861e+02  -1.80506407e+04  -3.24945404e+03  -2.84612789e+03\n",
        "   -9.61777595e+02   1.54538947e+04]\n",
        " [  2.56031821e+04  -1.19429116e+06  -2.14994830e+05  -1.88309418e+05\n",
        "   -6.36344449e+04   1.02248170e+06]\n",
        " [  5.25728612e+03  -2.45232420e+05  -4.41464398e+04  -3.86669314e+04\n",
        "   -1.30665197e+04   2.09953545e+05]\n",
        " [  1.29789632e+04  -6.05419267e+05  -1.08986835e+05  -9.54592606e+04\n",
        "   -3.22580627e+04   5.18324295e+05]\n",
        " [  7.75916460e+04  -3.61935531e+06  -6.51551906e+05  -5.70680516e+05\n",
        "   -1.92847166e+05   3.09867870e+06]\n",
        " [ -1.41220732e+05   6.58741039e+06   1.18585754e+06   1.03866751e+06\n",
        "    3.50991633e+05  -5.63975254e+06]]\n",
        "temp:\n",
        "[ 2.15319646  2.02486643  3.33200097  1.26406099 -2.03017392 -1.67335874\n",
        "  2.04798887  0.89715269  2.19748179  1.87191391  0.60478945  1.15927595\n",
        "  1.95575662  1.54812428  3.22184713  1.83471048  1.67702614  0.38055609\n",
        "  3.53556805  2.77918785  0.44656771  3.45848555  3.16416854  0.29624635\n",
        " -3.22065686  2.25185474  1.59732578  0.99453363  5.31323983  1.62986607\n",
        "  1.66217366  2.79740742  1.21851001 -0.7917503   2.9766341  -0.91383755\n",
        "  0.2641463   3.84293815  1.86235705  2.41203402  1.82691986  3.65831752\n",
        "  2.36326465  2.68482896  0.82624419  1.12869772 -1.11551658  2.12805966\n",
        "  3.41588399  2.81510755  4.51330529  2.98650481 -0.64081869  5.17164504\n",
        "  2.63593294 -0.83398106  0.37567086  3.23083008  1.312559    3.82976635\n",
        "  0.01294327  7.36230658  2.73104056  2.98165208  4.92883691  2.56866452\n",
        " -0.15572336 -1.88063996  1.96547574  3.65577688  6.06636949 -1.31223935\n",
        "  1.68823779  1.10782835  2.13890826  7.43482382  1.0296646   2.35108281\n",
        "  2.96429626  3.52127153  1.05111188  3.14298963  3.41881214  4.13141877\n",
        "  0.1799234   1.87517124  4.71652644  0.23445799  2.84819451  1.85494717\n",
        "  3.92822371 -0.86776409  2.32018596  2.15314645  3.58769436  0.18658231\n",
        "  0.40002413  1.95482019  0.09440192  0.14049547]\n",
        "summation:\n",
        "[ -15.39640832  263.80495464   37.9114923   -16.12558386 -441.80568694\n",
        "  285.99426367]\n",
        "\n",
        " w at 3 :\n",
        "[[ -1.80084133e-04  -1.87463023e-04  -1.69092346e-04  -1.91532051e-04\n",
        "   -1.89054809e-04  -1.72494934e-04]\n",
        " [  6.07778801e+00   6.32682343e+00   5.70681832e+00   6.46415195e+00\n",
        "    6.38054575e+00   5.82165471e+00]\n",
        " [  2.83460532e-02   2.95075237e-02   2.66158964e-02   3.01480070e-02\n",
        "    2.97580780e-02   2.71514791e-02]\n",
        " [ -9.25381069e-03  -9.63298263e-03  -8.68898625e-03  -9.84207385e-03\n",
        "   -9.71477821e-03  -8.86383180e-03]\n",
        " [ -2.93857515e-02  -3.05898233e-02  -2.75921347e-02  -3.12537987e-02\n",
        "   -3.08495676e-02  -2.81473619e-02]\n",
        " [  4.82967815e+00   5.02757267e+00   4.53488930e+00   5.13669996e+00\n",
        "    5.07026280e+00   4.62614335e+00]]\n",
        "temp:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 2.15322015  2.02489012  3.33202466  1.26408468 -2.03015023 -1.67333505\n",
        "  2.04801256  0.89717638  2.19750548  1.8719376   0.60481313  1.15929964\n",
        "  1.95578031  1.54814797  3.22187082  1.83473416  1.67704982  0.38057978\n",
        "  3.53559174  2.77921154  0.4465914   3.45850924  3.16419223  0.29627004\n",
        " -3.22063317  2.25187843  1.59734946  0.99455731  5.31326352  1.62988976\n",
        "  1.66219735  2.79743111  1.2185337  -0.79172661  2.97665779 -0.91381386\n",
        "  0.26416999  3.84296184  1.86238074  2.41205771  1.82694355  3.65834121\n",
        "  2.36328834  2.68485265  0.82626788  1.12872141 -1.11549289  2.12808335\n",
        "  3.41590768  2.81513124  4.51332898  2.9865285  -0.640795    5.17166873\n",
        "  2.63595663 -0.83395737  0.37569455  3.23085377  1.31258269  3.82979004\n",
        "  0.01296695  7.36233027  2.73106425  2.98167577  4.9288606   2.56868821\n",
        " -0.15569967 -1.88061627  1.96549942  3.65580057  6.06639318 -1.31221566\n",
        "  1.68826148  1.10785203  2.13893195  7.43484751  1.02968829  2.3511065\n",
        "  2.96431995  3.52129521  1.05113557  3.14301332  3.41883583  4.13144246\n",
        "  0.17994709  1.87519493  4.71655013  0.23448168  2.8482182   1.85497086\n",
        "  3.92824739 -0.8677404   2.32020965  2.15317014  3.58771804  0.186606\n",
        "  0.40004782  1.95484388  0.09442561  0.14051916]\n",
        "summation:\n",
        "[  1.02565760e-11   9.30431738e-12   9.58663406e-12   1.08751212e-11\n",
        "   1.07642624e-11   8.81843101e-12]\n",
        "\n",
        " w at 4 :\n",
        "[[  2.07977297e-13   3.11074549e-13   1.10426663e-13   1.72329018e-13\n",
        "    1.69160632e-13   2.33542014e-13]\n",
        " [  1.88667913e-13   2.82193233e-13   1.00174241e-13   1.56329352e-13\n",
        "    1.53455131e-13   2.11859106e-13]\n",
        " [  1.94392578e-13   2.90755695e-13   1.03213783e-13   1.61072783e-13\n",
        "    1.58111351e-13   2.18287451e-13]\n",
        " [  2.20519822e-13   3.29834579e-13   1.17086184e-13   1.82721695e-13\n",
        "    1.79362234e-13   2.47626275e-13]\n",
        " [  2.18271889e-13   3.26472313e-13   1.15892631e-13   1.80859068e-13\n",
        "    1.77533852e-13   2.45102023e-13]\n",
        " [  1.78815372e-13   2.67456650e-13   9.49429818e-14   1.48165583e-13\n",
        "    1.45441458e-13   2.00795484e-13]]\n",
        "temp:\n",
        "[ 2.15322015  2.02489012  3.33202466  1.26408468 -2.03015023 -1.67333505\n",
        "  2.04801256  0.89717638  2.19750548  1.8719376   0.60481313  1.15929964\n",
        "  1.95578031  1.54814797  3.22187082  1.83473416  1.67704982  0.38057978\n",
        "  3.53559174  2.77921154  0.4465914   3.45850924  3.16419223  0.29627004\n",
        " -3.22063317  2.25187843  1.59734946  0.99455731  5.31326352  1.62988976\n",
        "  1.66219735  2.79743111  1.2185337  -0.79172661  2.97665779 -0.91381386\n",
        "  0.26416999  3.84296184  1.86238074  2.41205771  1.82694355  3.65834121\n",
        "  2.36328834  2.68485265  0.82626788  1.12872141 -1.11549289  2.12808335\n",
        "  3.41590768  2.81513124  4.51332898  2.9865285  -0.640795    5.17166873\n",
        "  2.63595663 -0.83395737  0.37569455  3.23085377  1.31258269  3.82979004\n",
        "  0.01296695  7.36233027  2.73106425  2.98167577  4.9288606   2.56868821\n",
        " -0.15569967 -1.88061627  1.96549942  3.65580057  6.06639318 -1.31221566\n",
        "  1.68826148  1.10785203  2.13893195  7.43484751  1.02968829  2.3511065\n",
        "  2.96431995  3.52129521  1.05113557  3.14301332  3.41883583  4.13144246\n",
        "  0.17994709  1.87519493  4.71655013  0.23448168  2.8482182   1.85497086\n",
        "  3.92824739 -0.8677404   2.32020965  2.15317014  3.58771804  0.186606\n",
        "  0.40004782  1.95484388  0.09442561  0.14051916]\n",
        "summation:\n",
        "[  3.12080685e-24   4.84874927e-24   1.48582747e-24   2.52333045e-24\n",
        "   2.47022731e-24   3.54927955e-24]\n",
        "\n",
        " w at 5 :\n",
        "[[  6.32820326e-26   6.21770171e-26   5.71295237e-26   6.23545414e-26\n",
        "    6.55568656e-26   5.33076183e-26]\n",
        " [  9.83203140e-26   9.66034685e-26   8.87612563e-26   9.68792853e-26\n",
        "    1.01854687e-25   8.28232210e-26]\n",
        " [  3.01288055e-26   2.96027036e-26   2.71995737e-26   2.96872236e-26\n",
        "    3.12118618e-26   2.53799506e-26]\n",
        " [  5.11667295e-26   5.02732684e-26   4.61921143e-26   5.04168058e-26\n",
        "    5.30060472e-26   4.31019102e-26]\n",
        " [  5.00899328e-26   4.92152745e-26   4.52200077e-26   4.93557912e-26\n",
        "    5.18905423e-26   4.21948365e-26]\n",
        " [  7.19703701e-26   7.07136409e-26   6.49731496e-26   7.09155385e-26\n",
        "    7.45575275e-26   6.06265138e-26]]\n",
        "temp:\n",
        "[ 2.15322015  2.02489012  3.33202466  1.26408468 -2.03015023 -1.67333505\n",
        "  2.04801256  0.89717638  2.19750548  1.8719376   0.60481313  1.15929964\n",
        "  1.95578031  1.54814797  3.22187082  1.83473416  1.67704982  0.38057978\n",
        "  3.53559174  2.77921154  0.4465914   3.45850924  3.16419223  0.29627004\n",
        " -3.22063317  2.25187843  1.59734946  0.99455731  5.31326352  1.62988976\n",
        "  1.66219735  2.79743111  1.2185337  -0.79172661  2.97665779 -0.91381386\n",
        "  0.26416999  3.84296184  1.86238074  2.41205771  1.82694355  3.65834121\n",
        "  2.36328834  2.68485265  0.82626788  1.12872141 -1.11549289  2.12808335\n",
        "  3.41590768  2.81513124  4.51332898  2.9865285  -0.640795    5.17166873\n",
        "  2.63595663 -0.83395737  0.37569455  3.23085377  1.31258269  3.82979004\n",
        "  0.01296695  7.36233027  2.73106425  2.98167577  4.9288606   2.56868821\n",
        " -0.15569967 -1.88061627  1.96549942  3.65580057  6.06639318 -1.31221566\n",
        "  1.68826148  1.10785203  2.13893195  7.43484751  1.02968829  2.3511065\n",
        "  2.96431995  3.52129521  1.05113557  3.14301332  3.41883583  4.13144246\n",
        "  0.17994709  1.87519493  4.71655013  0.23448168  2.8482182   1.85497086\n",
        "  3.92824739 -0.8677404   2.32020965  2.15317014  3.58771804  0.186606\n",
        "  0.40004782  1.95484388  0.09442561  0.14051916]\n",
        "summation:\n",
        "[  9.82135998e-37   9.62997063e-37   8.75574180e-37   9.66071795e-37\n",
        "   1.02153624e-36   8.09378552e-37]\n",
        "\n",
        " w at 6 :\n",
        "[[  1.99152223e-38   2.22398276e-38   1.47215391e-38   1.76075956e-38\n",
        "    1.91397987e-38   1.66804629e-38]\n",
        " [  1.95271334e-38   2.18064390e-38   1.44346597e-38   1.72644754e-38\n",
        "    1.87668205e-38   1.63554098e-38]\n",
        " [  1.77544194e-38   1.98268049e-38   1.31242512e-38   1.56971703e-38\n",
        "    1.70631294e-38   1.48706316e-38]\n",
        " [  1.95894811e-38   2.18760643e-38   1.44807478e-38   1.73195988e-38\n",
        "    1.88267407e-38   1.64076307e-38]\n",
        " [  2.07141591e-38   2.31320204e-38   1.53121215e-38   1.83139576e-38\n",
        "    1.99076280e-38   1.73496312e-38]\n",
        " [  1.64121403e-38   1.83278482e-38   1.21320245e-38   1.45104245e-38\n",
        "    1.57731135e-38   1.37463742e-38]]\n",
        "temp:\n",
        "[ 2.15322015  2.02489012  3.33202466  1.26408468 -2.03015023 -1.67333505\n",
        "  2.04801256  0.89717638  2.19750548  1.8719376   0.60481313  1.15929964\n",
        "  1.95578031  1.54814797  3.22187082  1.83473416  1.67704982  0.38057978\n",
        "  3.53559174  2.77921154  0.4465914   3.45850924  3.16419223  0.29627004\n",
        " -3.22063317  2.25187843  1.59734946  0.99455731  5.31326352  1.62988976\n",
        "  1.66219735  2.79743111  1.2185337  -0.79172661  2.97665779 -0.91381386\n",
        "  0.26416999  3.84296184  1.86238074  2.41205771  1.82694355  3.65834121\n",
        "  2.36328834  2.68485265  0.82626788  1.12872141 -1.11549289  2.12808335\n",
        "  3.41590768  2.81513124  4.51332898  2.9865285  -0.640795    5.17166873\n",
        "  2.63595663 -0.83395737  0.37569455  3.23085377  1.31258269  3.82979004\n",
        "  0.01296695  7.36233027  2.73106425  2.98167577  4.9288606   2.56868821\n",
        " -0.15569967 -1.88061627  1.96549942  3.65580057  6.06639318 -1.31221566\n",
        "  1.68826148  1.10785203  2.13893195  7.43484751  1.02968829  2.3511065\n",
        "  2.96431995  3.52129521  1.05113557  3.14301332  3.41883583  4.13144246\n",
        "  0.17994709  1.87519493  4.71655013  0.23448168  2.8482182   1.85497086\n",
        "  3.92824739 -0.8677404   2.32020965  2.15317014  3.58771804  0.186606\n",
        "  0.40004782  1.95484388  0.09442561  0.14051916]\n",
        "summation:\n",
        "[  3.02474884e-49   3.41709820e-49   2.14815344e-49   2.63526513e-49\n",
        "   2.89387202e-49   2.47878268e-49]\n",
        "\n",
        " w at 7 :\n",
        "[[  6.13342202e-51   6.09056159e-51   5.38603684e-51   5.82181649e-51\n",
        "    6.28081918e-51   5.02552734e-51]\n",
        " [  6.92900683e-51   6.88058684e-51   6.08467604e-51   6.57698200e-51\n",
        "    7.09552332e-51   5.67740376e-51]\n",
        " [  4.35590930e-51   4.32547015e-51   3.82512207e-51   4.13460945e-51\n",
        "    4.46058964e-51   3.56909099e-51]\n",
        " [  5.34364803e-51   5.30630655e-51   4.69250038e-51   5.07216659e-51\n",
        "    5.47206551e-51   4.37841211e-51]\n",
        " [  5.86803709e-51   5.82703117e-51   5.15299025e-51   5.56991431e-51\n",
        "    6.00905657e-51   4.80807952e-51]\n",
        " [  5.02634140e-51   4.99121726e-51   4.41385899e-51   4.77098057e-51\n",
        "    5.14713342e-51   4.11842134e-51]]\n",
        "temp:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 2.15322015  2.02489012  3.33202466  1.26408468 -2.03015023 -1.67333505\n",
        "  2.04801256  0.89717638  2.19750548  1.8719376   0.60481313  1.15929964\n",
        "  1.95578031  1.54814797  3.22187082  1.83473416  1.67704982  0.38057978\n",
        "  3.53559174  2.77921154  0.4465914   3.45850924  3.16419223  0.29627004\n",
        " -3.22063317  2.25187843  1.59734946  0.99455731  5.31326352  1.62988976\n",
        "  1.66219735  2.79743111  1.2185337  -0.79172661  2.97665779 -0.91381386\n",
        "  0.26416999  3.84296184  1.86238074  2.41205771  1.82694355  3.65834121\n",
        "  2.36328834  2.68485265  0.82626788  1.12872141 -1.11549289  2.12808335\n",
        "  3.41590768  2.81513124  4.51332898  2.9865285  -0.640795    5.17166873\n",
        "  2.63595663 -0.83395737  0.37569455  3.23085377  1.31258269  3.82979004\n",
        "  0.01296695  7.36233027  2.73106425  2.98167577  4.9288606   2.56868821\n",
        " -0.15569967 -1.88061627  1.96549942  3.65580057  6.06639318 -1.31221566\n",
        "  1.68826148  1.10785203  2.13893195  7.43484751  1.02968829  2.3511065\n",
        "  2.96431995  3.52129521  1.05113557  3.14301332  3.41883583  4.13144246\n",
        "  0.17994709  1.87519493  4.71655013  0.23448168  2.8482182   1.85497086\n",
        "  3.92824739 -0.8677404   2.32020965  2.15317014  3.58771804  0.186606\n",
        "  0.40004782  1.95484388  0.09442561  0.14051916]\n",
        "summation:\n",
        "[  9.14062611e-62   9.06946518e-62   7.89974670e-62   8.62326921e-62\n",
        "   9.38534878e-62   7.30119482e-62]\n",
        "\n",
        " w at 8 :\n",
        "[[  1.85348670e-63   1.91019596e-63   1.51761305e-63   1.67410891e-63\n",
        "    1.85494304e-63   1.48005901e-63]\n",
        " [  1.83905707e-63   1.89532484e-63   1.50579824e-63   1.66107576e-63\n",
        "    1.84050208e-63   1.46853657e-63]\n",
        " [  1.60186789e-63   1.65087862e-63   1.31159053e-63   1.44684141e-63\n",
        "    1.60312653e-63   1.27913462e-63]\n",
        " [  1.74857987e-63   1.80207940e-63   1.43171657e-63   1.57935481e-63\n",
        "    1.74995378e-63   1.39628808e-63]\n",
        " [  1.90311024e-63   1.96133778e-63   1.55824422e-63   1.71892995e-63\n",
        "    1.90460557e-63   1.51968475e-63]\n",
        " [  1.48049678e-63   1.52579404e-63   1.21221330e-63   1.33721642e-63\n",
        "    1.48166005e-63   1.18221652e-63]]\n",
        "temp:\n",
        "[ 2.15322015  2.02489012  3.33202466  1.26408468 -2.03015023 -1.67333505\n",
        "  2.04801256  0.89717638  2.19750548  1.8719376   0.60481313  1.15929964\n",
        "  1.95578031  1.54814797  3.22187082  1.83473416  1.67704982  0.38057978\n",
        "  3.53559174  2.77921154  0.4465914   3.45850924  3.16419223  0.29627004\n",
        " -3.22063317  2.25187843  1.59734946  0.99455731  5.31326352  1.62988976\n",
        "  1.66219735  2.79743111  1.2185337  -0.79172661  2.97665779 -0.91381386\n",
        "  0.26416999  3.84296184  1.86238074  2.41205771  1.82694355  3.65834121\n",
        "  2.36328834  2.68485265  0.82626788  1.12872141 -1.11549289  2.12808335\n",
        "  3.41590768  2.81513124  4.51332898  2.9865285  -0.640795    5.17166873\n",
        "  2.63595663 -0.83395737  0.37569455  3.23085377  1.31258269  3.82979004\n",
        "  0.01296695  7.36233027  2.73106425  2.98167577  4.9288606   2.56868821\n",
        " -0.15569967 -1.88061627  1.96549942  3.65580057  6.06639318 -1.31221566\n",
        "  1.68826148  1.10785203  2.13893195  7.43484751  1.02968829  2.3511065\n",
        "  2.96431995  3.52129521  1.05113557  3.14301332  3.41883583  4.13144246\n",
        "  0.17994709  1.87519493  4.71655013  0.23448168  2.8482182   1.85497086\n",
        "  3.92824739 -0.8677404   2.32020965  2.15317014  3.58771804  0.186606\n",
        "  0.40004782  1.95484388  0.09442561  0.14051916]\n",
        "summation:\n",
        "[  2.80800902e-74   2.90350781e-74   2.24239538e-74   2.50593553e-74\n",
        "   2.81046151e-74   2.17915412e-74]\n",
        "\n",
        " w at 9 :\n",
        "[[  5.69392875e-76   5.66791571e-76   4.94241824e-76   5.31180375e-76\n",
        "    5.80076306e-76   4.60387973e-76]\n",
        " [  5.88757603e-76   5.86067831e-76   5.11050708e-76   5.49245518e-76\n",
        "    5.99804372e-76   4.76045507e-76]\n",
        " [  4.54700802e-76   4.52623475e-76   3.94687331e-76   4.24185397e-76\n",
        "    4.63232283e-76   3.67652617e-76]\n",
        " [  5.08140048e-76   5.05818581e-76   4.41073423e-76   4.74038284e-76\n",
        "    5.17674201e-76   4.10861423e-76]\n",
        " [  5.69890177e-76   5.67286601e-76   4.94673490e-76   5.31644303e-76\n",
        "    5.80582939e-76   4.60790071e-76]\n",
        " [  4.41877081e-76   4.39858340e-76   3.83556142e-76   4.12222288e-76\n",
        "    4.50167952e-76   3.57283876e-76]]\n",
        "\n",
        " w final:\n",
        "[[  5.69392875e-76   5.66791571e-76   4.94241824e-76   5.31180375e-76\n",
        "    5.80076306e-76   4.60387973e-76]\n",
        " [  5.88757603e-76   5.86067831e-76   5.11050708e-76   5.49245518e-76\n",
        "    5.99804372e-76   4.76045507e-76]\n",
        " [  4.54700802e-76   4.52623475e-76   3.94687331e-76   4.24185397e-76\n",
        "    4.63232283e-76   3.67652617e-76]\n",
        " [  5.08140048e-76   5.05818581e-76   4.41073423e-76   4.74038284e-76\n",
        "    5.17674201e-76   4.10861423e-76]\n",
        " [  5.69890177e-76   5.67286601e-76   4.94673490e-76   5.31644303e-76\n",
        "    5.80582939e-76   4.60790071e-76]\n",
        " [  4.41877081e-76   4.39858340e-76   3.83556142e-76   4.12222288e-76\n",
        "    4.50167952e-76   3.57283876e-76]]\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 7. PCA Representation of MNIST (10 points)\n",
      "\n",
      "Download the MNIST dataset from here http://deeplearning.net/tutorial/gettingstarted.html (the page contains python code for loading the data). Run your algorithm on (part of) this dataset, and visualize the results.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}