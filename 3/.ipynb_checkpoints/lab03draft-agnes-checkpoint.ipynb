{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lab 2: Bayesian PCA\n",
      "\n",
      "### Machine Learning: Principles and Methods, November 2013\n",
      "\n",
      "* The lab exercises should be made in groups of three people, or at least two people.\n",
      "* The deadline is Wednesday, 11 December, 23:59.\n",
      "* Assignment should be sent to T.S.Cohen at uva dot nl (Taco Cohen). The subject line of your email should be \"[MLPM2013] lab#_lastname1\\_lastname2\\_lastname3\". \n",
      "* Put your and your teammates' names in the body of the email\n",
      "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"[MLPM2013] lab01\\_Kingma\\_Hu\", the attached file should be \"lab01\\_Kingma\\_Hu.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
      "\n",
      "Notes on implementation:\n",
      "\n",
      "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact us.\n",
      "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
      "* NOTE: test your code and make sure we can run your notebook / scripts!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction\n",
      "\n",
      "In this lab assignment, we will implement a variational algorithm for Bayesian PCA. Unlike regular PCA based on maximization of retained variance or minimization of projection error (see Bishop, 12.1.1 and 12.1.2), probabilistic PCA defines a proper density model over observed and latent variables. We will work with a fully Bayesian model this time, which is to say that we will put priors on our parameters and will be interested in learning the posterior over those parameters. Bayesian methods are very elegant, but require a shift in mindset: we are no longer looking for a point estimate of the parameters (as in maximum likelihood or MAP), but for a full posterior distribution.\n",
      "\n",
      "The integrals involved in a Bayesian analysis are usually analytically intractable, so that we must resort to approximations. In this lab assignment, we will implement the variational method described in Bishop99. Chapters 10 and 12 of the PRML book contain additional material that may be useful when doing this exercise.\n",
      "\n",
      "* [Bishop99] Variational Principal Components, C. Bishop, ICANN 1999 - http://research.microsoft.com/pubs/67241/bishop-vpca-icann-99.pdf\n",
      "\n",
      "Below, you will find some code to get you started."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import scipy.special as sp\n",
      "\n",
      "import sys\n",
      "\n",
      "class BayesianPCA(object):\n",
      "    \n",
      "    def __init__(self, d, N, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        self.d = d # number of dimensions\n",
      "        self.N = N # number of data points\n",
      "        \n",
      "        # Hyperparameters\n",
      "        self.a_alpha = a_alpha\n",
      "        self.b_alpha = b_alpha\n",
      "        self.a_tau = a_tau\n",
      "        self.b_tau = b_tau\n",
      "        self.beta = beta\n",
      "\n",
      "        # Variational parameters\n",
      "        self.means_z = np.random.randn(d, N) # called x in bishop99\n",
      "        self.sigma_z = np.random.randn(d, d)\n",
      "        self.mean_mu = np.random.randn(d, 1)\n",
      "        self.sigma_mu = np.random.randn(d, d)\n",
      "        self.means_w = np.random.randn(d, d)\n",
      "        self.sigma_w = np.random.randn(d, d)\n",
      "        self.a_alpha_tilde = np.abs(np.random.randn(1))\n",
      "        self.bs_alpha_tilde = np.abs(np.random.randn(d, 1))\n",
      "        self.a_tau_tilde = np.abs(np.random.randn(1))\n",
      "        self.b_tau_tilde = np.abs(np.random.randn(1))\n",
      "    \n",
      "    def __update_z(self, X):\n",
      "        #Update means of z\n",
      "        exp_tau = self.a_tau_tilde / self.b_tau_tilde\n",
      "        exp_Wt = np.transpose(self.means_w)\n",
      "        exp_mu = self.mean_mu\n",
      "        self.means_z = np.dot( np.dot( np.dot(exp_tau, self.sigma_z), exp_Wt), (X-exp_mu) )\n",
      "        \n",
      "        #update sigma of z\n",
      "        exp_WtW = numpy.trace(self.sigma_w) + np.dot(np.transpose(self.means_w), self.means_w)\n",
      "        print expWtW\n",
      "        # sigma w = identity+ exp_tau*expWtW inverse\n",
      "        self.sigma_z = numpy.identity(2)\n",
      "    \n",
      "    def __update_mu(self):\n",
      "        pass\n",
      "    \n",
      "    def __update_w(self, X):\n",
      "        pass\n",
      "    \n",
      "    def __update_alpha(self):\n",
      "        pass\n",
      "\n",
      "    def __update_tau(self, X):\n",
      "        pass\n",
      "\n",
      "    def L(self, X):\n",
      "        # p(X)\n",
      "        p_x = - (self.N / 2 ) * np.trace(self.sigma_z) \n",
      "        for i in range(self.N):\n",
      "            p_x += np.dot(self.sigma_z[:,i], self.sigma_z[:,i].T)\n",
      "        print \"P(X):\", p_x\n",
      "        \n",
      "        # p(W|\\alpha)\n",
      "        p_w_alpha = (self.d / float(4) ) * np.log(2 * np.pi)\n",
      "        p_w_alpha_temp = (self.d * (sp.psi(self.a_alpha_tilde[0] )) )  \n",
      "        p_w_alpha_temp -= sum( map ( lambda x:  np.log( x ), self.bs_alpha_tilde))               \n",
      "        p_w_alpha_temp -= sum (map ( lambda x:  ((self.d * (self.a_alpha_tilde[0]**2 + self.a_alpha_tilde[0] )) / (x**2)),\n",
      "                                    self.bs_alpha_tilde))         \n",
      "        p_w_alpha = p_w_alpha * p_w_alpha_temp        \n",
      "        print \"P(W|\\\\alpha):\", p_w_alpha\n",
      "            \n",
      "        # P(\\alpha)        \n",
      "        p_alpha = (self.a_alpha - 1) * (d * sp.psi(self.a_alpha_tilde[0]) -  np.sum(map(lambda x: np.log(x),  self.bs_alpha_tilde)))\n",
      "        p_alpha -= self.b_alpha * np.sum(map(lambda x: (self.a_alpha_tilde[0] / float( x)),  self.bs_alpha_tilde))\n",
      "        print \"P(\\\\alpha):\", p_alpha\n",
      "        \n",
      "        \n",
      "        # P(\\mu)\n",
      "        p_mu = - (self.beta / float(2)) *  ( \\\n",
      "                    ( d / float(self.beta + self.N*(self.a_alpha_tilde[0] / float(self.a_alpha_tilde[0])))) +\n",
      "                    (np.dot(self.mean_mu.T, self.mean_mu)[0][0]) )        \n",
      "        print \"P(\\\\mu):\", p_mu\n",
      "        \n",
      "        # P(\\tau)\n",
      "        # Note: c_tau and d_tau correspond to a_tau_tilde, b_tau_tilde, respectively        \n",
      "        p_tau = (self.a_tau_tilde[0] - 1) * ( sp.psi(self.a_tau_tilde[0]) - np.log(self.b_tau_tilde[0]))\n",
      "        p_tau -= self.a_tau_tilde[0] # since d_tau is b_tau_tilde        \n",
      "        print \"P(\\\\tau):\", p_tau\n",
      "        \n",
      "        #Q(\\alpha)\n",
      "        q_alpha = (self.a_alpha_tilde[0] - 1) * (d * sp.psi(self.a_alpha_tilde[0]) -  np.sum(map(lambda x: np.log(x),  self.bs_alpha_tilde)))\n",
      "        q_alpha -= self.d * self.a_alpha_tilde[0] \n",
      "        print \"Q(\\\\alpha):\", q_alpha\n",
      "        \n",
      "        #Q(\\tau)\n",
      "        q_tau = (sp.psi(self.a_tau_tilde[0]) - np.log(self.b_tau_tilde[0])) - self.a_tau_tilde[0]\n",
      "        print \"Q(\\\\tau):\", q_tau\n",
      "        \n",
      "        L = p_x + p_w_alpha + p_alpha + p_mu + p_tau - q_alpha - q_tau\n",
      "        return L\n",
      "    \n",
      "    def fit(self, X):\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = 3\n",
      "N = 2\n",
      "\n",
      "pca = BayesianPCA(d, N)\n",
      "X = np.random.randn(d, N)\n",
      "pca.fit(X)\n",
      "\n",
      "pca.L(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P(X): 14.3581143139\n",
        "P(W|\\alpha): -29.5463999124\n",
        "P(\\alpha): 5.78701073778\n",
        "P(\\mu): -0.0254435588673\n",
        "P(\\tau): -1.0262428289\n",
        "Q(\\alpha): 2.01589284815\n",
        "Q(\\tau): -0.335601208605\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "-12.133252888087144"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "sp.psi(np.abs(np.random.randn(1))[0])\n",
      "\n",
      "np.dot(np.array([[1],[2],[3]]).T,np.array([[1,2,3]]).T)[0][0]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "array([1])"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import display, Math, Latex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1. The Q-distribution (5 points)\n",
      "\n",
      "In variational Bayes, we introduce a distribution $Q(\\Theta)$ over parameters / latent variables in order to make inference tractable. We can think of $Q$ as being an approximation of a certain distribution. What function does $Q$ approximate, $p(D|\\Theta)$, $p(\\Theta|D)$, $p(D, \\Theta)$, $p(\\Theta)$, or $p(D)$, and how do you see that?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function $Q$ approximates the function that is the posterior distribution, $p(\\Theta | D) $.\n",
      "\n",
      "The posterior $P(\\Theta | D)$ is given by multiplying the likelihood by the prior and then normalizing:\n",
      "\n",
      "\\begin{align}\n",
      "P(\\Theta | D) &= \\dfrac{P(D | \\Theta)P( \\Theta)}{P(D)} \\\\\n",
      "              &= \\dfrac{P(D | \\Theta)P( \\Theta)}{\\int P(D | \\Theta)P(\\Theta) \\hspace{1mm} d\\Theta}\n",
      "\\end{align}\n",
      "\n",
      "Why is this useful? \n",
      "For Maximum a Posteriori (MAP) learning, we could use the posterior to get a point estimate of the parameters:\n",
      "\n",
      "\\begin{align}\n",
      "\\widetilde{\\Theta}_{\\text{MAP}}(D)&= \\arg\\max_{\\Theta} P(\\Theta | D) \\\\\n",
      "            & \\propto {P(D | \\Theta)P( \\Theta)}\n",
      "\\end{align}\n",
      "\n",
      "So for MAP learning the normalization (and therefore integration) is not needed. Like Maximum likelihood estimation (MLE) learning, MAP learning learns a point estimate of our parameters.\n",
      "\n",
      "In the Bayesian framework no fixed parameter setting is chosen. The posterior is used to obtain a predictive density (also called the posterior predictive distribution) for a new data point $t^{*}$ with:\n",
      "\n",
      "\\begin{align}\n",
      "P(t^{*} | D) &= \\int P(t^{*} | \\Theta) P(\\Theta | D) \\hspace{2mm} d\\Theta\n",
      "\\end{align}\n",
      "\n",
      "This means that in the Bayesian learning framework we consider all possible parametrizations of our models by incorporating the posterior this way. Each of these is weighted by the evidence for them in $D$.\n",
      "(Note that when predicting, we also return a probability distribution as prediction for $t^{*}$. \n",
      "Retaining all possible parametrizations is in this case of Principal Component Analysis) particularly advantageous for automatically determining an adequate number of principal components given $D$.\n",
      "\n",
      "Note that we need to integrate over all possible parametrizations for this. In practice this can quickly become infeasible for models with many parameters; when using a mixture of models; or when the prior distribution is not of the same functional form as the likelihood distribution. Therefore we use a ''simpler'' distribution $Q(\\Theta)$ to approximate $P(\\Theta|D)$. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2. The mean-field approximation (15 points)\n",
      "\n",
      "Equation 13 from [Bishop99] is a very powerful result: assuming only that $Q(\\Theta)$ factorizes in a certain way (no assumptions on the functional form of the factors $Q_i$!), we get a set of coupled equations for the $Q_i$.\n",
      "\n",
      "However, the expression given in eq. 13 for Q_i contains a small mistake. Starting with the expression for the lower bound $\\mathcal{L}(Q)$, derive the correct expression (and include your derivation). You can proceed as follows: first, substitute the factorization of $Q$ (eq. 12) into the definition of $\\mathcal{L}(Q)$ and separate $\\mathcal{L}(Q)$ into $Q_i$-dependent and $Q_i$-independent terms. At this point, you should be able to spot the expectations $\\langle\\cdot\\rangle_{k \\neq i}$ over the other $Q$-distributions that appear in Bishop's solution (eq. 13). Now, keeping all $Q_k, k \\neq i$ fixed, maximize the expression with respect to $Q_i$. You should be able to spot the form of the optimal $ln Q_i$, from which $Q_i$ can easily be obtained."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is given that the lower bound is \n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q(\\Theta) \\log \\dfrac{P(D, \\Theta)}{Q(\\Theta)}\n",
      "                \\hspace{3mm} d \\Theta  \\\\\n",
      "\\end{align} \n",
      "$$\n",
      "\n",
      "And the factorization is\n",
      "\n",
      "\\begin{equation}\n",
      "Q(\\Theta)= \\prod_{k} Q_k(\\Theta_k)\n",
      "\\end{equation}\n",
      "\n",
      "Plugging in the factorization into the lower bound, we get\n",
      "\n",
      "\\begin{align}                \n",
      "\\mathcal{L}(Q) =&  \\int \n",
      "                     \\prod_{k} Q_k( \\Theta_k) \n",
      "                     \\left[\n",
      "                         \\log P(D,\\Theta) - \\log \\left( \\prod_k Q_k(\\Theta_k) \\right)\n",
      "                     \\right]\n",
      "                 \\hspace{3mm} d \\Theta  \\\\\n",
      "              =& \\int\n",
      "              \\prod_{k} Q_k( \\Theta_k) \n",
      "                     \\left[\n",
      "                         \\log P(D,\\Theta) - \\sum_k \\log Q_k(\\Theta_k)\n",
      "                     \\right]\n",
      "                  \\hspace{3mm} d \\Theta\n",
      "\\end{align}  \n",
      "\n",
      "By denoting a specific factor $Q_i$ separately we arrive at\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q_i( \\Theta_i) \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "                \\left[ \\log P(D, \\Theta) - \\log \\left( \\prod_{j \\neq i}  Q_j(\\Theta_j) \\right) - \\log Q_i (\\Theta_i) \\right] \n",
      "                \\hspace{3mm} d \\Theta  \n",
      "\\end{align}                \n",
      "\n",
      "By taking the integral of $Q_i$ apart from those for $\\{Q_1, \\dots ,Q_{i-1},Q_{i+1}, \\dots ,Q_M \\}$, we get\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "    \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "            \\log \\left( \\prod_{j \\neq i} Q_j(\\Theta_j) \\right)\n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{3mm} - \\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[ \n",
      "            \\int\n",
      "             \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Now, when we keep the $Q_k \\neq Q_{i}$ fixed the following happens, the integral over $\\Theta_{\\setminus i}$  in the third row of the equation for $\\mathcal{L}(Q)$ will become 1, because all $Q_n(\\Theta_n)$ are probability distributions. \n",
      "\n",
      "\\begin{align}\n",
      "\\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[ \n",
      "            \\int\n",
      "             \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "        =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[\n",
      "        \\prod_{k \\neq i}         \n",
      "            \\int\n",
      "              Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{k}\n",
      "        \\right]        \n",
      "        \\hspace{3mm} d \\Theta_{i} \\\\\n",
      "     =&\n",
      "      \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[\n",
      "        \\prod_{k \\neq i}         \n",
      "            1\n",
      "        \\right]        \n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "\\end{align}\n",
      "\n",
      "So we can rewrite the equation as\n",
      "\n",
      "\\begin{align}       \n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "    \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "            \\log \\left( \\prod_{j \\neq i} Q_j(\\Theta_j) \\right)\n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{3mm} - \\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}         \n",
      "\\end{align}\n",
      "\n",
      "\n",
      "Furthermore, when we keep the $Q_k \\neq Q_{i}$ fixed, the $\\Theta_{\\setminus i}$ integral in the second row of the equation for  $\\mathcal{L}(Q)$ will become a constant. We know that $\\int Q_i(\\Theta_i)  d\\Theta_{i}$ should be 1 because $Q_i$ is a probability distribution. So the whole second row can be viewed as a constant. \n",
      "\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{1mm} + \\text{const}        \n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the integral over $\\Theta_{\\setminus i}$  in the  first line in $\\mathcal{L}(Q)$ could be rewritten as an expectation :\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \\cdot\n",
      "        \\mathbb{E}_{[k \\neq i]}[ \\log P(D, \\Theta)]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{1mm} + \\text{const}        \n",
      "\\end{align}\n",
      "\n",
      "Where $\\mathbb{E}_{[k \\neq i]}$ denotes this expected value is with respect to the $Q$ distributions over all variables $\\Theta_k \\neq \\Theta_i$.\n",
      "\n",
      "\n",
      "We can define $\\mathbb{E}_{[k \\neq i]}[ \\log P(D, \\Theta)]$ as $\\log \\widetilde{P}(D, \\Theta_i)$.\n",
      "\n",
      "Intuitively it makes sense to see  $\\log \\widetilde{P}(D, \\Theta_i)$, the joint probability of that parameter and the data, as the expectation of $\\log P(D, \\Theta)$ under all $\\Theta_k$ that are not $\\Theta_i$ , i.e., to not marginalize out  $\\Theta_i$ in $\\log P(D, \\Theta)$.\n",
      "It is here particularly useful to be able to rewrite the above equation as:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =&  \\int Q_i(\\Theta_i) \\log \\widetilde{P}(D, \\Theta_i) -  \\int Q_i(\\Theta_i) \\log Q_i(\\Theta_i) \\\\\n",
      "                =& \\int Q_i(\\Theta_i) \\log \\dfrac{\\widetilde{P}(D , \\Theta_i)}{ Q_i(\\Theta_i)} \\\\\n",
      "                =& - \\text{KL} \\left(Q_i(\\Theta_i) || \\widetilde{P}(D, \\Theta_i) \\right)\n",
      "\\end{align}\n",
      "\n",
      "Recall we wanted to maximize the lower bound function $\\mathcal{L}(Q)$. We see that this negative KL-divergence will be maximal when \n",
      "\n",
      "\\begin{align}\n",
      " Q_i(\\Theta_i) &= \\widetilde{P}(D, \\Theta_i)  \\\\  \n",
      " \\log  Q_i(\\Theta_i) &= \\log \\widetilde{P}(D, \\Theta_i)  \\\\\n",
      "                     &= \\mathbb{E}_{[k \\neq i]}[\\log P(D, \\Theta)]\\\\\n",
      " Q_i(\\Theta_i)      &= \\exp \\left( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] \\right)    \n",
      "\\end{align}\n",
      "\n",
      "So in line with Bishop (except a subscript mismatch on his side), accounting for normalization (marginalizing over all $\\Theta_{\\setminus i}$) we get that the optimal value for $Q_i$ is \n",
      "\n",
      "\\begin{align}\n",
      " Q_i^{*}(\\Theta_i)      &= \\dfrac{\\exp ( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] )   }\n",
      "                      {\\int \\exp ( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] ) \\hspace{2mm} d\\Theta_i  }\n",
      "\\end{align}\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3. The log-probability (10 points)\n",
      "\n",
      "Write down the log-prob of data and parameters, $\\ln p(\\mathbf{X}, \\mathbf{Z}, \\mathbf{W}, \\mathbf{\\alpha}, \\tau, \\mathbf{\\mu})$, in full detail (where $\\mathbf{X}$ are observed, $\\mathbf{Z}$ is latent; this is different from [Bishop99] who uses $\\mathbf{T}$ and $\\mathbf{X}$ respectively, but $\\mathbf{X}$ and $\\mathbf{Z}$ are consistent with the PRML book and are more common nowadays). Could we use this to assess the convergence of the variational Bayesian PCA algorithm? If yes, how? If no, why not?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The log-probability of the data and parameters is defined as:\n",
      "$$\n",
      "\\begin{align}\n",
      "\\ln p ( X, Z, W, \\alpha, \\tau, \\mu) =& \n",
      "\\ln \\left(  \\prod_{n=1}^N p(x_n| z_n, W, \\mu, \\tau)p(Z)p(W|\\alpha)p(\\alpha)p(\\mu)p(\\tau) \\right) \\\\\n",
      "=& \\ln \\left( \\prod_{n=1}^N \\left[ \\left( \\frac{\\tau}{2 \\pi} \\right) ^{D/2} \\exp \\left( - \\frac{\\tau}{2} \\parallel x_t-Wz_n-\\mu \\parallel ^2  \\right) \\right]\n",
      "\\prod_{n=1}^N \\left[  \\left( \\frac{1}{2 \\pi} \\right)^{q/2} \\exp \\left( - \\frac{1}{2} z_n^\\top z_n \\right) \\right] \\\\\n",
      "\\prod_{i=1}^q \\left[ \\left( \\frac{\\alpha_i}{2 \\pi} \\right)^{D/2} \\exp \\left( -\\frac{\\alpha_i}{2}  w_i^\\top w_i \\right) \\right]\n",
      "\\prod_{i=1}^q \\left[ \\frac{1}{\\Gamma(a_\\alpha)} b_\\alpha^{a_\\alpha} \\alpha_i^{a_\\alpha-1} e^{-b_\\alpha \\: \\alpha_i} \\right]\\\\ \n",
      "\\left[ \\left(\\frac{ \\beta}{2\\pi} \\right)^{D/2} \\exp \\left( - \\frac{\\beta}{2} \\mu^\\top \\mu \\right) \\right]\n",
      "\\left[ \\frac{1}{\\Gamma(c_\\tau)} d_\\tau^{c_\\tau} \\tau^{c_\\tau-1} e^{-d_\\tau \\: \\tau} \\right] \\right) \n",
      "\\\\\n",
      "=& \\frac{ND}{2} \\ln \\frac{\\tau}{2 \\pi} + \\sum_{n=1}^N \\left[ - \\frac{\\tau}{2} \\parallel x_n-Wz_n-\\mu \\parallel ^2 \\right]\n",
      "+ \\frac{Nq}{2} \\ln \\frac{1}{2 \\pi} + \\sum_{n=1}^N \\left[ - \\frac{1}{2} z_n^\\top z_n \\right] \\\\\n",
      "&+ \\sum_{i=1}^q \\left[ \\frac{D}{2} \\ln \\frac{\\alpha_i}{2 \\pi} \\right] + \\sum_{i=1}^q \\left[ - \\frac{\\alpha_i}{2} w_i^\\top w_i \\right]\n",
      "+ \\: q \\ln \\frac{ b_\\alpha^{a_\\alpha}}{\\Gamma(a_\\alpha)} + \\sum_{i=1}^q \\left[ (a_\\alpha -1) \\ln \\alpha_i - b_\\alpha \\alpha_i \\right] \\\\\n",
      "&+ \\frac{D}{2} \\ln \\frac{\\beta}{2 \\pi} - \\frac{\\beta}{2} \\mu^\\top\\mu - \\ln \\Gamma(c_\\tau) + \\ln d_\\tau^{c_\\tau} + (c_\\tau-1)\\ln \\tau - d_\\tau \\tau\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bayesian learning results in a posterior distribution over the parameters. If we wish to use $\\ln P(\\Theta,D)$ to assess convergence we have no choice but to use some point estimate of the parameters from the approximated posterior $Q$ to evaluate  $\\ln P(\\Theta,D)$ (we cannot plug in the distribution). However, we consider convergence as $Q$ being as similar to the real posterior $P(\\Theta|D)$ as possible. There is no guarantee that if $Q$ becomes more similar to $P(\\Theta|D)$, that the used point estimate of $Q$ results in a higher value when evaluating $\\ln P(\\Theta,D)$. Therefore $\\ln P(\\Theta,D)$ is not a suitable option to assess convergence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4. The lower bound $\\mathcal{L}(Q)$ (25 points)\n",
      "\n",
      "Derive an expression for the lower bound $\\mathcal{L}(Q)$ of the log-prob $\\ln p(X)$ for Bayesian PCA, making use of the factorization (eq. 12) and the form of the Q-distributions (eq. 16-20) as listed in [Bishop99]. Show your steps. Implement this function.\n",
      "\n",
      "The following result may be useful:\n",
      "\n",
      "For $x \\sim \\Gamma(a,b)$, we have $\\langle \\ln x\\rangle = \\ln b + \\psi(a)$, where $\\psi(a) = \\frac{\\Gamma'(a)}{\\Gamma(a)}$ is the digamma function (which is implemented in numpy.special)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The lower bound can be defined as"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q(\\Theta) \\ln \\dfrac{P(D,\\Theta)}{Q(\\Theta)} \\hspace{2mm} d \\Theta\\\\\n",
      "                =& \\int Q(\\Theta) \\ln P(D,\\Theta) \\hspace{2mm} d \\Theta  - \\int Q(\\Theta) \\ln Q(\\Theta)  \\hspace{2mm} d \\Theta\\\\\n",
      "                = & \\mathbb{E}_Q[ \\ln P(D,\\Theta)] - \\mathbb{E}_Q[\\ln Q(\\Theta)]\\\\\n",
      "                ~\n",
      "               =& \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} P (t_n | x_n, W, \\mu, \\tau) \\right) \\right] \\\\\n",
      "               & + \\mathbb{E}_Q \\left[ \\ln P(X) \\right ] \\\\\n",
      "               & + \\mathbb{E}_Q \\left[\\ln  P(W | \\alpha ) \\right] \\\\\n",
      "               & + \\mathbb{E}_Q \\left[ \\ln P(\\alpha) \\right]\\\\\n",
      "               &+ \\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln Q(X) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln Q(W) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( Q(\\alpha) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln Q(\\mu) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( Q(\\tau) \\right) \\right]\\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we can treat $ P (t_n | x_n, W, \\mu, \\tau)  $  as a constant because $t$ is observed. So $\\mathcal{L}(Q)$ becomes\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n|0, I_q) \\right) \\right]              \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} (\\dfrac{\\alpha_i}{2 \\pi})^{d/2}  \\exp( - \\dfrac{1}{2} \\alpha_i ||w_i||^2 ) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma ( \\alpha_i| a_{\\alpha}, b_{\\alpha}) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu|0, \\beta^{-1} I) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau|c_\\tau, d_\\tau ) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n| m_x^{(n)}, \\Sigma_x) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{k=1}^{d}  \\mathcal{N}(\\widetilde{w}_k | m_w^{(k)}, \\Sigma_w) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma(\\alpha_i| \\widetilde{a}_\\alpha, \\widetilde{b}_{\\alpha i} ) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu | m_\\mu, \\Sigma_\\mu) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau | \\widetilde{a}_\\tau, \\widetilde{b}_\\tau ) \\right) \\right] \\\\\n",
      "                & + \\text{const}\n",
      "\\end{align}\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have evaluated the ten integrals corresponding to these ten expectations analytically. \n",
      "\n",
      "Below we give the derivation for each of them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      " \\mathbb{E}_Q \\left[ \\ln  P(X) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n|0, I_q) \\right) \\right]   \\\\\n",
      "= & \\int Q(\\Theta) \\sum_{n=1}^N \\ln \\left( \\mathcal{N}(x_n | 0, I_q \\right) d \\Theta\\\\\n",
      "= & \\sum_{n=1}^N \\int Q(x_n) \\ln \\left( \\mathcal{N} (x_n | 0, I_q \\right) d x_n \\\\\n",
      "= & \\sum_{n=1}^N \\int Q(x_n)\\left[ - \\dfrac{1}{2} (x_n - 0)^{T} I_q^{-1} (x_n - 0) - \\dfrac{q}{2} \\ln ( 2 \\pi)\n",
      "            - \\dfrac{1}{2} \\ln (\\text{det}(I_q)) \\right] d x_n \\\\\n",
      "= & -\\dfrac{1}{2} \\sum_{n=1}^N \\int Q(x_n) x_n^{T}x_n d x_n - \\dfrac{q}{2} \\ln (2 \\pi) \\sum_{n=1}^N \\int Q(x_n) d x_n -\n",
      "                    \\dfrac{1}{2} \\ln (\\text{det} (I_q)) \\sum_{n=1}^N \\int Q(x_n ) d x_n \\\\\n",
      "= & - \\dfrac{1}{2} \\sum_{n=1}^{N} \\int Q(x_n) x_n^{T}x_n d x_n + \\text{const} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Rule 355 of <a href=\"http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\"> the Matrix Cookbook</a> says that $ \\mathbb{E}_P[x^{T}Ax] = \\text{Tr}(A \\Sigma) + m^{T} A m $ where $m$ and $\\Sigma$ are the mean and covariance of $P$, so\n",
      "\n",
      "\\begin{align}\n",
      " \\mathbb{E}_Q \\left[ \\ln  P(X) \\right]   = & \n",
      "     -\\dfrac{1}{2} \\left( N \\cdot \\text{Tr}(\\Sigma_x) + \\sum_{n=1}^N {m_x^{(n)}}^{T}  {m_x^{(n)}} \\right) + \\text{const}\\\\\n",
      "     = & -\\dfrac{N}{2} \\text{Tr}(\\Sigma_x) -\\dfrac{1}{2}  \\sum_{n=1}^N {m_x^{(n)}}^{T}  {m_x^{(n)}} + \\text{const}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q [\\ln P(W|\\alpha)] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} (\\dfrac{\\alpha_i}{2 \\pi})^{d/2}  \\exp( - \\dfrac{1}{2} \\alpha_i ||w_i||^2 ) \\right) \\right]\\\\\n",
      "=& \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) \\int Q(\\alpha_i) \\left[ \\int Q(w_i) \\left( \\ln (\\alpha_i) - \\dfrac{1}{2} \\alpha_i w_i^{T}w_i \\right)  d w_i \\right] d \\alpha_i \\\\\n",
      "=&  \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) - \\dfrac{1}{2} \\int Q(\\alpha_i) \\left( \\ln(\\alpha_i) - \\alpha_i \\left[ \\int Q(w_i) w_i^{T} w_i  d w_i\\right] \\right)  d \\alpha_i \\\\\n",
      "\\end{align}\n",
      "\n",
      "Again we can use rule 355 from the Matrix Cookbook.\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q [\\ln P(W|\\alpha)] =& \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) (- \n",
      "    \\dfrac{1}{2}) \\int Q(\\alpha_i) \n",
      "        \\left( \\ln(\\alpha_i) - \\alpha_i( \\text{Tr}(\\alpha_i I) \\right) d \\alpha_i \\\\\n",
      "= & \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) (- \n",
      "    \\dfrac{1}{2}) \\int Q(\\alpha_i)      \n",
      "        \\left( \\ln(\\alpha_i) - d \\alpha_i^{2} \\right)\n",
      "        d \\alpha_i \\\\\n",
      "= & \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi)( - \n",
      "    \\dfrac{1}{2})\n",
      "    \\left( \n",
      "        \\int Q(\\alpha_i)  \\ln(\\alpha_i) d \\alpha_i\n",
      "        -\n",
      "        d \\int Q(\\alpha_i) \\alpha_i^2 d \\alpha_i\n",
      "        \\right)        \\\\\n",
      "\\end{align}\n",
      "\n",
      "We can make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;\n",
      "\n",
      "and we can make use of the fact that $\\mathbb{E}_P[x^2] = \\dfrac{(a+1)a}{b^2}$ if $P(x) = \\Gamma(x|a,b)$:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q [\\ln P(W|\\alpha)] = &  \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) (- \n",
      "    \\dfrac{1}{2})\n",
      "        \\left(\n",
      "            (\\psi(\\widetilde{a}_\\alpha) - \\ln( \\widetilde{b}_{\\alpha i} ) )\n",
      "            - d \\dfrac{ ( \\widetilde{a}_\\alpha + 1 ) \\widetilde{a}_\\alpha}\n",
      "                        { \\widetilde{b}_{\\alpha i}^{2} }\n",
      "         \\right)\\\\\n",
      "= &  \\dfrac{d}{4} \\ln( 2 \\pi) \\sum_{i=1}^q \n",
      "            \\left( \n",
      "              (  \\psi(\\widetilde{a}_\\alpha) - \\ln ( \\widetilde{b}_{\\alpha i}) )-\n",
      "                    \\dfrac{d ( \\widetilde{a}_{\\alpha} + 1 ) \\widetilde{a}_\\alpha }\n",
      "                            { \\widetilde{b}^{2}_{\\alpha i} }\n",
      "              \\right)            \\\\\n",
      "= & \\dfrac{d}{4} \\ln(2 \\pi) \\left( q \\cdot \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q \\ln (\\widetilde{b}_{\\alpha i}) \n",
      "                               - \\sum_{i=1}^{q} \\left( \\dfrac{d (\\widetilde{a}_\\alpha^2 + \\widetilde{a}_\\alpha) } {\\widetilde{b}_{\\alpha i}^2 } \\right) \\right)               \n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\alpha) \\right] =& \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma ( \\alpha_i| a_{\\alpha}, b_{\\alpha}) \\right) \\right]\\\\\n",
      "= & \\int Q(\\alpha) \\sum_{i=1}^q \\ln( \\Gamma(\\alpha_i|a_\\alpha, b_\\alpha)) d \\alpha \\\\\n",
      "=& \\sum_{i=1}^q  \\int Q(\\alpha_i) \\ln (\\Gamma (\\alpha_i| a_\\alpha , b_\\alpha)) d\\alpha_i \\\\\n",
      "= & \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left( \\dfrac{1}{\\Gamma(a_\\alpha)} \\right) d \\alpha_i +\n",
      "   \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln (b_{\\alpha}^{a_\\alpha} )d \\alpha_i  + \\\\\n",
      "    & \\sum_{i=1}^q \\int Q(\\alpha_i)  \\ln \\left( \\alpha_{i}^{a_\\alpha - 1} \\right) d \\alpha_i + \n",
      "    \\sum_{i=1}^q \\int Q(\\alpha_i) (- b_\\alpha \\alpha_i ) d \\alpha_i \\\\\n",
      "= & \\text{const} +  \n",
      "         (a_\\alpha - 1) \\sum_{i=1}^q   \\int Q(\\alpha_i)  \\ln \\left( \\alpha_{i} \\right) d \\alpha_i - \n",
      "         b_\\alpha \\sum_{i=1}^q \\int Q(\\alpha_i)  \\alpha_i  d \\alpha_i \\\\\n",
      "\\end{align}\n",
      "\n",
      "We make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;  \n",
      "and \n",
      "of the fact that $\\mathbb{E}_P[x] = \\dfrac{a}{b}$ if $P(x) = \\Gamma(x|a,b)$.\n",
      "Then:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\alpha) \\right] =& \n",
      "     \\cdot  (a_\\alpha - 1) \\cdot  \\sum_{i=1}^q   \\left(\\psi(\\widetilde{a}_\\alpha) - \\ln(\\widetilde{b}_{\\alpha, i})\\right)\n",
      "     -\n",
      "      b_\\alpha \\sum_{i=1}^q  \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha, i}} \\right)\n",
      "     + \\text{const} \\\\\n",
      "     = &    (a_\\alpha - 1) \\left(  q \\cdot  \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q  \\left( \\ln(\\widetilde{b}_{\\alpha, i})\\right) \\right)\n",
      "     -\n",
      "      b_\\alpha \\sum_{i=1}^q  \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha, i}} \\right)\n",
      "     + \\text{const} \n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu|0, \\beta^{-1} I) \\right) \\right]\\\\\n",
      "= & \\mathbb{E}_Q \\left[ - \\dfrac{d}{2}  \\ln ( 2 \\pi)  - \\dfrac{1}{2} \\mu^{T} \\beta \\mu  - \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) \\right]\\\\\n",
      "= & -  \\int Q(\\Theta)  \\dfrac{d}{2} \\ln \\left( {2\\pi} \\right) + \\left( \\dfrac{\\beta}{2} \\mu^{T}\\mu \\right)   + \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) d \\Theta \\\\\n",
      "= & - \\int Q(\\mu)  \\dfrac{d}{2} \\ln \\left( {2\\pi} \\right) + \\left( \\dfrac{\\beta}{2} \\mu^{T}\\mu \\right)   + \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) d \\mu \\\\\n",
      "= & - \\dfrac{d}{2} \\ln \\left( {2\\pi} \\right)  \\int Q(\\mu)  d \\mu - \n",
      "        \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) \\int Q(\\mu) d \\mu\n",
      "        - \\int Q(\\mu) \\left( \\dfrac{\\beta}{2} \\mu^{T}\\mu \\right) d \\mu\\\\\n",
      "= &  - \\text{const} - \\dfrac{\\beta}{2} \\int Q(\\mu) \\mu^{T}\\mu d \\mu \\\\\n",
      "= & - \\dfrac{\\beta}{2} \\int Q(\\mu) \\mu^{T}\\mu d \\mu + \\text{const} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Again we make use of rule 355 of the Matrix Cookbook which states that $ \\mathbb{E}_P[x^{T}Ax] = \\text{Tr}(A \\Sigma) + m^{T} A m $ where $m$ and $\\Sigma$ are the mean and covariance of $P$, so\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = & - \\dfrac{\\beta}{2} \\left( \\text{Tr}(\\Sigma_\\mu) + m_\\mu^{T} m_\\mu \\right) +  \\text{const} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Because $\\mu$ is $d$-dimensional and $\\Sigma_\\mu$ is defined as $(\\beta + N\\langle \\tau \\rangle)^{-1} I$\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = &- \\dfrac{\\beta}{2} \\left( \\dfrac{d}{\\beta + N \\langle \\tau \\rangle } + m_\\mu^{T} m_\\mu \\right) +  \\text{const}\\\\\n",
      "\\end{align}\n",
      "\n",
      "Because $Q(\\tau) = \\Gamma (\\tau | \\widetilde{a}_\\tau, \\widetilde{b}_\\tau)$we end up with\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = &\n",
      "    - \\dfrac{\\beta}{2} \n",
      "    \\left( \\dfrac{d}{\\beta + N ({ \\widetilde{a}_\\tau } / { \\widetilde{b}_\\tau} ) }\n",
      "+ m_\\mu^{T} m_\\mu  \\right) +  \\text{const}\\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau|c_\\tau, d_\\tau ) \\right) \\right] \\\\\n",
      "= & \\mathbb{E}\\left[ \\ln \\left( \\dfrac{1}{\\Gamma (c_\\tau)} d_\\tau^{c_\\tau} \\tau^{c_\\tau -1} e^{-c_\\tau \\cdot \\tau} \\right) \\right]\\\\\n",
      "= & \\int Q(\\Theta) \\left( - \\ln(\\Gamma (c_\\tau)) + c_\\tau \\ln(d_\\tau) + (c_\\tau - 1) \\ln (\\tau) - d_\\tau \\cdot \\tau \\right) \\hspace{3mm} d \\Theta \\\\\n",
      "= & - \\int Q(\\tau) \\ln(\\Gamma (c_\\tau)) \\hspace{3mm} d \\tau \n",
      "    + \\int Q(\\tau) c_\\tau \\ln(d_\\tau)  \\hspace{3mm} d \\tau  \n",
      "    + \\int Q(\\tau) (c_\\tau - 1) \\ln (\\tau) \\hspace{3mm} d \\tau  \n",
      "    - \\int Q(\\tau) d_\\tau \\cdot \\tau \\hspace{3mm} d \\tau    \\\\\n",
      "= &   (c_\\tau - 1) \\int Q(\\tau)  \\ln (\\tau) \\hspace{3mm} d \\tau  \n",
      "    -   d_\\tau \\int Q(\\tau)  \\tau \\hspace{3mm} d \\tau   \n",
      "     + \\text{const} \\\\   \n",
      "\\end{align}\n",
      "\n",
      "Again make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;  \n",
      "and \n",
      "of the fact that $\\mathbb{E}_P[x] = \\dfrac{a}{b}$ if $P(x) = \\Gamma(x|a,b)$.\n",
      "Then:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] \n",
      "= &   (c_\\tau - 1) \\left( \\psi(\\widetilde{a}_\\tau) - \\ln(\\widetilde{b}_\\tau )\\right)\n",
      "    -   d_\\tau  \\dfrac{\\widetilde{a}_\\tau }{\\widetilde{b}_\\tau}  \n",
      "     + \\text{const} \\\\   \n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "6"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln Q(X) \\right] = &  \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n| m_x^{(n)}, \\Sigma_x) \\right) \\right] \\\\\n",
      "= & \\sum_{n=1}^N \\mathbb{E}_Q \\left[ \n",
      "                                    \\ln \\left( \\dfrac{1}{(2 \\pi)^{(q/2)}} \\dfrac{1}{\\text{det}(\\Sigma_x)^{(1/2)}} \\right)\n",
      "                                  + \\left( -\\dfrac{1}{2} (x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) \\right) \n",
      "                                  \\right] \\\\\n",
      "= & N \\cdot  \\ln \\left( \\dfrac{1}{(2 \\pi)^{(q/2)}} \\right)\n",
      "        +  N \\cdot \\left( \\dfrac{1}{\\text{det}(\\Sigma_x)^{(1/2)}} \\right)\n",
      "        - \\sum_{n=1}^N \\dfrac{1}{2} \\int Q(x_n)  (x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) \\hspace{3mm} d x_n\\\\\n",
      "= &  - \\sum_{n=1}^N \\dfrac{1}{2} \\int Q(x_n)  (x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) \\hspace{3mm} d x_n + \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "We recognize $(x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) $ as the square of the Mahalanobis distance $\\vartriangle$.\n",
      "\n",
      "As stated  for example <a href=\"http://stat.ethz.ch/education/semesters/ss2012/ams/slides/v7.2.pdf\"> here</a> , the expectation of the squared Mahalanobis distance pertaining to $x$ and $m$ under a multivariate Gaussian  $\\mathcal{N}(x|m, \\Sigma)$  corresponds to a Chi-square distribution $\\chi^2_{n}$ with the dimensionality of the space of the distribution as the degrees of freedom $n$. This $\\chi^2_n$ distribution has as expected value also $n$. \n",
      "\n",
      "So: $\\mathbb{E}_P[\\vartriangle^2] =  \\text{dimensionality}(x \\sim P(x))$\n",
      "\n",
      "\n",
      "This can also be derived by using rule 357 from the Matrix Cookbook which states that $\\mathbb{E}_P[(x - m')^{T} A (x-m')] = (m-m')^{T} A (m-m') + \\text{Tr}(A \\Sigma)$, if $P(x) = \\mathcal{N}(x | m, \\Sigma)$. We would get $\\text{Tr}(I_q)$.\n",
      "\n",
      "Therefore we end up with\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln Q(X) \\right] = & -  \\dfrac{N}{2} q + \\text{const}\\\\\n",
      "= & \\text{const}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "7"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q\\left[\\ln Q(W) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{k=1}^{d}  \\mathcal{N}(\\widetilde{w}_k | m_w^{(k)}, \\Sigma_w) \\right) \\right]\\\\\n",
      "= &  - \\sum_{k=1}^d \\dfrac{1}{2} \\int Q(\\widetilde{w}_k)  (\\widetilde{w}_k - m_w^{(k)} )^{T}\\Sigma_w^{-1} (\\widetilde{w}_k - m_w^{(k)}) \\hspace{3mm} d \\widetilde{w}_k + \\text{const} \\\\\n",
      "= & - \\dfrac{d}{2} q + \\text{const}\\\\\n",
      "= & \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Where we have followed the same reasoning as for $\\mathbb{E}_Q[\\ln Q(X)]$ above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " <font color='red' size=\"3\" >\n",
      " \n",
      "     Wat betreft 7 en 8 </br></br>\n",
      "     Is <b>q</b> echt constant ?\n",
      " </font>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "8"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      " \\mathbb{E}_Q\\left[ \\ln Q(\\alpha) \\right] =&  \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma(\\alpha_i| \\widetilde{a}_\\alpha, \\widetilde{b}_{\\alpha i} ) \\right) \\right] \\\\\n",
      " =& \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left( \n",
      "     \\dfrac{1}{\\Gamma(\\widetilde{a}_\\alpha)} \\cdot\n",
      "     \\widetilde{b}^{\\widetilde{a}_\\alpha}_{\\alpha i} \\cdot\n",
      "     \\alpha_i^{(\\widetilde{a}_\\alpha - 1) } \\cdot\n",
      "     e^{- \\widetilde{b}_{\\alpha i} \\alpha_i} \n",
      "     \\right) \\hspace{3mm} d \\alpha_i \\\\\n",
      "=& \\sum_{i=1}^q \\int Q(\\alpha_i)  \\dfrac{1}{\\Gamma(\\widetilde{a}_\\alpha)} \\hspace{3mm} d \\alpha_i +\n",
      "   \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left(   \\widetilde{b}^{\\widetilde{a}_\\alpha}_{\\alpha i} \\right) \\hspace{3mm} d \\alpha_i  +\n",
      "   \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left(   \\alpha_i^{(\\widetilde{a}_\\alpha - 1) }   \\right) \\hspace{3mm} d \\alpha_i  -\n",
      "    \\sum_{i=1}^q \\int Q(\\alpha_i) \\widetilde{b}_{\\alpha i} \\alpha_i \\hspace{3mm} d \\alpha_i \\\\\n",
      "=& (\\widetilde{a}_\\alpha - 1)  \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln   \\alpha_i    \\hspace{3mm} d \\alpha_i -\n",
      "    \\sum_{i=1}^q \\widetilde{b}_{\\alpha i}  \\int Q(\\alpha_i) \\alpha_i \\hspace{3mm} d \\alpha_i \n",
      "    + \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Again we make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;  \n",
      "and \n",
      "of the fact that $\\mathbb{E}_P[x] = \\dfrac{a}{b}$ if $P(x) = \\Gamma(x|a,b)$.\n",
      "Then:\n",
      "\n",
      "\\begin{align}\n",
      " \\mathbb{E}_Q\\left[ \\ln Q(\\alpha) \\right] \n",
      "=&    (\\widetilde{a}_\\alpha - 1)  \\sum_{i=1}^q \\left( \\psi(\\widetilde{a}_\\alpha) - \\ln(\\widetilde{b}_{\\alpha i}) \\right) -\n",
      "      \\sum_{i=1}^q  \\widetilde{b}_{\\alpha i} \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha i}} \\right)\n",
      "      + \\text{const} \\\\\n",
      "=&    (\\widetilde{a}_\\alpha - 1)   \\left( q  \\cdot \\psi(\\widetilde{a}_\\alpha) - \\left(  \\sum_{i=1}^q \\ln(\\widetilde{b}_{\\alpha i}) \\right) \\right)  -\n",
      "        q \\cdot \\widetilde{a}_\\alpha\n",
      "         + \\text{const} \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "9"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_q [ \\ln Q(\\mu)] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu | m_\\mu, \\Sigma_\\mu) \\right) \\right] \\\\\n",
      "=& \\int Q(\\mu) \\left[ \\left( -\\dfrac{1}{2} (\\mu - m_\\mu)^{T} \\Sigma_\\mu (\\mu - m_\\mu) \\right) -\n",
      "                    \\left( \\dfrac{d}{2} \\ln (2 \\pi) \\right) -\n",
      "                    \\left( \\dfrac{1}{2} \\ln (\\text{det}(\\Sigma_\\mu)) \\right)\n",
      "                \\right] d \\mu\\\\\n",
      "=& - \\dfrac{1}{2} \\int Q(\\mu) (\\mu - m_\\mu)^{T} \\Sigma_\\mu (\\mu - m_\\mu) d \\mu + \\text{const} \\\\\n",
      "=& - \\dfrac{d}{2} + \\text{const} \\\\\n",
      "=& \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Where we have followed the same reasoning as for $\\mathbb{E}_Q[\\ln Q(X)]$ above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "10"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q[\\ln Q(\\tau)] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau | \\widetilde{a}_\\tau, \\widetilde{b}_\\tau ) \\right) \\right]\\\\\n",
      "= & \\left(\\psi(\\widetilde{a}_\\tau) - \\ln( \\widetilde{b}_\\tau) \\right) - \\widetilde{b}_\\tau \\left( \\dfrac{ \\widetilde{a}_\\tau}{\\widetilde{b}_\\tau} \\right) + \\text{const}\\\\\n",
      "= & \\left(\\psi(\\widetilde{a}_\\tau) - \\ln( \\widetilde{b}_\\tau) \\right) - \\widetilde{a}_\\tau + \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Where we have followed the same reasoning as for e.g.  $\\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] $ above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The whole lower bound $\\mathcal{L}(Q)$ can now be expressed as:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& -\\dfrac{N}{2} \\text{Tr}(\\Sigma_x) -\\dfrac{1}{2}  \\sum_{n=1}^N {m_x^{(n)}}^{T}  {m_x^{(n)}} \\\\\n",
      "               & + \\dfrac{d}{4} \\ln(2 \\pi) \\left( q \\cdot \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q \\ln (\\widetilde{b}_{\\alpha i}) \n",
      "                               - \\sum_{i=1}^{q} \\left( \\dfrac{d (\\widetilde{a}_\\alpha^2 + \\widetilde{a}_\\alpha) } {\\widetilde{b}_{\\alpha i}^2 } \\right) \\right) \\\\\n",
      "              & +    (a_\\alpha - 1) \\left(  q \\cdot  \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q  \\left( \\ln(\\widetilde{b}_{\\alpha, i})\\right) \\right)\n",
      "                    - b_\\alpha \\sum_{i=1}^q  \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha, i}} \\right) \\\\\n",
      "              & - \\dfrac{\\beta}{2}     \\left( \\dfrac{d}{\\beta + N ({ \\widetilde{a}_\\tau } / { \\widetilde{b}_\\tau} ) } + m_\\mu^{T} m_\\mu  \\right)\\\\\n",
      "              & + (c_\\tau - 1) \\left( \\psi(\\widetilde{a}_\\tau) - \\ln(\\widetilde{b}_\\tau )\\right)     -   d_\\tau  \\dfrac{\\widetilde{a}_\\tau }{\\widetilde{b}_\\tau} \\\\\n",
      "              & - (\\widetilde{a}_\\alpha - 1)   \\left( q  \\cdot \\psi(\\widetilde{a}_\\alpha) - \\left(  \\sum_{i=1}^q \\ln(\\widetilde{b}_{\\alpha i}) \\right) \\right)  +\n",
      "        q \\cdot \\widetilde{a}_\\alpha \\\\\n",
      "              & - \\left(\\psi(\\widetilde{a}_\\tau) - \\ln( \\widetilde{b}_\\tau) \\right) + \\widetilde{a}_\\tau\\\\\n",
      "              & + \\text{const}\n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 5. Optimize variational parameters (50 points)\n",
      "Implement the update equations for the Q-distributions, in the __update_XXX methods. Each update function should re-estimate the variational parameters of the Q-distribution corresponding to one group of variables (i.e. either $Z$, $\\mu$, $W$, $\\alpha$ or $\\tau$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6. Learning algorithm (10 points)\n",
      "Implement the learning algorithm described in [Bishop99], i.e. iteratively optimize each of the Q-distributions holding the others fixed.\n",
      "\n",
      "What would be a good way to track convergence of the algorithm? Implement your suggestion.\n",
      "\n",
      "Test the algorithm on some test data drawn from a Gaussian with different variances in orthogonal directions. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 7. PCA Representation of MNIST (10 points)\n",
      "\n",
      "Download the MNIST dataset from here http://deeplearning.net/tutorial/gettingstarted.html (the page contains python code for loading the data). Run your algorithm on (part of) this dataset, and visualize the results.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle, gzip, numpy\n",
      "\n",
      "# Load the dataset\n",
      "f = gzip.open('../mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#An image is represented as numpy 1-dimensional array of 784 (28 x 28) float values between 0 and 1 \n",
      "dimIm = 28\n",
      "nrOriginalDimensions = dimIm**2\n",
      "\n",
      "[_, nrImsTestSet] =  shape(test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def showExamples(W, nrExamples=3):\n",
      "    \n",
      "    Wpi = np.linalg.pinv(W)  \n",
      "    numbers = []\n",
      "    \n",
      "    for i in range(nrExamples):\n",
      "        randomIndex =  randint(0,nrImsTestSet-1)    \n",
      "        originalIm = test_set[0][randomIndex]\n",
      "        originalLabel = test_set[1][randomIndex]\n",
      "        \n",
      "        imMatrix = originalIm.reshape((dimIm, dimIm))    \n",
      "        matshow(imMatrix,cmap=cm.gray)    \n",
      "        \n",
      "        imReduced = np.dot( W.T  , originalIm)       \n",
      "        \n",
      "        imRecov = np.dot(Wpi.T, imReduced)\n",
      "        \n",
      "        imRecovMatrix = imRecov.reshape((dimIm, dimIm))\n",
      "        matshow(imRecovMatrix,cmap=cm.gray)\n",
      "        \n",
      "        numbers.append(originalLabel)\n",
      "        \n",
      "    print \"Showing numbers %s\" % numbers\n",
      "    \n",
      "    #print shape(imReduced)\n",
      "    #print type(imReduced)\n",
      "    #print shape(W)\n",
      "    #print shape(Wpi)\n",
      "    #print shape(imRecov)\n",
      "\n",
      "\n",
      "nrReducedDimensions = 500\n",
      "W = np.random.randn(nrOriginalDimensions,nrReducedDimensions)\n",
      "showExamples(W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Showing numbers [6, 2, 1]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFqHwgEUoPGARCg9YhMIDFvk/bAIRK2nOxfwAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x4b69f10>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x3215410>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDvk/ZDPn+m8s7K8AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x2f938d0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x2ce1e10>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8IAh/wuDfH5ckTWUkAAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x31ff8d0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAVPZBAWPJFBWPBEBvl/A8b/DoguSrEAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x2e58910>"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}