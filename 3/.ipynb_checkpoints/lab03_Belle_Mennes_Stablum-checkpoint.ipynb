{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lab 3: Bayesian PCA\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction\n",
      "\n",
      "In this lab assignment, we will implement a variational algorithm for Bayesian PCA. Unlike regular PCA based on maximization of retained variance or minimization of projection error (see Bishop, 12.1.1 and 12.1.2), probabilistic PCA defines a proper density model over observed and latent variables. We will work with a fully Bayesian model this time, which is to say that we will put priors on our parameters and will be interested in learning the posterior over those parameters. Bayesian methods are very elegant, but require a shift in mindset: we are no longer looking for a point estimate of the parameters (as in maximum likelihood or MAP), but for a full posterior distribution.\n",
      "\n",
      "The integrals involved in a Bayesian analysis are usually analytically intractable, so that we must resort to approximations. In this lab assignment, we will implement the variational method described in Bishop99. Chapters 10 and 12 of the PRML book contain additional material that may be useful when doing this exercise.\n",
      "\n",
      "* [Bishop99] Variational Principal Components, C. Bishop, ICANN 1999 - http://research.microsoft.com/pubs/67241/bishop-vpca-icann-99.pdf\n",
      "\n",
      "Below, you will find some code to get you started."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import scipy.special as sp\n",
      "\n",
      "class BayesianPCA(object):\n",
      "    \n",
      "    def __init__(self, d, N, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        self.d = d # number of dimensions\n",
      "        self.N = N # number of data points\n",
      "        \n",
      "        # Hyperparameters\n",
      "        self.a_alpha = a_alpha\n",
      "        self.b_alpha = b_alpha\n",
      "        self.a_tau = a_tau\n",
      "        self.b_tau = b_tau\n",
      "        self.beta = beta\n",
      "\n",
      "        # Variational parameters\n",
      "        self.means_z = np.random.randn(d, N) # called x in bishop99\n",
      "        self.sigma_z = np.random.randn(d, d)\n",
      "        self.mean_mu = np.random.randn(d, 1)\n",
      "        self.sigma_mu = np.random.randn(d, d)\n",
      "        self.means_w = np.random.randn(d, d)\n",
      "        self.sigma_w = np.random.randn(d, d)\n",
      "        self.a_alpha_tilde = np.abs(np.random.randn(1))\n",
      "        self.bs_alpha_tilde = np.abs(np.random.randn(d, 1))\n",
      "        self.a_tau_tilde = np.abs(np.random.randn(1))\n",
      "        self.b_tau_tilde = np.abs(np.random.randn(1))\n",
      "    \n",
      "    def __update_z(self, X):\n",
      "        #Update means of z\n",
      "        exp_tau = self.a_tau_tilde / self.b_tau_tilde\n",
      "        exp_Wt = np.transpose(self.means_w)\n",
      "        exp_mu = self.mean_mu\n",
      "        self.means_z = np.dot( np.dot( np.dot(exp_tau, self.sigma_z), exp_Wt), (X-exp_mu) )\n",
      "        \n",
      "        #update sigma of z\n",
      "        exp_WtW = numpy.trace(self.sigma_w) + np.dot(np.transpose(self.means_w), self.means_w)\n",
      "        print expWtW\n",
      "        # sigma w = identity+ exp_tau*expWtW inverse\n",
      "        self.sigma_z = numpy.identity(2)\n",
      "    \n",
      "    def __update_mu(self):\n",
      "        pass\n",
      "    \n",
      "    def __update_w(self, X):\n",
      "        pass\n",
      "    \n",
      "    def __update_alpha(self):\n",
      "        pass\n",
      "\n",
      "    def __update_tau(self, X):\n",
      "        pass\n",
      "\n",
      "    def L(self, X):\n",
      "        # p(X)\n",
      "        p_x = - (self.N / 2 ) * np.trace(self.sigma_z) \n",
      "        for i in range(self.N):\n",
      "            p_x += np.dot(self.sigma_z[:,i], self.sigma_z[:,i].T)\n",
      "        print \"P(X):\", p_x\n",
      "        \n",
      "        # p(W|\\alpha)\n",
      "        p_w_alpha = (self.d / float(4) ) * np.log(2 * np.pi)\n",
      "        p_w_alpha_temp = (self.d * (sp.psi(self.a_alpha_tilde[0] )) )  \n",
      "        p_w_alpha_temp -= sum( map ( lambda x:  np.log( x ), self.bs_alpha_tilde))               \n",
      "        p_w_alpha_temp -= sum (map ( lambda x:  ((self.d * (self.a_alpha_tilde[0]**2 + self.a_alpha_tilde[0] )) / (x**2)),\n",
      "                                    self.bs_alpha_tilde))         \n",
      "        p_w_alpha = p_w_alpha * p_w_alpha_temp        \n",
      "        print \"P(W|\\\\alpha):\", p_w_alpha\n",
      "            \n",
      "        # P(\\alpha)        \n",
      "        p_alpha = (self.a_alpha - 1) * (d * sp.psi(self.a_alpha_tilde[0]) -  np.sum(map(lambda x: np.log(x),  self.bs_alpha_tilde)))\n",
      "        p_alpha -= self.b_alpha * np.sum(map(lambda x: (self.a_alpha_tilde[0] / float( x)),  self.bs_alpha_tilde))\n",
      "        print \"P(\\\\alpha):\", p_alpha\n",
      "        \n",
      "        \n",
      "        # P(\\mu)\n",
      "        p_mu = - (self.beta / float(2)) *  ( \\\n",
      "                    ( d / float(self.beta + self.N*(self.a_alpha_tilde[0] / float(self.a_alpha_tilde[0])))) +\n",
      "                    (np.dot(self.mean_mu.T, self.mean_mu)[0][0]) )        \n",
      "        print \"P(\\\\mu):\", p_mu\n",
      "        \n",
      "        # P(\\tau)\n",
      "        # Note: c_tau and d_tau correspond to a_tau_tilde, b_tau_tilde, respectively        \n",
      "        p_tau = (self.a_tau_tilde[0] - 1) * ( sp.psi(self.a_tau_tilde[0]) - np.log(self.b_tau_tilde[0]))\n",
      "        p_tau -= self.a_tau_tilde[0] # since d_tau is b_tau_tilde        \n",
      "        print \"P(\\\\tau):\", p_tau\n",
      "        \n",
      "        #Q(\\alpha)\n",
      "        q_alpha = (self.a_alpha_tilde[0] - 1) * (d * sp.psi(self.a_alpha_tilde[0]) -  np.sum(map(lambda x: np.log(x),  self.bs_alpha_tilde)))\n",
      "        q_alpha -= self.d * self.a_alpha_tilde[0] \n",
      "        print \"Q(\\\\alpha):\", q_alpha\n",
      "        \n",
      "        #Q(\\tau)\n",
      "        q_tau = (sp.psi(self.a_tau_tilde[0]) - np.log(self.b_tau_tilde[0])) - self.a_tau_tilde[0]\n",
      "        print \"Q(\\\\tau):\", q_tau\n",
      "        \n",
      "        L = p_x + p_w_alpha + p_alpha + p_mu + p_tau - q_alpha - q_tau\n",
      "        return L\n",
      "    \n",
      "    \n",
      "    def fit(self, X):\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = 3\n",
      "N = 2\n",
      "\n",
      "pca = BayesianPCA(d, N)\n",
      "X = np.random.randn(d, N)\n",
      "pca.fit(X)\n",
      "\n",
      "pca.L(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P(X): 6.23202016979\n",
        "P(W|\\alpha): -16.6072653815\n",
        "P(\\alpha): 4.40218469418\n",
        "P(\\mu): -0.0337992949155\n",
        "P(\\tau): -0.314666928397\n",
        "Q(\\alpha): -0.566094980196\n",
        "Q(\\tau): -0.799996972469\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "-4.9554347881844887"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "sp.psi(np.abs(np.random.randn(1))[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "-1.1539174538099943"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import display, Math, Latex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1. The Q-distribution (5 points)\n",
      "\n",
      "In variational Bayes, we introduce a distribution $Q(\\Theta)$ over parameters / latent variables in order to make inference tractable. We can think of $Q$ as being an approximation of a certain distribution. What function does $Q$ approximate, $p(D|\\Theta)$, $p(\\Theta|D)$, $p(D, \\Theta)$, $p(\\Theta)$, or $p(D)$, and how do you see that?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function $Q$ approximates the function that is the posterior distribution, $p(\\Theta | D) $.\n",
      "\n",
      "The posterior $P(\\Theta | D)$ is given by multiplying the likelihood by the prior and then normalizing:\n",
      "\n",
      "\\begin{align}\n",
      "P(\\Theta | D) &= \\dfrac{P(D | \\Theta)P( \\Theta)}{P(D)} \\\\\n",
      "              &= \\dfrac{P(D | \\Theta)P( \\Theta)}{\\int P(D | \\Theta)P(\\Theta) \\hspace{1mm} d\\Theta}\n",
      "\\end{align}\n",
      "\n",
      "Why is this useful? \n",
      "For Maximum a Posteriori (MAP) learning, we could use the posterior to get a point estimate of the parameters:\n",
      "\n",
      "\\begin{align}\n",
      "\\widetilde{\\Theta}_{\\text{MAP}}(D)&= \\arg\\max_{\\Theta} P(\\Theta | D) \\\\\n",
      "            & \\propto {P(D | \\Theta)P( \\Theta)}\n",
      "\\end{align}\n",
      "\n",
      "So for MAP learning the normalization (and therefore integration) is not needed. Like Maximum likelihood estimation (MLE) learning, MAP learning learns a point estimate of our parameters.\n",
      "\n",
      "In the Bayesian framework no fixed parameter setting is chosen. The posterior is used to obtain a predictive density (also called the posterior predictive distribution) for a new data point $t^{*}$ with:\n",
      "\n",
      "\\begin{align}\n",
      "P(t^{*} | D) &= \\int P(t^{*} | \\Theta) P(\\Theta | D) \\hspace{2mm} d\\Theta\n",
      "\\end{align}\n",
      "\n",
      "This means that in the Bayesian learning framework we consider all possible parametrizations of our models by incorporating the posterior this way. Each of these is weighted by the evidence for them in $D$.\n",
      "(Note that when predicting, we also return a probability distribution as prediction for $t^{*}$. \n",
      "Retaining all possible parametrizations is in this case of Principal Component Analysis) particularly advantageous for automatically determining an adequate number of principal components given $D$.\n",
      "\n",
      "Note that we need to integrate over all possible parametrizations for this. In practice this can quickly become infeasible for models with many parameters; when using a mixture of models; or when the prior distribution is not of the same functional form as the likelihood distribution. Therefore we use a ''simpler'' distribution $Q(\\Theta)$ to approximate $P(\\Theta|D)$. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2. The mean-field approximation (15 points)\n",
      "\n",
      "Equation 13 from [Bishop99] is a very powerful result: assuming only that $Q(\\Theta)$ factorizes in a certain way (no assumptions on the functional form of the factors $Q_i$!), we get a set of coupled equations for the $Q_i$.\n",
      "\n",
      "However, the expression given in eq. 13 for Q_i contains a small mistake. Starting with the expression for the lower bound $\\mathcal{L}(Q)$, derive the correct expression (and include your derivation). You can proceed as follows: first, substitute the factorization of $Q$ (eq. 12) into the definition of $\\mathcal{L}(Q)$ and separate $\\mathcal{L}(Q)$ into $Q_i$-dependent and $Q_i$-independent terms. At this point, you should be able to spot the expectations $\\langle\\cdot\\rangle_{k \\neq i}$ over the other $Q$-distributions that appear in Bishop's solution (eq. 13). Now, keeping all $Q_k, k \\neq i$ fixed, maximize the expression with respect to $Q_i$. You should be able to spot the form of the optimal $ln Q_i$, from which $Q_i$ can easily be obtained."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is given that the lower bound is \n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q(\\Theta) \\log \\dfrac{P(D, \\Theta)}{Q(\\Theta)}\n",
      "                \\hspace{3mm} d \\Theta  \\\\\n",
      "\\end{align} \n",
      "$$\n",
      "\n",
      "And the factorization is\n",
      "\n",
      "\\begin{equation}\n",
      "Q(\\Theta)= \\prod_{k} Q_k(\\Theta_k)\n",
      "\\end{equation}\n",
      "\n",
      "Plugging in the factorization into the lower bound, we get\n",
      "\n",
      "\\begin{align}                \n",
      "\\mathcal{L}(Q) =&  \\int \n",
      "                     \\prod_{k} Q_k( \\Theta_k) \n",
      "                     \\left[\n",
      "                         \\log P(D,\\Theta) - \\log \\left( \\prod_k Q_k(\\Theta_k) \\right)\n",
      "                     \\right]\n",
      "                 \\hspace{3mm} d \\Theta  \\\\\n",
      "              =& \\int\n",
      "              \\prod_{k} Q_k( \\Theta_k) \n",
      "                     \\left[\n",
      "                         \\log P(D,\\Theta) - \\sum_k \\log Q_k(\\Theta_k)\n",
      "                     \\right]\n",
      "                  \\hspace{3mm} d \\Theta\n",
      "\\end{align}  \n",
      "\n",
      "By denoting a specific factor $Q_i$ separately we arrive at\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q_i( \\Theta_i) \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "                \\left[ \\log P(D, \\Theta) - \\log \\left( \\prod_{j \\neq i}  Q_j(\\Theta_j) \\right) - \\log Q_i (\\Theta_i) \\right] \n",
      "                \\hspace{3mm} d \\Theta  \n",
      "\\end{align}                \n",
      "\n",
      "By taking the integral of $Q_i$ apart from those for $\\{Q_1, \\dots ,Q_{i-1},Q_{i+1}, \\dots ,Q_M \\}$, we get\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "    \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "            \\log \\left( \\prod_{j \\neq i} Q_j(\\Theta_j) \\right)\n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{3mm} - \\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[ \n",
      "            \\int\n",
      "             \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Now, when we keep the $Q_k \\neq Q_{i}$ fixed the following happens, the integral over $\\Theta_{\\setminus i}$  in the third row of the equation for $\\mathcal{L}(Q)$ will become 1, because all $Q_n(\\Theta_n)$ are probability distributions. \n",
      "\n",
      "\\begin{align}\n",
      "\\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[ \n",
      "            \\int\n",
      "             \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "        =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[\n",
      "        \\prod_{k \\neq i}         \n",
      "            \\int\n",
      "              Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{k}\n",
      "        \\right]        \n",
      "        \\hspace{3mm} d \\Theta_{i} \\\\\n",
      "     =&\n",
      "      \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[\n",
      "        \\prod_{k \\neq i}         \n",
      "            1\n",
      "        \\right]        \n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "\\end{align}\n",
      "\n",
      "So we can rewrite the equation as\n",
      "\n",
      "\\begin{align}       \n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "    \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "            \\log \\left( \\prod_{j \\neq i} Q_j(\\Theta_j) \\right)\n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{3mm} - \\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}         \n",
      "\\end{align}\n",
      "\n",
      "\n",
      "Furthermore, when we keep the $Q_k \\neq Q_{i}$ fixed, the $\\Theta_{\\setminus i}$ integral in the second row of the equation for  $\\mathcal{L}(Q)$ will become a constant. We know that $\\int Q_i(\\Theta_i)  d\\Theta_{i}$ should be 1 because $Q_i$ is a probability distribution. So the whole second row can be viewed as a constant. \n",
      "\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{1mm} + \\text{const}        \n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the integral over $\\Theta_{\\setminus i}$  in the  first line in $\\mathcal{L}(Q)$ could be rewritten as an expectation :\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \\cdot\n",
      "        \\mathbb{E}_{[k \\neq i]}[ \\log P(D, \\Theta)]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{1mm} + \\text{const}        \n",
      "\\end{align}\n",
      "\n",
      "Where $\\mathbb{E}_{[k \\neq i]}$ denotes this expected value is with respect to the $Q$ distributions over all variables $\\Theta_k \\neq \\Theta_i$.\n",
      "\n",
      "\n",
      "We can define $\\mathbb{E}_{[k \\neq i]}[ \\log P(D, \\Theta)]$ as $\\log \\widetilde{P}(D, \\Theta_i)$.\n",
      "\n",
      "Intuitively it makes sense to see  $\\log \\widetilde{P}(D, \\Theta_i)$, the joint probability of that parameter and the data, as the expectation of $\\log P(D, \\Theta)$ under all $\\Theta_k$ that are not $\\Theta_i$ , i.e., to not marginalize out  $\\Theta_i$ in $\\log P(D, \\Theta)$.\n",
      "It is here particularly useful to be able to rewrite the above equation as:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =&  \\int Q_i(\\Theta_i) \\log \\widetilde{P}(D, \\Theta_i) -  \\int Q_i(\\Theta_i) \\log Q_i(\\Theta_i) \\\\\n",
      "                =& \\int Q_i(\\Theta_i) \\log \\dfrac{\\widetilde{P}(D , \\Theta_i)}{ Q_i(\\Theta_i)} \\\\\n",
      "                =& - \\text{KL} \\left(Q_i(\\Theta_i) || \\widetilde{P}(D, \\Theta_i) \\right)\n",
      "\\end{align}\n",
      "\n",
      "Recall we wanted to maximize the lower bound function $\\mathcal{L}(Q)$. We see that this negative KL-divergence will be maximal when \n",
      "\n",
      "\\begin{align}\n",
      " Q_i(\\Theta_i) &= \\widetilde{P}(D, \\Theta_i)  \\\\  \n",
      " \\log  Q_i(\\Theta_i) &= \\log \\widetilde{P}(D, \\Theta_i)  \\\\\n",
      "                     &= \\mathbb{E}_{[k \\neq i]}[\\log P(D, \\Theta)]\\\\\n",
      " Q_i(\\Theta_i)      &= \\exp \\left( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] \\right)    \n",
      "\\end{align}\n",
      "\n",
      "So in line with Bishop (except a subscript mismatch on his side), accounting for normalization (marginalizing over all $\\Theta_{\\setminus i}$) we get that the optimal value for $Q_i$ is \n",
      "\n",
      "\\begin{align}\n",
      " Q_i^{*}(\\Theta_i)      &= \\dfrac{\\exp ( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] )   }\n",
      "                      {\\int \\exp ( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] ) \\hspace{2mm} d\\Theta_i  }\n",
      "\\end{align}\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3. The log-probability (10 points)\n",
      "\n",
      "Write down the log-prob of data and parameters, $\\ln p(\\mathbf{X}, \\mathbf{Z}, \\mathbf{W}, \\mathbf{\\alpha}, \\tau, \\mathbf{\\mu})$, in full detail (where $\\mathbf{X}$ are observed, $\\mathbf{Z}$ is latent; this is different from [Bishop99] who uses $\\mathbf{T}$ and $\\mathbf{X}$ respectively, but $\\mathbf{X}$ and $\\mathbf{Z}$ are consistent with the PRML book and are more common nowadays). Could we use this to assess the convergence of the variational Bayesian PCA algorithm? If yes, how? If no, why not?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The log-probability of the data and parameters is defined as:\n",
      "$$\n",
      "\\begin{align}\n",
      "\\ln p ( X, Z, W, \\alpha, \\tau, \\mu) =& \n",
      "\\ln \\left(  \\prod_{n=1}^N p(x_n| z_n, W, \\mu, \\tau)p(Z)p(W|\\alpha)p(\\alpha)p(\\mu)p(\\tau) \\right) \\\\\n",
      "=& \\ln \\left( \\prod_{n=1}^N \\left[ \\left( \\frac{\\tau}{2 \\pi} \\right) ^{D/2} \\exp \\left( - \\frac{\\tau}{2} \\parallel x_t-Wz_n-\\mu \\parallel ^2  \\right) \\right]\n",
      "\\prod_{n=1}^N \\left[  \\left( \\frac{1}{2 \\pi} \\right)^{q/2} \\exp \\left( - \\frac{1}{2} z_n^\\top z_n \\right) \\right] \\\\\n",
      "\\prod_{i=1}^q \\left[ \\left( \\frac{\\alpha_i}{2 \\pi} \\right)^{D/2} \\exp \\left( -\\frac{\\alpha_i}{2}  w_i^\\top w_i \\right) \\right]\n",
      "\\prod_{i=1}^q \\left[ \\frac{1}{\\Gamma(a_\\alpha)} b_\\alpha^{a_\\alpha} \\alpha_i^{a_\\alpha-1} e^{-b_\\alpha \\: \\alpha_i} \\right]\\\\ \n",
      "\\left[ \\left(\\frac{ \\beta}{2\\pi} \\right)^{D/2} \\exp \\left( - \\frac{\\beta}{2} \\mu^\\top \\mu \\right) \\right]\n",
      "\\left[ \\frac{1}{\\Gamma(c_\\tau)} d_\\tau^{c_\\tau} \\tau^{c_\\tau-1} e^{-d_\\tau \\: \\tau} \\right] \\right) \n",
      "\\\\\n",
      "=& \\frac{ND}{2} \\ln \\frac{\\tau}{2 \\pi} + \\sum_{n=1}^N \\left[ - \\frac{\\tau}{2} \\parallel x_n-Wz_n-\\mu \\parallel ^2 \\right]\n",
      "+ \\frac{Nq}{2} \\ln \\frac{1}{2 \\pi} + \\sum_{n=1}^N \\left[ - \\frac{1}{2} z_n^\\top z_n \\right] \\\\\n",
      "&+ \\sum_{i=1}^q \\left[ \\frac{D}{2} \\ln \\frac{\\alpha_i}{2 \\pi} \\right] + \\sum_{i=1}^q \\left[ - \\frac{\\alpha_i}{2} w_i^\\top w_i \\right]\n",
      "+ \\: q \\ln \\frac{ b_\\alpha^{a_\\alpha}}{\\Gamma(a_\\alpha)} + \\sum_{i=1}^q \\left[ (a_\\alpha -1) \\ln \\alpha_i - b_\\alpha \\alpha_i \\right] \\\\\n",
      "&+ \\frac{D}{2} \\ln \\frac{\\beta}{2 \\pi} - \\frac{\\beta}{2} \\mu^\\top\\mu - \\ln \\Gamma(c_\\tau) + \\ln d_\\tau^{c_\\tau} + (c_\\tau-1)\\ln \\tau - d_\\tau \\tau\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bayesian learning results in a posterior distribution over the parameters. If we wish to use $\\ln P(\\Theta,D)$ to assess convergence we have no choice but to use some point estimate of the parameters from the approximated posterior $Q$ to evaluate  $\\ln P(\\Theta,D)$ (we cannot plug in the distribution). However, we consider convergence as $Q$ being as similar to the real posterior $P(\\Theta|D)$ as possible. There is no guarantee that if $Q$ becomes more similar to $P(\\Theta|D)$, that the used point estimate of $Q$ results in a higher value when evaluating $\\ln P(\\Theta,D)$. Therefore $\\ln P(\\Theta,D)$ is not a suitable option to assess convergence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4. The lower bound $\\mathcal{L}(Q)$ (25 points)\n",
      "\n",
      "Derive an expression for the lower bound $\\mathcal{L}(Q)$ of the log-prob $\\ln p(X)$ for Bayesian PCA, making use of the factorization (eq. 12) and the form of the Q-distributions (eq. 16-20) as listed in [Bishop99]. Show your steps. Implement this function.\n",
      "\n",
      "The following result may be useful:\n",
      "\n",
      "For $x \\sim \\Gamma(a,b)$, we have $\\langle \\ln x\\rangle = \\ln b + \\psi(a)$, where $\\psi(a) = \\frac{\\Gamma'(a)}{\\Gamma(a)}$ is the digamma function (which is implemented in numpy.special)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The lower bound can be defined as"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q(\\Theta) \\ln \\dfrac{P(D,\\Theta)}{Q(\\Theta)} \\hspace{2mm} d \\Theta\\\\\n",
      "                =& \\int Q(\\Theta) \\ln P(D,\\Theta) \\hspace{2mm} d \\Theta  - \\int Q(\\Theta) \\ln Q(\\Theta)  \\hspace{2mm} d \\Theta\\\\\n",
      "                = & \\mathbb{E}_Q[ \\ln P(D,\\Theta)] - \\mathbb{E}_Q[\\ln Q(\\Theta)]\\\\\n",
      "                ~\n",
      "               =& \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} P (t_n | x_n, W, \\mu, \\tau) \\right) \\right] \\\\\n",
      "               & + \\mathbb{E}_Q \\left[ \\ln P(X) \\right ] \\\\\n",
      "               & + \\mathbb{E}_Q \\left[\\ln  P(W | \\alpha ) \\right] \\\\\n",
      "               & + \\mathbb{E}_Q \\left[ \\ln P(\\alpha) \\right]\\\\\n",
      "               &+ \\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln Q(X) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln Q(W) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( Q(\\alpha) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln Q(\\mu) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( Q(\\tau) \\right) \\right]\\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we can treat $ P (t_n | x_n, W, \\mu, \\tau)  $  as a constant because $t$ is observed. So $\\mathcal{L}(Q)$ becomes\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n|0, I_q) \\right) \\right]              \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} (\\dfrac{\\alpha_i}{2 \\pi})^{d/2}  \\exp( - \\dfrac{1}{2} \\alpha_i ||w_i||^2 ) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma ( \\alpha_i| a_{\\alpha}, b_{\\alpha}) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu|0, \\beta^{-1} I) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau|c_\\tau, d_\\tau ) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n| m_x^{(n)}, \\Sigma_x) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{k=1}^{d}  \\mathcal{N}(\\widetilde{w}_k | m_w^{(k)}, \\Sigma_w) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma(\\alpha_i| \\widetilde{a}_\\alpha, \\widetilde{b}_{\\alpha i} ) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu | m_\\mu, \\Sigma_\\mu) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau | \\widetilde{a}_\\tau, \\widetilde{b}_\\tau ) \\right) \\right] \\\\\n",
      "                & + \\text{const}\n",
      "\\end{align}\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have evaluated the ten integrals corresponding to these ten expectations analytically. \n",
      "\n",
      "Below we give the derivation for each of them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      " \\mathbb{E}_Q \\left[ \\ln  P(X) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n|0, I_q) \\right) \\right]   \\\\\n",
      "= & \\int Q(\\Theta) \\sum_{n=1}^N \\ln \\left( \\mathcal{N}(x_n | 0, I_q \\right) d \\Theta\\\\\n",
      "= & \\sum_{n=1}^N \\int Q(x_n) \\ln \\left( \\mathcal{N} (x_n | 0, I_q \\right) d x_n \\\\\n",
      "= & \\sum_{n=1}^N \\int Q(x_n)\\left[ - \\dfrac{1}{2} (x_n - 0)^{T} I_q^{-1} (x_n - 0) - \\dfrac{q}{2} \\ln ( 2 \\pi)\n",
      "            - \\dfrac{1}{2} \\ln (\\text{det}(I_q)) \\right] d x_n \\\\\n",
      "= & -\\dfrac{1}{2} \\sum_{n=1}^N \\int Q(x_n) x_n^{T}x_n d x_n - \\dfrac{q}{2} \\ln (2 \\pi) \\sum_{n=1}^N \\int Q(x_n) d x_n -\n",
      "                    \\dfrac{1}{2} \\ln (\\text{det} (I_q)) \\sum_{n=1}^N \\int Q(x_n ) d x_n \\\\\n",
      "= & - \\dfrac{1}{2} \\sum_{n=1}^{N} \\int Q(x_n) x_n^{T}x_n d x_n + \\text{const} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Rule 355 of <a href=\"http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\"> the Matrix Cookbook</a> says that $ \\mathbb{E}_P[x^{T}Ax] = \\text{Tr}(A \\Sigma) + m^{T} A m $ where $m$ and $\\Sigma$ are the mean and covariance of $P$, so\n",
      "\n",
      "\\begin{align}\n",
      " \\mathbb{E}_Q \\left[ \\ln  P(X) \\right]   = & \n",
      "     -\\dfrac{1}{2} \\left( N \\cdot \\text{Tr}(\\Sigma_x) + \\sum_{n=1}^N {m_x^{(n)}}^{T}  {m_x^{(n)}} \\right) + \\text{const}\\\\\n",
      "     = & -\\dfrac{N}{2} \\text{Tr}(\\Sigma_x) -\\dfrac{1}{2}  \\sum_{n=1}^N {m_x^{(n)}}^{T}  {m_x^{(n)}} + \\text{const}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q [\\ln P(W|\\alpha)] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} (\\dfrac{\\alpha_i}{2 \\pi})^{d/2}  \\exp( - \\dfrac{1}{2} \\alpha_i ||w_i||^2 ) \\right) \\right]\\\\\n",
      "=& \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) \\int Q(\\alpha_i) \\left[ \\int Q(w_i) \\left( \\ln (\\alpha_i) - \\dfrac{1}{2} \\alpha_i w_i^{T}w_i \\right)  d w_i \\right] d \\alpha_i \\\\\n",
      "=&  \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) - \\dfrac{1}{2} \\int Q(\\alpha_i) \\left( \\ln(\\alpha_i) - \\alpha_i \\left[ \\int Q(w_i) w_i^{T} w_i  d w_i\\right] \\right)  d \\alpha_i \\\\\n",
      "\\end{align}\n",
      "\n",
      "Again we can use rule 355 from the Matrix Cookbook.\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q [\\ln P(W|\\alpha)] =& \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) (- \n",
      "    \\dfrac{1}{2}) \\int Q(\\alpha_i) \n",
      "        \\left( \\ln(\\alpha_i) - \\alpha_i( \\text{Tr}(\\alpha_i I) \\right) d \\alpha_i \\\\\n",
      "= & \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) (- \n",
      "    \\dfrac{1}{2}) \\int Q(\\alpha_i)      \n",
      "        \\left( \\ln(\\alpha_i) - d \\alpha_i^{2} \\right)\n",
      "        d \\alpha_i \\\\\n",
      "= & \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi)( - \n",
      "    \\dfrac{1}{2})\n",
      "    \\left( \n",
      "        \\int Q(\\alpha_i)  \\ln(\\alpha_i) d \\alpha_i\n",
      "        -\n",
      "        d \\int Q(\\alpha_i) \\alpha_i^2 d \\alpha_i\n",
      "        \\right)        \\\\\n",
      "\\end{align}\n",
      "\n",
      "We can make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;\n",
      "\n",
      "and we can make use of the fact that $\\mathbb{E}_P[x^2] = \\dfrac{(a+1)a}{b^2}$ if $P(x) = \\Gamma(x|a,b)$:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q [\\ln P(W|\\alpha)] = &  \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) (- \n",
      "    \\dfrac{1}{2})\n",
      "        \\left(\n",
      "            (\\psi(\\widetilde{a}_\\alpha) - \\ln( \\widetilde{b}_{\\alpha i} ) )\n",
      "            - d \\dfrac{ ( \\widetilde{a}_\\alpha + 1 ) \\widetilde{a}_\\alpha}\n",
      "                        { \\widetilde{b}_{\\alpha i}^{2} }\n",
      "         \\right)\\\\\n",
      "= &  \\dfrac{d}{4} \\ln( 2 \\pi) \\sum_{i=1}^q \n",
      "            \\left( \n",
      "              (  \\psi(\\widetilde{a}_\\alpha) - \\ln ( \\widetilde{b}_{\\alpha i}) )-\n",
      "                    \\dfrac{d ( \\widetilde{a}_{\\alpha} + 1 ) \\widetilde{a}_\\alpha }\n",
      "                            { \\widetilde{b}^{2}_{\\alpha i} }\n",
      "              \\right)            \\\\\n",
      "= & \\dfrac{d}{4} \\ln(2 \\pi) \\left( q \\cdot \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q \\ln (\\widetilde{b}_{\\alpha i}) \n",
      "                               - \\sum_{i=1}^{q} \\left( \\dfrac{d (\\widetilde{a}_\\alpha^2 + \\widetilde{a}_\\alpha) } {\\widetilde{b}_{\\alpha i}^2 } \\right) \\right)               \n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\alpha) \\right] =& \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma ( \\alpha_i| a_{\\alpha}, b_{\\alpha}) \\right) \\right]\\\\\n",
      "= & \\int Q(\\alpha) \\sum_{i=1}^q \\ln( \\Gamma(\\alpha_i|a_\\alpha, b_\\alpha)) d \\alpha \\\\\n",
      "=& \\sum_{i=1}^q  \\int Q(\\alpha_i) \\ln (\\Gamma (\\alpha_i| a_\\alpha , b_\\alpha)) d\\alpha_i \\\\\n",
      "= & \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left( \\dfrac{1}{\\Gamma(a_\\alpha)} \\right) d \\alpha_i +\n",
      "   \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln (b_{\\alpha}^{a_\\alpha} )d \\alpha_i  + \\\\\n",
      "    & \\sum_{i=1}^q \\int Q(\\alpha_i)  \\ln \\left( \\alpha_{i}^{a_\\alpha - 1} \\right) d \\alpha_i + \n",
      "    \\sum_{i=1}^q \\int Q(\\alpha_i) (- b_\\alpha \\alpha_i ) d \\alpha_i \\\\\n",
      "= & \\text{const} +  \n",
      "         (a_\\alpha - 1) \\sum_{i=1}^q   \\int Q(\\alpha_i)  \\ln \\left( \\alpha_{i} \\right) d \\alpha_i - \n",
      "         b_\\alpha \\sum_{i=1}^q \\int Q(\\alpha_i)  \\alpha_i  d \\alpha_i \\\\\n",
      "\\end{align}\n",
      "\n",
      "We make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;  \n",
      "and \n",
      "of the fact that $\\mathbb{E}_P[x] = \\dfrac{a}{b}$ if $P(x) = \\Gamma(x|a,b)$.\n",
      "Then:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\alpha) \\right] =& \n",
      "     \\cdot  (a_\\alpha - 1) \\cdot  \\sum_{i=1}^q   \\left(\\psi(\\widetilde{a}_\\alpha) - \\ln(\\widetilde{b}_{\\alpha, i})\\right)\n",
      "     -\n",
      "      b_\\alpha \\sum_{i=1}^q  \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha, i}} \\right)\n",
      "     + \\text{const} \\\\\n",
      "     = &    (a_\\alpha - 1) \\left(  q \\cdot  \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q  \\left( \\ln(\\widetilde{b}_{\\alpha, i})\\right) \\right)\n",
      "     -\n",
      "      b_\\alpha \\sum_{i=1}^q  \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha, i}} \\right)\n",
      "     + \\text{const} \n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu|0, \\beta^{-1} I) \\right) \\right]\\\\\n",
      "= & \\mathbb{E}_Q \\left[ - \\dfrac{d}{2}  \\ln ( 2 \\pi)  - \\dfrac{1}{2} \\mu^{T} \\beta \\mu  - \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) \\right]\\\\\n",
      "= & -  \\int Q(\\Theta)  \\dfrac{d}{2} \\ln \\left( {2\\pi} \\right) + \\left( \\dfrac{\\beta}{2} \\mu^{T}\\mu \\right)   + \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) d \\Theta \\\\\n",
      "= & - \\int Q(\\mu)  \\dfrac{d}{2} \\ln \\left( {2\\pi} \\right) + \\left( \\dfrac{\\beta}{2} \\mu^{T}\\mu \\right)   + \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) d \\mu \\\\\n",
      "= & - \\dfrac{d}{2} \\ln \\left( {2\\pi} \\right)  \\int Q(\\mu)  d \\mu - \n",
      "        \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) \\int Q(\\mu) d \\mu\n",
      "        - \\int Q(\\mu) \\left( \\dfrac{\\beta}{2} \\mu^{T}\\mu \\right) d \\mu\\\\\n",
      "= &  - \\text{const} - \\dfrac{\\beta}{2} \\int Q(\\mu) \\mu^{T}\\mu d \\mu \\\\\n",
      "= & - \\dfrac{\\beta}{2} \\int Q(\\mu) \\mu^{T}\\mu d \\mu + \\text{const} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Again we make use of rule 355 of the Matrix Cookbook which states that $ \\mathbb{E}_P[x^{T}Ax] = \\text{Tr}(A \\Sigma) + m^{T} A m $ where $m$ and $\\Sigma$ are the mean and covariance of $P$, so\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = & - \\dfrac{\\beta}{2} \\left( \\text{Tr}(\\Sigma_\\mu) + m_\\mu^{T} m_\\mu \\right) +  \\text{const} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Because $\\mu$ is $d$-dimensional and $\\Sigma_\\mu$ is defined as $(\\beta + N\\langle \\tau \\rangle)^{-1} I$\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = &- \\dfrac{\\beta}{2} \\left( \\dfrac{d}{\\beta + N \\langle \\tau \\rangle } + m_\\mu^{T} m_\\mu \\right) +  \\text{const}\\\\\n",
      "\\end{align}\n",
      "\n",
      "Because $Q(\\tau) = \\Gamma (\\tau | \\widetilde{a}_\\tau, \\widetilde{b}_\\tau)$we end up with\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = &\n",
      "    - \\dfrac{\\beta}{2} \n",
      "    \\left( \\dfrac{d}{\\beta + N ({ \\widetilde{a}_\\tau } / { \\widetilde{b}_\\tau} ) }\n",
      "+ m_\\mu^{T} m_\\mu  \\right) +  \\text{const}\\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau|c_\\tau, d_\\tau ) \\right) \\right] \\\\\n",
      "= & \\mathbb{E}\\left[ \\ln \\left( \\dfrac{1}{\\Gamma (c_\\tau)} d_\\tau^{c_\\tau} \\tau^{c_\\tau -1} e^{-c_\\tau \\cdot \\tau} \\right) \\right]\\\\\n",
      "= & \\int Q(\\Theta) \\left( - \\ln(\\Gamma (c_\\tau)) + c_\\tau \\ln(d_\\tau) + (c_\\tau - 1) \\ln (\\tau) - d_\\tau \\cdot \\tau \\right) \\hspace{3mm} d \\Theta \\\\\n",
      "= & - \\int Q(\\tau) \\ln(\\Gamma (c_\\tau)) \\hspace{3mm} d \\tau \n",
      "    + \\int Q(\\tau) c_\\tau \\ln(d_\\tau)  \\hspace{3mm} d \\tau  \n",
      "    + \\int Q(\\tau) (c_\\tau - 1) \\ln (\\tau) \\hspace{3mm} d \\tau  \n",
      "    - \\int Q(\\tau) d_\\tau \\cdot \\tau \\hspace{3mm} d \\tau    \\\\\n",
      "= &   (c_\\tau - 1) \\int Q(\\tau)  \\ln (\\tau) \\hspace{3mm} d \\tau  \n",
      "    -   d_\\tau \\int Q(\\tau)  \\tau \\hspace{3mm} d \\tau   \n",
      "     + \\text{const} \\\\   \n",
      "\\end{align}\n",
      "\n",
      "Again make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;  \n",
      "and \n",
      "of the fact that $\\mathbb{E}_P[x] = \\dfrac{a}{b}$ if $P(x) = \\Gamma(x|a,b)$.\n",
      "Then:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] \n",
      "= &   (c_\\tau - 1) \\left( \\psi(\\widetilde{a}_\\tau) - \\ln(\\widetilde{b}_\\tau )\\right)\n",
      "    -   d_\\tau  \\dfrac{\\widetilde{a}_\\tau }{\\widetilde{b}_\\tau}  \n",
      "     + \\text{const} \\\\   \n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "6"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln Q(X) \\right] = &  \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n| m_x^{(n)}, \\Sigma_x) \\right) \\right] \\\\\n",
      "= & \\sum_{n=1}^N \\mathbb{E}_Q \\left[ \n",
      "                                    \\ln \\left( \\dfrac{1}{(2 \\pi)^{(q/2)}} \\dfrac{1}{\\text{det}(\\Sigma_x)^{(1/2)}} \\right)\n",
      "                                  + \\left( -\\dfrac{1}{2} (x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) \\right) \n",
      "                                  \\right] \\\\\n",
      "= & N \\cdot  \\ln \\left( \\dfrac{1}{(2 \\pi)^{(q/2)}} \\right)\n",
      "        +  N \\cdot \\left( \\dfrac{1}{\\text{det}(\\Sigma_x)^{(1/2)}} \\right)\n",
      "        - \\sum_{n=1}^N \\dfrac{1}{2} \\int Q(x_n)  (x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) \\hspace{3mm} d x_n\\\\\n",
      "= &  - \\sum_{n=1}^N \\dfrac{1}{2} \\int Q(x_n)  (x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) \\hspace{3mm} d x_n + \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "We recognize $(x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) $ as the square of the Mahalanobis distance $\\vartriangle$.\n",
      "\n",
      "As stated  for example <a href=\"http://stat.ethz.ch/education/semesters/ss2012/ams/slides/v7.2.pdf\"> here</a> , the expectation of the squared Mahalanobis distance pertaining to $x$ and $m$ under a multivariate Gaussian  $\\mathcal{N}(x|m, \\Sigma)$  corresponds to a Chi-square distribution $\\chi^2_{n}$ with the dimensionality of the space of the distribution as the degrees of freedom $n$. This $\\chi^2_n$ distribution has as expected value also $n$. \n",
      "\n",
      "So: $\\mathbb{E}_P[\\vartriangle^2] =  \\text{dimensionality}(x \\sim P(x))$\n",
      "\n",
      "\n",
      "This can also be derived by using rule 357 from the Matrix Cookbook which states that $\\mathbb{E}_P[(x - m')^{T} A (x-m')] = (m-m')^{T} A (m-m') + \\text{Tr}(A \\Sigma)$, if $P(x) = \\mathcal{N}(x | m, \\Sigma)$. We would get $\\text{Tr}(I_q)$.\n",
      "\n",
      "Therefore we end up with\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln Q(X) \\right] = & -  \\dfrac{N}{2} q + \\text{const}\\\\\n",
      "= & \\text{const}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "7"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q\\left[\\ln Q(W) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{k=1}^{d}  \\mathcal{N}(\\widetilde{w}_k | m_w^{(k)}, \\Sigma_w) \\right) \\right]\\\\\n",
      "= &  - \\sum_{k=1}^d \\dfrac{1}{2} \\int Q(\\widetilde{w}_k)  (\\widetilde{w}_k - m_w^{(k)} )^{T}\\Sigma_w^{-1} (\\widetilde{w}_k - m_w^{(k)}) \\hspace{3mm} d \\widetilde{w}_k + \\text{const} \\\\\n",
      "= & - \\dfrac{d}{2} q + \\text{const}\\\\\n",
      "= & \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Where we have followed the same reasoning as for $\\mathbb{E}_Q[\\ln Q(X)]$ above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "8"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      " \\mathbb{E}_Q\\left[ \\ln Q(\\alpha) \\right] =&  \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma(\\alpha_i| \\widetilde{a}_\\alpha, \\widetilde{b}_{\\alpha i} ) \\right) \\right] \\\\\n",
      " =& \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left( \n",
      "     \\dfrac{1}{\\Gamma(\\widetilde{a}_\\alpha)} \\cdot\n",
      "     \\widetilde{b}^{\\widetilde{a}_\\alpha}_{\\alpha i} \\cdot\n",
      "     \\alpha_i^{(\\widetilde{a}_\\alpha - 1) } \\cdot\n",
      "     e^{- \\widetilde{b}_{\\alpha i} \\alpha_i} \n",
      "     \\right) \\hspace{3mm} d \\alpha_i \\\\\n",
      "=& \\sum_{i=1}^q \\int Q(\\alpha_i)  \\dfrac{1}{\\Gamma(\\widetilde{a}_\\alpha)} \\hspace{3mm} d \\alpha_i +\n",
      "   \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left(   \\widetilde{b}^{\\widetilde{a}_\\alpha}_{\\alpha i} \\right) \\hspace{3mm} d \\alpha_i  +\n",
      "   \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left(   \\alpha_i^{(\\widetilde{a}_\\alpha - 1) }   \\right) \\hspace{3mm} d \\alpha_i  -\n",
      "    \\sum_{i=1}^q \\int Q(\\alpha_i) \\widetilde{b}_{\\alpha i} \\alpha_i \\hspace{3mm} d \\alpha_i \\\\\n",
      "=& (\\widetilde{a}_\\alpha - 1)  \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln   \\alpha_i    \\hspace{3mm} d \\alpha_i -\n",
      "    \\sum_{i=1}^q \\widetilde{b}_{\\alpha i}  \\int Q(\\alpha_i) \\alpha_i \\hspace{3mm} d \\alpha_i \n",
      "    + \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Again we make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;  \n",
      "and \n",
      "of the fact that $\\mathbb{E}_P[x] = \\dfrac{a}{b}$ if $P(x) = \\Gamma(x|a,b)$.\n",
      "Then:\n",
      "\n",
      "\\begin{align}\n",
      " \\mathbb{E}_Q\\left[ \\ln Q(\\alpha) \\right] \n",
      "=&    (\\widetilde{a}_\\alpha - 1)  \\sum_{i=1}^q \\left( \\psi(\\widetilde{a}_\\alpha) - \\ln(\\widetilde{b}_{\\alpha i}) \\right) -\n",
      "      \\sum_{i=1}^q  \\widetilde{b}_{\\alpha i} \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha i}} \\right)\n",
      "      + \\text{const} \\\\\n",
      "=&    (\\widetilde{a}_\\alpha - 1)   \\left( q  \\cdot \\psi(\\widetilde{a}_\\alpha) - \\left(  \\sum_{i=1}^q \\ln(\\widetilde{b}_{\\alpha i}) \\right) \\right)  -\n",
      "        q \\cdot \\widetilde{a}_\\alpha\n",
      "         + \\text{const} \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "9"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_q [ \\ln Q(\\mu)] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu | m_\\mu, \\Sigma_\\mu) \\right) \\right] \\\\\n",
      "=& \\int Q(\\mu) \\left[ \\left( -\\dfrac{1}{2} (\\mu - m_\\mu)^{T} \\Sigma_\\mu (\\mu - m_\\mu) \\right) -\n",
      "                    \\left( \\dfrac{d}{2} \\ln (2 \\pi) \\right) -\n",
      "                    \\left( \\dfrac{1}{2} \\ln (\\text{det}(\\Sigma_\\mu)) \\right)\n",
      "                \\right] d \\mu\\\\\n",
      "=& - \\dfrac{1}{2} \\int Q(\\mu) (\\mu - m_\\mu)^{T} \\Sigma_\\mu (\\mu - m_\\mu) d \\mu + \\text{const} \\\\\n",
      "=& - \\dfrac{d}{2} + \\text{const} \\\\\n",
      "=& \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Where we have followed the same reasoning as for $\\mathbb{E}_Q[\\ln Q(X)]$ above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "10"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q[\\ln Q(\\tau)] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau | \\widetilde{a}_\\tau, \\widetilde{b}_\\tau ) \\right) \\right]\\\\\n",
      "= & \\left(\\psi(\\widetilde{a}_\\tau) - \\ln( \\widetilde{b}_\\tau) \\right) - \\widetilde{b}_\\tau \\left( \\dfrac{ \\widetilde{a}_\\tau}{\\widetilde{b}_\\tau} \\right) + \\text{const}\\\\\n",
      "= & \\left(\\psi(\\widetilde{a}_\\tau) - \\ln( \\widetilde{b}_\\tau) \\right) - \\widetilde{a}_\\tau + \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Where we have followed the same reasoning as for e.g.  $\\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] $ above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The whole lower bound $\\mathcal{L}(Q)$ can now be expressed as:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& -\\dfrac{N}{2} \\text{Tr}(\\Sigma_x) -\\dfrac{1}{2}  \\sum_{n=1}^N {m_x^{(n)}}^{T}  {m_x^{(n)}} \\\\\n",
      "               & + \\dfrac{d}{4} \\ln(2 \\pi) \\left( q \\cdot \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q \\ln (\\widetilde{b}_{\\alpha i}) \n",
      "                               - \\sum_{i=1}^{q} \\left( \\dfrac{d (\\widetilde{a}_\\alpha^2 + \\widetilde{a}_\\alpha) } {\\widetilde{b}_{\\alpha i}^2 } \\right) \\right) \\\\\n",
      "              & +    (a_\\alpha - 1) \\left(  q \\cdot  \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q  \\left( \\ln(\\widetilde{b}_{\\alpha, i})\\right) \\right)\n",
      "                    - b_\\alpha \\sum_{i=1}^q  \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha, i}} \\right) \\\\\n",
      "              & - \\dfrac{\\beta}{2}     \\left( \\dfrac{d}{\\beta + N ({ \\widetilde{a}_\\tau } / { \\widetilde{b}_\\tau} ) } + m_\\mu^{T} m_\\mu  \\right)\\\\\n",
      "              & + (c_\\tau - 1) \\left( \\psi(\\widetilde{a}_\\tau) - \\ln(\\widetilde{b}_\\tau )\\right)     -   d_\\tau  \\dfrac{\\widetilde{a}_\\tau }{\\widetilde{b}_\\tau} \\\\\n",
      "              & - (\\widetilde{a}_\\alpha - 1)   \\left( q  \\cdot \\psi(\\widetilde{a}_\\alpha) - \\left(  \\sum_{i=1}^q \\ln(\\widetilde{b}_{\\alpha i}) \\right) \\right)  +\n",
      "        q \\cdot \\widetilde{a}_\\alpha \\\\\n",
      "              & - \\left(\\psi(\\widetilde{a}_\\tau) - \\ln( \\widetilde{b}_\\tau) \\right) + \\widetilde{a}_\\tau\\\\\n",
      "              & + \\text{const}\n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 5. Optimize variational parameters (50 points)\n",
      "Implement the update equations for the Q-distributions, in the __update_XXX methods. Each update function should re-estimate the variational parameters of the Q-distribution corresponding to one group of variables (i.e. either $Z$, $\\mu$, $W$, $\\alpha$ or $\\tau$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6. Learning algorithm (10 points)\n",
      "Implement the learning algorithm described in [Bishop99], i.e. iteratively optimize each of the Q-distributions holding the others fixed.\n",
      "\n",
      "What would be a good way to track convergence of the algorithm? Implement your suggestion.\n",
      "\n",
      "Test the algorithm on some test data drawn from a Gaussian with different variances in orthogonal directions. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 7. PCA Representation of MNIST (10 points)\n",
      "\n",
      "Download the MNIST dataset from here http://deeplearning.net/tutorial/gettingstarted.html (the page contains python code for loading the data). Run your algorithm on (part of) this dataset, and visualize the results.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle, gzip, numpy\n",
      "\n",
      "# Load the dataset\n",
      "f = gzip.open('../mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#An image is represented as numpy 1-dimensional array of 784 (28 x 28) float values between 0 and 1 \n",
      "dimIm = 28\n",
      "nrOriginalDimensions = dimIm**2\n",
      "\n",
      "[_, nrImsTestSet] =  shape(test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def showExamples(W, nrExamples=3):\n",
      "    \n",
      "    Wpi = np.linalg.pinv(W)  \n",
      "    numbers = []\n",
      "    \n",
      "    for i in range(nrExamples):\n",
      "        randomIndex =  randint(0,nrImsTestSet-1)    \n",
      "        originalIm = test_set[0][randomIndex]\n",
      "        originalLabel = test_set[1][randomIndex]\n",
      "        \n",
      "        imMatrix = originalIm.reshape((dimIm, dimIm))    \n",
      "        matshow(imMatrix,cmap=cm.gray)    \n",
      "        \n",
      "        imReduced = np.dot( W.T  , originalIm)       \n",
      "        \n",
      "        imRecov = np.dot(Wpi.T, imReduced)\n",
      "        \n",
      "        imRecovMatrix = imRecov.reshape((dimIm, dimIm))\n",
      "        matshow(imRecovMatrix,cmap=cm.gray)\n",
      "        \n",
      "        numbers.append(originalLabel)\n",
      "        \n",
      "    print \"Showing numbers %s\" % numbers\n",
      "    \n",
      "    #print shape(imReduced)\n",
      "    #print type(imReduced)\n",
      "    #print shape(W)\n",
      "    #print shape(Wpi)\n",
      "    #print shape(imRecov)\n",
      "\n",
      "\n",
      "nrReducedDimensions = 760\n",
      "W = np.random.randn(nrOriginalDimensions,nrReducedDimensions)\n",
      "showExamples(W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Showing numbers [3, 0, 7]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neMAi/wMdVxHeNK7gCgAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x2602dd0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x279e850>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x27c2d10>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGTRJREFUeJztnX9M1Pcdxp8bUoGi1F/8EFCoiILya9PZLbKls7RdsjId\nzbRpLVFcO7OmMZqlZkk33B9Kl2zGWpN1a7uQmbD5j86l6pzbunas1TWDRVc3rAWkoCCgKCJi7Xd/\nGC+A3PuROw+u+zyvhMTjue/d5z7c4/e+9/7l8zzPgxDCCT433gsQQowdMrwQDiHDC+EQMrwQDiHD\nC+EQMrwQDhF2wx86dAjz58/H3Llz8dJLL4X76UZNRkYG8vPzUVRUhC9+8YvjvRysXbsWSUlJyMvL\n8/+uu7sbJSUlyM7OxsMPP4yLFy9G1PoqKyuRlpaGoqIiFBUV4dChQ+O2vpaWFjz44INYsGABFi5c\niJdffhlA5OxhoPWN2R56YeSTTz7x5syZ4zU2NnoDAwNeQUGB98EHH4TzKUdNRkaG19XVNd7L8PP2\n2297//znP72FCxf6f/f973/fe+mllzzP87yqqirvhRdeGK/ljbi+yspK76c//em4rWkwZ8+e9erq\n6jzP87zLly972dnZ3gcffBAxexhofWO1h2E9wx87dgxZWVnIyMhAdHQ0Vq1ahd/97nfhfMqg8CIo\n96i4uBhTpkwZ8rv9+/ejvLwcAFBeXo59+/aNx9IAjLw+IHL2MDk5GYWFhQCA+Ph45OTkoLW1NWL2\nMND6gLHZw7AavrW1Fenp6f7baWlp/hcXKfh8Pjz00ENYtGgRfvnLX473ckakvb0dSUlJAICkpCS0\nt7eP84puZ+fOnSgoKEBFRcW4XnIMpqmpCXV1dViyZElE7uGt9T3wwAMAxmYPw2p4n88Xzoe/K9TW\n1qKurg4HDx7Erl278M4774z3kkx8Pl/E7ev69evR2NiI+vp6pKSkYNOmTeO9JPT29qKsrAw7duzA\npEmThmiRsIe9vb14/PHHsWPHDsTHx4/ZHobV8KmpqWhpafHfbmlpQVpaWjifctSkpKQAAGbMmIEV\nK1bg2LFj47yi20lKSsK5c+cAAGfPnkViYuI4r2goiYmJfhOtW7du3Pfw+vXrKCsrw+rVq7F8+XIA\nkbWHt9b31FNP+dc3VnsYVsMvWrQIp06dQlNTEwYGBvDb3/4WpaWl4XzKUdHX14fLly8DAK5cuYLD\nhw8P+fY5UigtLUV1dTUAoLq62v8miRTOnj3r//fevXvHdQ89z0NFRQVyc3OxYcMG/+8jZQ8DrW/M\n9jDc3woeOHDAy87O9ubMmeNt3bo13E83Kj766COvoKDAKygo8BYsWBAR61u1apWXkpLiRUdHe2lp\nad4bb7zhdXV1ecuWLfPmzp3rlZSUeBcuXIiY9b3++uve6tWrvby8PC8/P9/75je/6Z07d27c1vfO\nO+94Pp/PKygo8AoLC73CwkLv4MGDEbOHI63vwIEDY7aHPs+LkK9XhRBhR5l2QjiEDC+EQ8jwQrhE\nsBf/Bw8e9ObNm+dlZWV5VVVVt+kA9KMf/Yzjz1370u7GjRuYN28ejhw5gtTUVCxevBg1NTXIycnx\n38fn82HRokX+221tbZg5c6b/9qlTp8znYPF6tuy4uDhTH57J1N3djalTp/pvJycnm8d3dnaa+qef\nfmrq7PG7u7uH3O7o6BgSO7569ap5fFdXl6lPnz7d1BMSEkz9woULt90enHLL9v9zn7M/XH7yySem\n3t/fb+rD9//ixYu47777/LdnzZplHs/278qVK6YeExNj6r29vUNuX7p0CZMnT/bfZolBUVFRAbXS\n0lK8/PLLI3okqI/0n5UceSHEUIIy/GchR14IcTsTgjnoTvOQ29ra/P+2PoJEArGxseO9BJN77713\nvJdgwj7CjjeRvr6JEyeGdHx/f7//Mufo0aMB7xeU4e80R37wNXukI8OHRqTv3/+74WNiYvyvccmS\nJQFNH9RH+kjPkRdCjExQZ/gJEybglVdewSOPPIIbN26goqJiyDf0QojIJGy59Ow6Pzs729Sjo6NN\nnYWNLl26ZOosLMQ+orKwzJkzZ0ydfafByjfZR8Bp06aZOuPkyZOmPjiENBKDQ2Aj0dHRMeo1DYaF\nNQdXn43E8Br54bCwX6iXCMPDcsMZGBgwdcsfpaWl2Llz590LywkhPpvI8EI4hAwvhEPI8EI4hAwv\nhEPI8EI4hAwvhEMElXhzp8yYMSOgxuLgLA7/0UcfmXqoqaisPJKV77Lyy+Hlr8O5fv26qTMGpz6P\nBCs/ZetncWI26IHlCdxzzz2mzsqP2fvnVrfiYOnr6zP1W+3Pw4U1qMLaG53hhXAIGV4Ih5DhhXAI\nGV4Ih5DhhXAIGV4IhwhrWG5wF9jRwspbWViFlWeGWv7Iyi9Z2Ip1df34449NnYXNWNiNhUVZWI3t\nLwuLsbAW2z/292Plw+zxWbcmdjzb/wkTbOux+fDx8fEBNeu9qzO8EA4hwwvhEDK8EA4hwwvhEDK8\nEA4hwwvhEDK8EA4R1ji81cqZxWFZnHfhwoWmPtrposNhbbat0l8AaG5uNnWWR/D5z3/e1FmcfsGC\nBab+jW98w9Tnzp1r6qy8lZUPv/fee6a+a9cuU29oaDB1Vl7L2oyfP3/e1Bksjs72h7UBv3btWkDN\n6jyvM7wQDiHDC+EQMrwQDiHDC+EQMrwQDiHDC+EQMrwQDhHWOPzs2bMDao2NjeaxrJ6YxVGvXr1q\n6mxcMKsXZ3FWFgdm9eIszp6VlWXqzz33nKmzcdSszTKr5+/p6TF1FkdfuXKlqVvvLQB49913Tf3n\nP/+5qYc67prlabD3N2tTbu2/9d4L2vAZGRmYPHkyoqKiEB0djWPHjgX7UEKIMSJow/t8Prz11lsh\ndbURQowtIV3DWyl8QojII6Qz/EMPPYSoqCg8++yz+M53vnPbfc6cOeP/d0JCAhISEoJ9OiGEQW9v\nL3p7ewEAf/vb3wLeL2jD19bWIiUlBefPn0dJSQnmz5+P4uLiIfdhjRaFEHeH+Ph4f2PLpUuXBjR9\n0B/pb32LO2PGDKxYsUJf2gnxGSAow/f19fnLO69cuYLDhw8jLy/vri5MCHH3CeojfXt7O1asWAHg\nZjzxySefxMMPP3zb/aze7awvN4vzsjglGxd963onEKwvPetLPmXKFFNneQL5+fmmXlhYGJLe2dlp\n6vv27TP1goICU2d5DG1tbabOxmlbfdkBXu/P5gocPnzY1EPN4wh1boI1N8B6bwdl+MzMTNTX1wdz\nqBBiHFFqrRAOIcML4RAyvBAOIcML4RAyvBAOIcML4RBhrYe3eruzvtsszs3imKzvPesLn5SUZOqs\nHpo9fmZmpqk/+eSTpl5ZWWnqf/rTn0yd7c9Xv/pVU58/f76pv/LKK6Z+4sQJU2d//9/85jemvnv3\nblNfs2aNqb///vumfunSJVNnfe1ZHgSrO7Hq6W/cuBFQ0xleCIeQ4YVwCBleCIeQ4YVwCBleCIeQ\n4YVwCBleCIcIaxz+woULATUrVggAUVFRpm7VAwNAbGysqU+cONHUWb006+vO6v03b95s6t/73vdC\nenxWz8/q2V988UVTZ3kCLI5t5WgAPA7P+h2wuQAzZ8409S1btpj6Cy+8YOqpqammzv4+LI8j2Aay\nOsML4RAyvBAOIcML4RAyvBAOIcML4RAyvBAOIcML4RBhjcNbNe/Tp083j2Vx2A8//NDUc3NzTZ31\ntWd5AqH2tWf16qxvPds/Vq/+gx/8wNTZ/PIf//jHpj7SnILBsPnzrC896/u+bt06U//1r39t6uz9\nx/rGszwP9vrY81v9Gqxaep3hhXAIGV4Ih5DhhXAIGV4Ih5DhhXAIGV4Ih5DhhXAIMw6/du1avPnm\nm0hMTMTx48cB3Iwfrly5Es3NzcjIyMCePXsC9oi3apLZfHDWtz47O9vUW1paTJ31/WZ929nxLA7/\nn//8x9SnTp1q6qwfwI9+9CNTZ3kGTH/jjTdMfcaMGabOYHkObD58aWmpqbN6+tmzZ5s6i7MzneU5\nsOe33j89PT0BNfMMv2bNGhw6dGjI76qqqlBSUoKGhgYsW7YMVVVV5sKEEJGDafji4mJMmTJlyO/2\n79+P8vJyAEB5eTn27dsXvtUJIe4qo76Gb29v96f1JSUl0Y+WQojIIaRcep/PZ/Ym6+rq8v87NjYW\ncXFxoTydECIA165d8+ff/+Mf/wh4v1Gf4ZOSknDu3DkANxs9Ws0cp02b5v+R2YUIHxMnTsSkSZMw\nadIkLF68OOD9Rm340tJSVFdXAwCqq6uxfPny4FcphBhTTMM/8cQT+PKXv4z//ve/SE9Px69+9Sts\n3rwZf/zjH5GdnY0///nPtN2yECJyMK/ha2pqRvz9kSNH7ujBrRnZrG83q1dn87knTZpk6qyeOpT5\n3ABfH+ubzvIA0tLSTP3MmTOmfvHiRVM/ffq0qf/hD38wdbY/rK8/e33WzAMAePvtt02d5UlkZWWZ\nOuurz/IYrl27ZuqsXt66RLZyAJRpJ4RDyPBCOIQML4RDyPBCOIQML4RDyPBCOIQML4RDhLUvvRVr\nZnFwFodmsDh8Y2OjqbM4MesbPmvWLFNnfc1ZPfmnn35q6uz179mzx9RZrkVsbKyps3r25uZmU4+O\njjZ19v5h+3PixAlTLygoMHVWrz64jmQk2Pz3zs5OU4+KigqoWTkAOsML4RAyvBAOIcML4RAyvBAO\nIcML4RAyvBAOIcML4RBhjcNbNc1s/jmrJw613j09Pd3UWUsuFkdncV42Hz1Qr/9bLFmyxNQfe+wx\nU8/MzDR1Vo/N9p/Vm7O+8iyOf+XKFVNn9fhbtmwxdfb66+vrTX369Ommzur52VwGKw/Eeu06wwvh\nEDK8EA4hwwvhEDK8EA4hwwvhEDK8EA4hwwvhEGGNw1s1z6yemNXDs/nbrG84i7OHGoeeNm1aSM+/\ncuVKU2d9848ePWrqt8aFBYLV47O/D8uDYHF49vdlcwuKi4tNnc0FYM/P8jhYngbbH+aP999/P6Bm\n5bjoDC+EQ8jwQjiEDC+EQ8jwQjiEDC+EQ8jwQjiEDC+EQ5hx+LVr1+LNN99EYmIijh8/DgCorKzE\na6+95o/Tbtu2DY8++uiIx1s13dbs+DuBzdeeMMFOMWDPz/rKt7a2mjqLw7/44oumPmXKFFPfuXOn\nqbN6cBaHZn3dWb03izOz+fVs/9nrW7VqlamfPHnS1J977jlTZ3F2NteA9UO4dOmSqVv7b+WImH+V\nNWvW4NChQ0N+5/P5sHHjRtTV1aGuri6g2YUQkYdp+OLi4hHPNJ7nhW1BQojwEdQ1/M6dO1FQUICK\nigpcvHjxbq9JCBEmRm349evXo7GxEfX19UhJScGmTZsC3re7u9v/w3rYCSGCp7+/Hz09Pejp6THr\nKEZt+MTERPh8Pvh8Pqxbtw7Hjh0LeN+pU6f6f9jwQSFE8MTExCAhIQEJCQlmg9NRG/7s2bP+f+/d\nuxd5eXnBrVAIMeaYsasnnngCf/3rX9HZ2Yn09HRs2bIFb731Furr6+Hz+ZCZmYlXX311rNYqhAgR\n0/A1NTW3/W7t2rV35YlZnJbVI997772m/vHHH5t6qPPb2fxyNn+dxdl/8pOfmDqbb8/6+rO+6Ozv\nw/Ic2PGsnp3t789+9jNTnzp1qqkzkpKSTJ3lAbD3T6j7P3PmzICalf+iTDshHEKGF8IhZHghHEKG\nF8IhZHghHEKGF8IhZHghHCKsfekvX74cUGN9yVmcl8VBWZyb1cOz45ctW2bqrB76L3/5S0g6S1U+\nffq0qScnJ5s6y4Ng89lZRWVqaqqpf+UrXzH1xYsXmzrrV/DDH/7Q1Nva2kyd9WNgcwNYX3uWpzA4\n43U4VkGbzvBCOIQML4RDyPBCOIQML4RDyPBCOIQML4RDyPBCOERY4/BWzTqrZ2d90QcGBkyd1YOz\nODKLk1o1xwCvV//73/9u6iyO297ebuqs3p/Nd4+JiTF1lkfB1sfq/a02TQCvJ6+srDT1U6dOmfrc\nuXNNnfXVZ+9fdnxvb6+p33///QE1y1s6wwvhEDK8EA4hwwvhEDK8EA4hwwvhEDK8EA4hwwvhEGGN\nw9+aIT8SLE7O6uHZ/GyrXhgAFi5caOqsnvpb3/qWqWdnZ5v6I488YuoHDhwwdVYvzWB5DGwW4NKl\nS0199erVpj579mxTZ33hGxoaTL2lpcXU2Xz748ePmzrLk2B5DNOmTTP1rq6uoI8Pej68EOL/Cxle\nCIeQ4YVwCBleCIeQ4YVwCBleCIeQ4YVwCDPY3dLSgqeffhodHR3w+Xx45pln8Pzzz6O7uxsrV65E\nc3MzMjIysGfPnhHrw62aaNb33OfzmXpUVJSpz5kzx9RDrYf+2te+ZuodHR2mXltba+pWLBUAmpub\nTf2ee+4x9TVr1pg6q0fPyckJ6fnr6+tNfevWrab+r3/9y9StmQgAz+PIyMgI6fGt3vAA7wfB5sN/\n+OGHAbXc3NzAj2s9aHR0NLZv345///vfeO+997Br1y6cPHkSVVVVKCkpQUNDA5YtW4aqqipzcUKI\nyMA0fHJyMgoLCwHc7HCSk5OD1tZW7N+/H+Xl5QCA8vJy7Nu3L/wrFUKEzB1fwzc1NaGurg5LlixB\ne3u7P/UxKSmJtjMSQkQGd5RL39vbi7KyMuzYseO2a0ufzxfwenvwdUxMTAzNLxZCBEd/fz/6+/sB\nAEePHg14P3qGv379OsrKyrB69WosX74cwM2z+rlz5wDcLFIJNDjxvvvu8//I7EKEj5iYGL/XrC9c\nTcN7noeKigrk5uZiw4YN/t+XlpaiuroaAFBdXe3/j0AIEdmYH+lra2uxe/du5Ofno6ioCACwbds2\nbN68Gd/+9rfx+uuv+8NyQojIxzT80qVLA/bXPnLkCH1wa4Z2qHFQNh+e1bOzens2P53Va7MvMh98\n8EFTZ/PpWRyX5QF897vfNfW4uDhTZ/Ppf/GLX5h6TU2NqXd2dpo6mwsQ6DLzFiyPg72/Qu1H0NTU\nZOosT8XqNaF6eCEEABleCKeQ4YVwCBleCIeQ4YVwCBleCIeQ4YVwiLD2pZ8/f35AjdUT38oLDgSL\ng7P576wvOavnZvO7J0+ebOqsL3teXp6pszh1amqqqbO5AL///e9Nne0vi8Oz57dyOAD++tn7i8Xp\n2fEsDs8en+1fdHS0qbe1tQXULly4EFDTGV4Ih5DhhXAIGV4Ih5DhhXAIGV4Ih5DhhXAIGV4Ihwhr\nHN6a0c76bofSlxvgfd0bGxtNndXTP/vss6a+fft2U2fz12fNmmXqM2fONHXWSZg1LbH+dsDNbkgW\nbP48qzdnfd1ZPTurl2f9ClieBOs3wPo9pKenm7oVZ2fHW7PjdYYXwiFkeCEcQoYXwiFkeCEcQoYX\nwiFkeCEcQoYXwiHCGoe3atpZPTSD1cOzODDr+836wnd3d5v6rem6wT4+q/dm9fgsD4Gtn9XT9/T0\nmDqLs7PRY1lZWabO4vSsn0JCQoKpB5rHcAs214DVw7M4PcsDserlrfe+zvBCOIQML4RDyPBCOIQM\nL4RDyPBCOIQML4RDyPBCOIQZTGxpacHTTz+Njo4O+Hw+PPPMM3j++edRWVmJ1157zT+jetu2bXj0\n0UdvO96KRbI4fEtLi6mfP3/e1FlfbxaHZ3F8Np/d6g0O8Hpy1vec1YM3NzebOqvHZnkAbH9ZnJrt\n74kTJ0zdmnkA8Dg76wvP+i3cf//9ps7yBJjO8iis/bNyIMy/SnR0NLZv347CwkL09vbiC1/4AkpK\nSuDz+bBx40Zs3LjRXJQQIrIwDZ+cnIzk5GQAQHx8PHJycvwZQOwMJYSIPO74Gr6pqQl1dXV44IEH\nAAA7d+5EQUEBKioq6McTIURkcEeG7+3txeOPP44dO3YgPj4e69evR2NjI+rr65GSkoJNmzaNeFxr\na6v/h+UOCyGCp6+vD11dXejq6sK7774b8H60eOb69esoKyvDU089heXLlwMY+mXcunXr8Nhjj414\nLCvAEELcHeLi4hAXFwcA+NKXvhTQ9OYZ3vM8VFRUIDc3Fxs2bPD/fnBH071799JJp0KIyMA8w9fW\n1mL37t3Iz89HUVERAGDr1q2oqalBfX09fD4fMjMz8eqrr47JYoUQoWEafunSpSPWBX/961+/owdn\nNdcWbL46q8e2enMDPE7MdBbHZfh8PlNnr4/FmVm9OYvjs/WxvvqxsbGmzuLw7HKQ5WGwfgNs/1i/\nAaazuQps/1k9v/X+tJ5bmXZCOIQML4RDyPBCOIQML4RDyPBCOIQML4RDyPBCOERY+9Jb8cC+vj7z\nWFaNl5mZaeosB4DFOePj402dxWHZ8SyOzV4/6/uem5tr6qzem/VVZ/XwbH46i7Ozfgjs9bP59tOn\nTzd11s8gLS3N1Nl8+ltpsIFgeQzW67d66usML4RDyPBCOIQML4RDjJnh2TX7eMOu6ccblns+3kR6\nv4NI37/Lly+PyfOMmeHZl1TjjQwfGjJ8aPzfGV4IMf6ENSy3YMEC/79PnjyJnJwc/212RmVhKRY2\nYuWlw9tkNzQ0IDs723+bhU3Y/8gsrMIYHvY7ffo05syZc8ePz8Zps+NZeenw8syrV69i3rx5/tvs\n78PCYqx8lbU5Hx6aGr5/LGzGyqvZ/o62zfTAwMCQ1tuhhK2tkKHPC1P7WVZPLYQILyNZO2xneLWx\nFiLy0DW8EA4hwwvhEDK8EA4hwwvhEDK8EA7xP3RU200aMirNAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x296c210>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADppJREFUeJzt3W9olfX/x/HX9at97zRLo52z0YSN6XLqdrZaKpU3Sucf\norUxSUfqcBNBCDGH5I1uzG7ovBGyljdELQbC0DuapA4TM0WI9WeDoZZCm4x1NskpOC2c9fne+P4c\nids1PTvXOUffzwccsPMmz9srn13uXNdxnnPOCYAJ/5fsBQAkDsEDhhA8YAjBA4YQPGAIwQOGBB58\nW1ubZsyYoenTp2vHjh1Bv9wjy8nJUVFRkUpKSjRnzpxkr6Pa2lqFw2EVFhaOPDc4OKiysjLl5+dr\n0aJFunHjRkrt19DQoOzsbJWUlKikpERtbW1J26+3t1dvvvmmZs2apdmzZ+uzzz6TlDrHcKz9EnYM\nXYDu3r3r8vLyXHd3t7tz546LRCLuwoULQb7kI8vJyXHXrl1L9hojzpw5437++Wc3e/bskec2b97s\nduzY4ZxzrrGx0X300UfJWm/U/RoaGtynn36atJ3+LRqNuo6ODuecczdv3nT5+fnuwoULKXMMx9ov\nUccw0DN8e3u7pk2bppycHKWlpWnFihX66quvgnzJmLgUuvdo/vz5mjJlyn3PHTlyRDU1NZKkmpoa\nHT58OBmrSRp9Pyl1jmFmZqaKi4slSenp6SooKFBfX1/KHMOx9pMScwwDDb6vr09Tp04d+efs7OyR\nX1yq8DxPCxcuVGlpqfbs2ZPsdUY1MDCgcDgsSQqHwxoYGEjyRg9qbm5WJBJRXV1dUr/k+Leenh51\ndHRo7ty5KXkM7+03b948SYk5hoEG73lekD99XJw7d04dHR06fvy4du3apbNnzyZ7JV+e56XccV2/\nfr26u7vV2dmprKws1dfXJ3slDQ0NqaqqSk1NTZo0adJ9s1Q4hkNDQ1q2bJmampqUnp6esGMYaPAv\nvviient7R/65t7dX2dnZQb7kI8vKypIkZWRkqLKyUu3t7Une6EHhcFj9/f2SpGg0qlAolOSN7hcK\nhUYiWrt2bdKP4fDwsKqqqrRq1SpVVFRISq1jeG+/lStXjuyXqGMYaPClpaW6fPmyenp6dOfOHR04\ncEDl5eVBvuQjuX37tm7evClJunXrlk6cOHHfu8+pory8XC0tLZKklpaWkd8kqSIajY78+NChQ0k9\nhs451dXVaebMmdq4cePI86lyDMfaL2HHMOh3BY8dO+by8/NdXl6e27ZtW9Av90h+++03F4lEXCQS\ncbNmzUqJ/VasWOGysrJcWlqay87Odl988YW7du2aW7BggZs+fborKytz169fT5n99u3b51atWuUK\nCwtdUVGRe/fdd11/f3/S9jt79qzzPM9FIhFXXFzsiouL3fHjx1PmGI6237FjxxJ2DD3nUuTtVQCB\n4047wBCCBwwheMCSWL/4P378uHvppZfctGnTXGNj4wNzSTx48EjiYzQxBf8w98gn+xfLg4fVR3V1\ntZNGTzumP9I/LvfIA7hfTME/DvfIA3hQTMEn+z5kAGPr6uoacxZT8I/DPfKAVX635cYUfKrfIw9g\ndE/H9C89/bQ+//xzLV68WH///bfq6upUUFAQ790AxFlMwUvS0qVLtXTp0njuAiBg3GkHGELwgCEE\nDxhC8IAhBA8YQvCAIQQPGELwgCEEDxhC8IAhBA8YQvCAIQQPGELwgCEEDxhC8IAhBA8YQvCAIQQP\nGELwgCEEDxhC8IAhBA8YQvCAIQQPGELwgCEEDxhC8IAhBA8YQvCAIQQPGBLz94fPycnRs88+q6ee\nekppaWlqb2+P514AAhBz8J7n6fTp03r++efjuQ+AAE3oj/TOuXjtASABYg7e8zwtXLhQpaWl2rNn\nTzx3AjABXV1dYw9djH7//XfnnHNXr151kUjEnTlz5r65JB48eCThUV1d7aTR0475DJ+VlSVJysjI\nUGVlJW/aAY+BmIK/ffu2bt68KUm6deuWTpw4ocLCwrguBiD+YnqXfmBgQJWVlZKku3fv6v3339ei\nRYviutjj7rnnnvOdv/rqq77zxYsX+87r6+t9519//bXv/Pz5877zifrxxx9956dOnfKdX79+PZ7r\n4P/FFHxubq46OzvjvQuAgHGnHWAIwQOGEDxgCMEDhhA8YAjBA4bE/Gk56zzP851/++23vvOioqIJ\n/fzjfXDp7bffntB8osbb7+7du77z5uZm3/knn3ziO793YxjuxxkeMITgAUMIHjCE4AFDCB4whOAB\nQwgeMITr8AEZ7zr6ePPH3Xi/vrS0NN/5hx9+6Dsf7/P2Bw4c8J1bxRkeMITgAUMIHjCE4AFDCB4w\nhOABQwgeMITr8DEa7/Pe69at851v3rzZdx6JRHzn06ZN852fPn3ad37x4kXfeV5enu/89ddf950/\n88wzvvOJetLvYwgKZ3jAEIIHDCF4wBCCBwwheMAQggcMIXjAEN/r8LW1tTp69KhCoZC6urokSYOD\ng1q+fLmuXLminJwcHTx4UJMnT07Iso+TH374wXe+fPly33k4HPadZ2Zm+s57e3t954ODg77z8a5z\n3/v9MJYZM2b4zpEcvmf4NWvWqK2t7b7nGhsbVVZWpkuXLmnBggVqbGwMdEEA8eMb/Pz58zVlypT7\nnjty5IhqamokSTU1NTp8+HBw2wGIq0f+Gn5gYGDkj5vhcFgDAwNxXwpAMCb0pp3nedzTDKQYv/dX\nHjn4cDis/v5+SVI0GlUoFIp9MwBxV1hYOObskYMvLy9XS0uLJKmlpUUVFRWxbwYgoXyDr66u1muv\nvaZff/1VU6dO1ZdffqktW7bom2++UX5+vk6dOqUtW7YkalcAE+R7Hb61tXXU50+ePBnIMpaM93n6\ne182xTqfqIULF/rOx/s8/kQNDQ35zi9fvhzo6z+puNMOMITgAUMIHjCE4AFDCB4whOABQwgeMIS/\nl96o8T4D8dZbb/nOn3462N86169f953/9NNPgb7+k4ozPGAIwQOGEDxgCMEDhhA8YAjBA4YQPGAI\n1+GN+s9//uM7H+86fNBu3bqV1Nd/UnGGBwwheMAQggcMIXjAEIIHDCF4wBCCBwzhOrxRc+bM8Z2X\nlpYG+vp//fWX73zZsmWBvr5VnOEBQwgeMITgAUMIHjCE4AFDCB4whOABQ3yvw9fW1uro0aMKhULq\n6uqSJDU0NGjv3r3KyMiQJG3fvl1LliwJflM8UYaHh33nv/zyS4I2scX3DL9mzRq1tbXd95znedq0\naZM6OjrU0dFB7MBjxDf4+fPna8qUKQ8875wLbCEAwYnpa/jm5mZFIhHV1dXpxo0b8d4JQEAeOfj1\n69eru7tbnZ2dysrKUn19fRB7AYjRvffbRvPIwYdCIXmeJ8/ztHbtWrW3t09oOQDxVVhYOObskYOP\nRqMjPz506JDvTw4gtfhelquurtZ3332nP/74Q1OnTtXWrVt1+vRpdXZ2yvM85ebmavfu3YnaFcAE\n+Qbf2tr6wHO1tbWBLYPEycvLS/YKSALutAMMIXjAEIIHDCF4wBCCBwwheMAQggcM4e+lf0J5nuc7\nr6ysTNAmSCWc4QFDCB4whOABQwgeMITgAUMIHjCE4AFDuA6PpNi1a1eyVzCJMzxgCMEDhhA8YAjB\nA4YQPGAIwQOGEDxgCNfhn1ChUMh3/tJLLwX6+uN9h+E///wz0NfH6DjDA4YQPGAIwQOGEDxgCMED\nhhA8YAjBA4b4Xofv7e3V6tWrdfXqVXmep3Xr1mnDhg0aHBzU8uXLdeXKFeXk5OjgwYOaPHlyonbG\nQ3jhhRd859OnT0/QJkglvmf4tLQ07dy5U+fPn9f333+vXbt26eLFi2psbFRZWZkuXbqkBQsWqLGx\nMVH7ApgA3+AzMzNVXFwsSUpPT1dBQYH6+vp05MgR1dTUSJJqamp0+PDh4DcFMGEP/TV8T0+POjo6\nNHfuXA0MDCgcDkuSwuGwBgYGAlsQQPw8VPBDQ0OqqqpSU1OTJk2adN/M87xxv48ZgMTp6uoaczZu\n8MPDw6qqqtKqVatUUVEh6X9n9f7+fklSNBod94MaABKnsLBwzJlv8M451dXVaebMmdq4cePI8+Xl\n5WppaZEktbS0jPyPAEBq870sd+7cOe3fv19FRUUqKSmRJG3fvl1btmzRe++9p3379o1clgOQ+nyD\nf+ONN/TPP/+MOjt58mQgCyE+3nnnnWSvgBTEnXaAIQQPGELwgCEEDxhC8IAhBA8YQvCAIfy99I+p\n8T6/MGPGjARtMrrx9svOzk7QJvg3zvCAIQQPGELwgCEEDxhC8IAhBA8YQvCAIVyHR1LcuXMn2SuY\nxBkeMITgAUMIHjCE4AFDCB4whOABQwgeMITr8AiE3/c3k6StW7cmaBP8G2d4wBCCBwwheMAQggcM\nIXjAEIIHDCF4wBDf6/C9vb1avXq1rl69Ks/ztG7dOm3YsEENDQ3au3evMjIyJEnbt2/XkiVLErIw\n/sc55zv/4IMPfOdXrlzxnX/88ce+8/Gus5eVlfnOr1275jtHMHyDT0tL086dO1VcXKyhoSG98sor\nKisrk+d52rRpkzZt2pSoPQHEgW/wmZmZyszMlCSlp6eroKBAfX19ksY/wwBIPQ/9NXxPT486Ojo0\nb948SVJzc7MikYjq6up048aNwBYEED8PFfzQ0JCWLVumpqYmpaena/369eru7lZnZ6eysrJUX18f\n9J4AHpLf+yvjBj88PKyqqiqtXLlSFRUVkqRQKCTP8+R5ntauXav29vb4bQtgQgoLC8ec+QbvnFNd\nXZ1mzpypjRs3jjwfjUZHfnzo0CHfFwCQOnzftDt37pz279+voqIilZSUSJK2bdum1tZWdXZ2yvM8\n5ebmavfu3QlZFsDEeC6gt9vH+/7gSK6g//twFSd5qqur1draOup/A+60AwwheMAQggcMIXjAEIIH\nDCF4wBCCBwzh76U3iuvkNnGGBwwheMAQggcMIXjAEIIHDCF4wJBAL8u9/PLLIz+ORqPKysoK8uUm\nhP0mhv0mJp775ebmjjnj8/DAE2q0tAM7w3NjB5B6+BoeMITgAUMIHjCE4AFDCB4w5L+U7JljJUoK\nLQAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x29916d0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAwvhEP8D8LKxLEMWNJ1AAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x6397b90>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}