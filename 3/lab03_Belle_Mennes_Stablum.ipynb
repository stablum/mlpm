{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lab 3: Bayesian PCA\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction\n",
      "\n",
      "In this lab assignment, we will implement a variational algorithm for Bayesian PCA. Unlike regular PCA based on maximization of retained variance or minimization of projection error (see Bishop, 12.1.1 and 12.1.2), probabilistic PCA defines a proper density model over observed and latent variables. We will work with a fully Bayesian model this time, which is to say that we will put priors on our parameters and will be interested in learning the posterior over those parameters. Bayesian methods are very elegant, but require a shift in mindset: we are no longer looking for a point estimate of the parameters (as in maximum likelihood or MAP), but for a full posterior distribution.\n",
      "\n",
      "The integrals involved in a Bayesian analysis are usually analytically intractable, so that we must resort to approximations. In this lab assignment, we will implement the variational method described in Bishop99. Chapters 10 and 12 of the PRML book contain additional material that may be useful when doing this exercise.\n",
      "\n",
      "* [Bishop99] Variational Principal Components, C. Bishop, ICANN 1999 - http://research.microsoft.com/pubs/67241/bishop-vpca-icann-99.pdf\n",
      "\n",
      "Below, you will find some code to get you started."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import scipy.special as sp\n",
      "\n",
      "class BayesianPCA(object):\n",
      "    \n",
      "    def __init__(self, d, N, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        self.d = d # number of dimensions\n",
      "        self.N = N # number of data points\n",
      "        \n",
      "        # Hyperparameters\n",
      "        self.a_alpha = a_alpha\n",
      "        self.b_alpha = b_alpha\n",
      "        self.a_tau = a_tau\n",
      "        self.b_tau = b_tau\n",
      "        self.beta = beta\n",
      "\n",
      "        # Variational parameters\n",
      "        self.means_z = np.random.randn(d, N) # called x in bishop99\n",
      "        self.sigma_z = np.random.randn(d, d)\n",
      "        self.mean_mu = np.random.randn(d, 1)\n",
      "        self.sigma_mu = np.random.randn(d, d)\n",
      "        self.means_w = np.random.randn(d, d)\n",
      "        self.sigma_w = np.random.randn(d, d)\n",
      "        self.a_alpha_tilde = np.abs(np.random.randn(1))\n",
      "        self.bs_alpha_tilde = np.abs(np.random.randn(d, 1))\n",
      "        self.a_tau_tilde = np.abs(np.random.randn(1))\n",
      "        self.b_tau_tilde = np.abs(np.random.randn(1))\n",
      "    \n",
      "    def __update_z(self, X):\n",
      "        #Update means of z\n",
      "        exp_tau = self.a_tau_tilde / self.b_tau_tilde\n",
      "        exp_Wt = np.transpose(self.means_w)\n",
      "        exp_mu = self.mean_mu\n",
      "        self.means_z = np.dot( np.dot( np.dot(exp_tau, self.sigma_z), exp_Wt), (X-exp_mu) )\n",
      "        \n",
      "        #update sigma of z\n",
      "        exp_WtW = numpy.trace(self.sigma_w) + np.dot(np.transpose(self.means_w), self.means_w)\n",
      "        print expWtW\n",
      "        # sigma w = identity+ exp_tau*expWtW inverse\n",
      "        self.sigma_z = numpy.identity(2)\n",
      "    \n",
      "    def __update_mu(self):\n",
      "        pass\n",
      "    \n",
      "    def __update_w(self, X):\n",
      "        pass\n",
      "    \n",
      "    def __update_alpha(self):\n",
      "        pass\n",
      "\n",
      "    def __update_tau(self, X):\n",
      "        pass\n",
      "\n",
      "    def L(self, X):\n",
      "        # p(X)\n",
      "        p_x = - (self.N / 2 ) * np.trace(self.sigma_z) \n",
      "        for i in range(self.N):\n",
      "            p_x += np.dot(self.sigma_z[:,i], self.sigma_z[:,i].T)\n",
      "\n",
      "        print \"P(X):\", p_x\n",
      "        \n",
      "        # p(W|\\alpha)\n",
      "        p_w_alpha = (self.d / float(4) ) * np.log(2 * np.pi)\n",
      "        p_w_alpha_temp = (self.d * (sp.psi(self.a_alpha_tilde[0] )) )\n",
      "\n",
      "        for i in range(d):\n",
      "            p_w_alpha_temp -= np.log( self.bs_alpha_tilde[i,:][0] )\n",
      "        for i in range(d):\n",
      "            p_w_alpha_temp -= ((self.d * (self.a_alpha_tilde[0]**2 + self.a_alpha_tilde[0] )) / (self.bs_alpha_tilde[i,:][0]**2))        \n",
      "        p_w_alpha = p_w_alpha * p_w_alpha_temp\n",
      "        \n",
      "        print \"P(W|\\\\alpha):\", p_w_alpha\n",
      "            \n",
      "        # P(\\alpha)        \n",
      "        p_alpha = (self.a_alpha - 1) * (d * sp.psi(self.a_alpha_tilde[0]) -  np.sum(map(lambda x: np.log(x),  self.bs_alpha_tilde)))\n",
      "        p_alpha -= self.b_alpha * np.sum(map(lambda x: (self.a_alpha_tilde[0] / float( x)),  self.bs_alpha_tilde))\n",
      "\n",
      "        print \"P(\\\\alpha):\", p_alpha\n",
      "        \n",
      "        \n",
      "        # P(\\mu)\n",
      "        \n",
      "        \n",
      "        L = p_x + p_w_alpha + p_alpha\n",
      "        return L\n",
      "    \n",
      "    def fit(self, X):\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = 3\n",
      "N = 2\n",
      "\n",
      "pca = BayesianPCA(d, N)\n",
      "X = np.random.randn(d, N)\n",
      "pca.fit(X)\n",
      "\n",
      "pca.L(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P(X): 5.31244246819\n",
        "P(W|\\alpha): -23.5330936764\n",
        "P(\\alpha): 3.00128708842\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "-18.220651208208736"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "sp.psi(np.abs(np.random.randn(1))[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "-1.1539174538099943"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import display, Math, Latex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1. The Q-distribution (5 points)\n",
      "\n",
      "In variational Bayes, we introduce a distribution $Q(\\Theta)$ over parameters / latent variables in order to make inference tractable. We can think of $Q$ as being an approximation of a certain distribution. What function does $Q$ approximate, $p(D|\\Theta)$, $p(\\Theta|D)$, $p(D, \\Theta)$, $p(\\Theta)$, or $p(D)$, and how do you see that?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function $Q$ approximates the function that is the posterior distribution, $p(\\Theta | D) $.\n",
      "\n",
      "The posterior $P(\\Theta | D)$ is given by multiplying the likelihood by the prior and then normalizing:\n",
      "\n",
      "\\begin{align}\n",
      "P(\\Theta | D) &= \\dfrac{P(D | \\Theta)P( \\Theta)}{P(D)} \\\\\n",
      "              &= \\dfrac{P(D | \\Theta)P( \\Theta)}{\\int P(D | \\Theta)P(\\Theta) \\hspace{1mm} d\\Theta}\n",
      "\\end{align}\n",
      "\n",
      "Why is this useful? \n",
      "For Maximum a Posteriori (MAP) learning, we could use the posterior to get a point estimate of the parameters:\n",
      "\n",
      "\\begin{align}\n",
      "\\widetilde{\\Theta}_{\\text{MAP}}(D)&= \\arg\\max_{\\Theta} P(\\Theta | D) \\\\\n",
      "            & \\propto {P(D | \\Theta)P( \\Theta)}\n",
      "\\end{align}\n",
      "\n",
      "So for MAP learning the normalization (and therefore integration) is not needed. Like Maximum likelihood estimation (MLE) learning, MAP learning learns a point estimate of our parameters.\n",
      "\n",
      "In the Bayesian framework no fixed parameter setting is chosen. The posterior is used to obtain a predictive density (also called the posterior predictive distribution) for a new data point $t^{*}$ with:\n",
      "\n",
      "\\begin{align}\n",
      "P(t^{*} | D) &= \\int P(t^{*} | \\Theta) P(\\Theta | D) \\hspace{2mm} d\\Theta\n",
      "\\end{align}\n",
      "\n",
      "This means that in the Bayesian learning framework we consider all possible parametrizations of our models by incorporating the posterior this way. Each of these is weighted by the evidence for them in $D$.\n",
      "(Note that when predicting, we also return a probability distribution as prediction for $t^{*}$. \n",
      "Retaining all possible parametrizations is in this case of Principal Component Analysis) particularly advantageous for automatically determining an adequate number of principal components given $D$.\n",
      "\n",
      "Note that we need to integrate over all possible parametrizations for this. In practice this can quickly become infeasible for models with many parameters; when using a mixture of models; or when the prior distribution is not of the same functional form as the likelihood distribution. Therefore we use a ''simpler'' distribution $Q(\\Theta)$ to approximate $P(\\Theta|D)$. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2. The mean-field approximation (15 points)\n",
      "\n",
      "Equation 13 from [Bishop99] is a very powerful result: assuming only that $Q(\\Theta)$ factorizes in a certain way (no assumptions on the functional form of the factors $Q_i$!), we get a set of coupled equations for the $Q_i$.\n",
      "\n",
      "However, the expression given in eq. 13 for Q_i contains a small mistake. Starting with the expression for the lower bound $\\mathcal{L}(Q)$, derive the correct expression (and include your derivation). You can proceed as follows: first, substitute the factorization of $Q$ (eq. 12) into the definition of $\\mathcal{L}(Q)$ and separate $\\mathcal{L}(Q)$ into $Q_i$-dependent and $Q_i$-independent terms. At this point, you should be able to spot the expectations $\\langle\\cdot\\rangle_{k \\neq i}$ over the other $Q$-distributions that appear in Bishop's solution (eq. 13). Now, keeping all $Q_k, k \\neq i$ fixed, maximize the expression with respect to $Q_i$. You should be able to spot the form of the optimal $ln Q_i$, from which $Q_i$ can easily be obtained."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is given that the lower bound is \n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q(\\Theta) \\log \\dfrac{P(D, \\Theta)}{Q(\\Theta)}\n",
      "                \\hspace{3mm} d \\Theta  \\\\\n",
      "\\end{align} \n",
      "$$\n",
      "\n",
      "And the factorization is\n",
      "\n",
      "\\begin{equation}\n",
      "Q(\\Theta)= \\prod_{k} Q_k(\\Theta_k)\n",
      "\\end{equation}\n",
      "\n",
      "Plugging in the factorization into the lower bound, we get\n",
      "\n",
      "\\begin{align}                \n",
      "\\mathcal{L}(Q) =&  \\int \n",
      "                     \\prod_{k} Q_k( \\Theta_k) \n",
      "                     \\left[\n",
      "                         \\log P(D,\\Theta) - \\log \\left( \\prod_k Q_k(\\Theta_k) \\right)\n",
      "                     \\right]\n",
      "                 \\hspace{3mm} d \\Theta  \\\\\n",
      "              =& \\int\n",
      "              \\prod_{k} Q_k( \\Theta_k) \n",
      "                     \\left[\n",
      "                         \\log P(D,\\Theta) - \\sum_k \\log Q_k(\\Theta_k)\n",
      "                     \\right]\n",
      "                  \\hspace{3mm} d \\Theta\n",
      "\\end{align}  \n",
      "\n",
      "By denoting a specific factor $Q_i$ separately we arrive at\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q_i( \\Theta_i) \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "                \\left[ \\log P(D, \\Theta) - \\log \\left( \\prod_{j \\neq i}  Q_j(\\Theta_j) \\right) - \\log Q_i (\\Theta_i) \\right] \n",
      "                \\hspace{3mm} d \\Theta  \n",
      "\\end{align}                \n",
      "\n",
      "By taking the integral of $Q_i$ apart from those for $\\{Q_1, \\dots ,Q_{i-1},Q_{i+1}, \\dots ,Q_M \\}$, we get\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "    \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "            \\log \\left( \\prod_{j \\neq i} Q_j(\\Theta_j) \\right)\n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{3mm} - \\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[ \n",
      "            \\int\n",
      "             \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Now, when we keep the $Q_k \\neq Q_{i}$ fixed the following happens, the integral over $\\Theta_{\\setminus i}$  in the third row of the equation for $\\mathcal{L}(Q)$ will become 1, because all $Q_n(\\Theta_n)$ are probability distributions. \n",
      "\n",
      "\\begin{align}\n",
      "\\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[ \n",
      "            \\int\n",
      "             \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "        =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[\n",
      "        \\prod_{k \\neq i}         \n",
      "            \\int\n",
      "              Q_k( \\Theta_k) \n",
      "             \\hspace{3mm} d \\Theta_{k}\n",
      "        \\right]        \n",
      "        \\hspace{3mm} d \\Theta_{i} \\\\\n",
      "     =&\n",
      "      \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)\n",
      "        \\left[\n",
      "        \\prod_{k \\neq i}         \n",
      "            1\n",
      "        \\right]        \n",
      "        \\hspace{3mm} d \\Theta_{i} \n",
      "\\end{align}\n",
      "\n",
      "So we can rewrite the equation as\n",
      "\n",
      "\\begin{align}       \n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "    \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \n",
      "            \\log \\left( \\prod_{j \\neq i} Q_j(\\Theta_j) \\right)\n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{3mm} - \\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}         \n",
      "\\end{align}\n",
      "\n",
      "\n",
      "Furthermore, when we keep the $Q_k \\neq Q_{i}$ fixed, the $\\Theta_{\\setminus i}$ integral in the second row of the equation for  $\\mathcal{L}(Q)$ will become a constant. We know that $\\int Q_i(\\Theta_i)  d\\Theta_{i}$ should be 1 because $Q_i$ is a probability distribution. So the whole second row can be viewed as a constant. \n",
      "\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\left[ \n",
      "            \\int  \\prod_{k \\neq i} Q_k( \\Theta_k) \\log P(D, \\Theta) \n",
      "            \\hspace{3mm} d \\Theta_{\\setminus i}\n",
      "        \\right]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{1mm} + \\text{const}        \n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the integral over $\\Theta_{\\setminus i}$  in the  first line in $\\mathcal{L}(Q)$ could be rewritten as an expectation :\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =& \n",
      "    \\int Q_i(\\Theta_i) \\cdot\n",
      "        \\mathbb{E}_{[k \\neq i]}[ \\log P(D, \\Theta)]\n",
      "        \\hspace{3mm} d \\Theta_{i} \\hspace{3mm} -\\\\\n",
      "& \n",
      "    \\int Q_i(\\Theta_i) \n",
      "        \\log Q_i (\\Theta_i)    \n",
      "        \\hspace{3mm} d \\Theta_{i}  \\hspace{1mm} + \\text{const}        \n",
      "\\end{align}\n",
      "\n",
      "Where $\\mathbb{E}_{[k \\neq i]}$ denotes this expected value is with respect to the $Q$ distributions over all variables $\\Theta_k \\neq \\Theta_i$.\n",
      "\n",
      "\n",
      "We can define $\\mathbb{E}_{[k \\neq i]}[ \\log P(D, \\Theta)]$ as $\\log \\widetilde{P}(D, \\Theta_i)$.\n",
      "\n",
      "Intuitively it makes sense to see  $\\log \\widetilde{P}(D, \\Theta_i)$, the joint probability of that parameter and the data, as the expectation of $\\log P(D, \\Theta)$ under all $\\Theta_k$ that are not $\\Theta_i$ , i.e., to not marginalize out  $\\Theta_i$ in $\\log P(D, \\Theta)$.\n",
      "It is here particularly useful to be able to rewrite the above equation as:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q)  =&  \\int Q_i(\\Theta_i) \\log \\widetilde{P}(D, \\Theta_i) -  \\int Q_i(\\Theta_i) \\log Q_i(\\Theta_i) \\\\\n",
      "                =& \\int Q_i(\\Theta_i) \\log \\dfrac{\\widetilde{P}(D , \\Theta_i)}{ Q_i(\\Theta_i)} \\\\\n",
      "                =& - \\text{KL} \\left(Q_i(\\Theta_i) || \\widetilde{P}(D, \\Theta_i) \\right)\n",
      "\\end{align}\n",
      "\n",
      "Recall we wanted to maximize the lower bound function $\\mathcal{L}(Q)$. We see that this negative KL-divergence will be maximal when \n",
      "\n",
      "\\begin{align}\n",
      " Q_i(\\Theta_i) &= \\widetilde{P}(D, \\Theta_i)  \\\\  \n",
      " \\log  Q_i(\\Theta_i) &= \\log \\widetilde{P}(D, \\Theta_i)  \\\\\n",
      "                     &= \\mathbb{E}_{[k \\neq i]}[\\log P(D, \\Theta)]\\\\\n",
      " Q_i(\\Theta_i)      &= \\exp \\left( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] \\right)    \n",
      "\\end{align}\n",
      "\n",
      "So in line with Bishop (except a subscript mismatch on his side), accounting for normalization (marginalizing over all $\\Theta_{\\setminus i}$) we get that the optimal value for $Q_i$ is \n",
      "\n",
      "\\begin{align}\n",
      " Q_i^{*}(\\Theta_i)      &= \\dfrac{\\exp ( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] )   }\n",
      "                      {\\int \\exp ( \\mathbb{E}_{[k \\neq i]} [\\log P(D, \\Theta)] ) \\hspace{2mm} d\\Theta_i  }\n",
      "\\end{align}\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3. The log-probability (10 points)\n",
      "\n",
      "Write down the log-prob of data and parameters, $\\ln p(\\mathbf{X}, \\mathbf{Z}, \\mathbf{W}, \\mathbf{\\alpha}, \\tau, \\mathbf{\\mu})$, in full detail (where $\\mathbf{X}$ are observed, $\\mathbf{Z}$ is latent; this is different from [Bishop99] who uses $\\mathbf{T}$ and $\\mathbf{X}$ respectively, but $\\mathbf{X}$ and $\\mathbf{Z}$ are consistent with the PRML book and are more common nowadays). Could we use this to assess the convergence of the variational Bayesian PCA algorithm? If yes, how? If no, why not?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The log-probability of the data and parameters is defined as:\n",
      "$$\n",
      "\\begin{align}\n",
      "\\ln p ( X, Z, W, \\alpha, \\tau, \\mu) =& \n",
      "\\ln \\left(  \\prod_{n=1}^N p(x_n| z_n, W, \\mu, \\tau)p(Z)p(W|\\alpha)p(\\alpha)p(\\mu)p(\\tau) \\right) \\\\\n",
      "=& \\ln \\left( \\prod_{n=1}^N \\left[ \\left( \\frac{\\tau}{2 \\pi} \\right) ^{D/2} \\exp \\left( - \\frac{\\tau}{2} \\parallel x_t-Wz_n-\\mu \\parallel ^2  \\right) \\right]\n",
      "\\prod_{n=1}^N \\left[  \\left( \\frac{1}{2 \\pi} \\right)^{q/2} \\exp \\left( - \\frac{1}{2} z_n^\\top z_n \\right) \\right] \\\\\n",
      "\\prod_{i=1}^q \\left[ \\left( \\frac{\\alpha_i}{2 \\pi} \\right)^{D/2} \\exp \\left( -\\frac{\\alpha_i}{2}  w_i^\\top w_i \\right) \\right]\n",
      "\\prod_{i=1}^q \\left[ \\frac{1}{\\Gamma(a_\\alpha)} b_\\alpha^{a_\\alpha} \\alpha_i^{a_\\alpha-1} e^{-b_\\alpha \\: \\alpha_i} \\right]\\\\ \n",
      "\\left[ \\left(\\frac{ \\beta}{2\\pi} \\right)^{D/2} \\exp \\left( - \\frac{\\beta}{2} \\mu^\\top \\mu \\right) \\right]\n",
      "\\left[ \\frac{1}{\\Gamma(c_\\tau)} d_\\tau^{c_\\tau} \\tau^{c_\\tau-1} e^{-d_\\tau \\: \\tau} \\right] \\right) \n",
      "\\\\\n",
      "=& \\frac{ND}{2} \\ln \\frac{\\tau}{2 \\pi} + \\sum_{n=1}^N \\left[ - \\frac{\\tau}{2} \\parallel x_n-Wz_n-\\mu \\parallel ^2 \\right]\n",
      "+ \\frac{Nq}{2} \\ln \\frac{1}{2 \\pi} + \\sum_{n=1}^N \\left[ - \\frac{1}{2} z_n^\\top z_n \\right] \\\\\n",
      "&+ \\sum_{i=1}^q \\left[ \\frac{D}{2} \\ln \\frac{\\alpha_i}{2 \\pi} \\right] + \\sum_{i=1}^q \\left[ - \\frac{\\alpha_i}{2} w_i^\\top w_i \\right]\n",
      "+ \\: q \\ln \\frac{ b_\\alpha^{a_\\alpha}}{\\Gamma(a_\\alpha)} + \\sum_{i=1}^q \\left[ (a_\\alpha -1) \\ln \\alpha_i - b_\\alpha \\alpha_i \\right] \\\\\n",
      "&+ \\frac{D}{2} \\ln \\frac{\\beta}{2 \\pi} - \\frac{\\beta}{2} \\mu^\\top\\mu - \\ln \\Gamma(c_\\tau) + \\ln d_\\tau^{c_\\tau} + (c_\\tau-1)\\ln \\tau - d_\\tau \\tau\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bayesian learning results in a posterior distribution over the parameters. If we wish to use $\\ln P(\\Theta,D)$ to assess convergence we have no choice but to use some point estimate of the parameters from the approximated posterior $Q$ to evaluate  $\\ln P(\\Theta,D)$ (we cannot plug in the distribution). However, we consider convergence as $Q$ being as similar to the real posterior $P(\\Theta|D)$ as possible. There is no guarantee that if $Q$ becomes more similar to $P(\\Theta|D)$, that the used point estimate of $Q$ results in a higher value when evaluating $\\ln P(\\Theta,D)$. Therefore $\\ln P(\\Theta,D)$ is not a suitable option to assess convergence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4. The lower bound $\\mathcal{L}(Q)$ (25 points)\n",
      "\n",
      "Derive an expression for the lower bound $\\mathcal{L}(Q)$ of the log-prob $\\ln p(X)$ for Bayesian PCA, making use of the factorization (eq. 12) and the form of the Q-distributions (eq. 16-20) as listed in [Bishop99]. Show your steps. Implement this function.\n",
      "\n",
      "The following result may be useful:\n",
      "\n",
      "For $x \\sim \\Gamma(a,b)$, we have $\\langle \\ln x\\rangle = \\ln b + \\psi(a)$, where $\\psi(a) = \\frac{\\Gamma'(a)}{\\Gamma(a)}$ is the digamma function (which is implemented in numpy.special)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The lower bound can be defined as"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q(\\Theta) \\ln \\dfrac{P(D,\\Theta)}{Q(\\Theta)} \\hspace{2mm} d \\Theta\\\\\n",
      "                =& \\int Q(\\Theta) \\ln P(D,\\Theta) \\hspace{2mm} d \\Theta  - \\int Q(\\Theta) \\ln Q(\\Theta)  \\hspace{2mm} d \\Theta\\\\\n",
      "                = & \\mathbb{E}_Q[ \\ln P(D,\\Theta)] - \\mathbb{E}_Q[\\ln Q(\\Theta)]\\\\\n",
      "                ~\n",
      "               =& \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} P (t_n | x_n, W, \\mu, \\tau) \\right) \\right] \\\\\n",
      "               & + \\mathbb{E}_Q \\left[ \\ln P(X) \\right ] \\\\\n",
      "               & + \\mathbb{E}_Q \\left[\\ln  P(W | \\alpha ) \\right] \\\\\n",
      "               & + \\mathbb{E}_Q \\left[ \\ln P(\\alpha) \\right]\\\\\n",
      "               &+ \\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln Q(X) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln Q(W) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( Q(\\alpha) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln Q(\\mu) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( Q(\\tau) \\right) \\right]\\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we can treat $ P (t_n | x_n, W, \\mu, \\tau)  $  as a constant because $t$ is observed. So $\\mathcal{L}(Q)$ becomes\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n|0, I_q) \\right) \\right]              \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} (\\dfrac{\\alpha_i}{2 \\pi})^{d/2}  \\exp( - \\dfrac{1}{2} \\alpha_i ||w_i||^2 ) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma ( \\alpha_i| a_{\\alpha}, b_{\\alpha}) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu|0, \\beta^{-1} I) \\right) \\right] \\\\\n",
      "                & + \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau|c_\\tau, d_\\tau ) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n| m_x^{(n)}, \\Sigma_x) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{k=1}^{d}  \\mathcal{N}(\\widetilde{w}_k | m_w^{(k)}, \\Sigma_w) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma(\\alpha_i| \\widetilde{a}_\\alpha, \\widetilde{b}_{\\alpha i} ) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu | m_\\mu, \\Sigma_\\mu) \\right) \\right] \\\\\n",
      "                & - \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau | \\widetilde{a}_\\tau, \\widetilde{b}_\\tau ) \\right) \\right] \\\\\n",
      "                & + \\text{const}\n",
      "\\end{align}\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have evaluated the ten integrals corresponding to these ten expectations analytically. \n",
      "\n",
      "Below we give the derivation for each of them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      " \\mathbb{E}_Q \\left[ \\ln  P(X) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n|0, I_q) \\right) \\right]   \\\\\n",
      "= & \\int Q(\\Theta) \\sum_{n=1}^N \\ln \\left( \\mathcal{N}(x_n | 0, I_q \\right) d \\Theta\\\\\n",
      "= & \\sum_{n=1}^N \\int Q(x_n) \\ln \\left( \\mathcal{N} (x_n | 0, I_q \\right) d x_n \\\\\n",
      "= & \\sum_{n=1}^N \\int Q(x_n)\\left[ - \\dfrac{1}{2} (x_n - 0)^{T} I_q^{-1} (x_n - 0) - \\dfrac{q}{2} \\ln ( 2 \\pi)\n",
      "            - \\dfrac{1}{2} \\ln (\\text{det}(I_q)) \\right] d x_n \\\\\n",
      "= & -\\dfrac{1}{2} \\sum_{n=1}^N \\int Q(x_n) x_n^{T}x_n d x_n - \\dfrac{q}{2} \\ln (2 \\pi) \\sum_{n=1}^N \\int Q(x_n) d x_n -\n",
      "                    \\dfrac{1}{2} \\ln (\\text{det} (I_q)) \\sum_{n=1}^N \\int Q(x_n ) d x_n \\\\\n",
      "= & - \\dfrac{1}{2} \\sum_{n=1}^{N} \\int Q(x_n) x_n^{T}x_n d x_n + \\text{const} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Rule 355 of <a href=\"http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\"> the Matrix Cookbook</a> says that $ \\mathbb{E}_P[x^{T}Ax] = \\text{Tr}(A \\Sigma) + m^{T} A m $ where $m$ and $\\Sigma$ are the mean and covariance of $P$, so\n",
      "\n",
      "\\begin{align}\n",
      " \\mathbb{E}_Q \\left[ \\ln  P(X) \\right]   = & \n",
      "     -\\dfrac{1}{2} \\left( N \\cdot \\text{Tr}(\\Sigma_x) + \\sum_{n=1}^N {m_x^{(n)}}^{T}  {m_x^{(n)}} \\right) + \\text{const}\\\\\n",
      "     = & -\\dfrac{N}{2} \\text{Tr}(\\Sigma_x) -\\dfrac{1}{2}  \\sum_{n=1}^N {m_x^{(n)}}^{T}  {m_x^{(n)}} + \\text{const}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q [\\ln P(W|\\alpha)] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} (\\dfrac{\\alpha_i}{2 \\pi})^{d/2}  \\exp( - \\dfrac{1}{2} \\alpha_i ||w_i||^2 ) \\right) \\right]\\\\\n",
      "=& \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) \\int Q(\\alpha_i) \\left[ \\int Q(w_i) \\left( \\ln (\\alpha_i) - \\dfrac{1}{2} \\alpha_i w_i^{T}w_i \\right)  d w_i \\right] d \\alpha_i \\\\\n",
      "=&  \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) - \\dfrac{1}{2} \\int Q(\\alpha_i) \\left( \\ln(\\alpha_i) - \\alpha_i \\left[ \\int Q(w_i) w_i^{T} w_i  d w_i\\right] \\right)  d \\alpha_i \\\\\n",
      "\\end{align}\n",
      "\n",
      "Again we can use rule 355 from the Matrix Cookbook.\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q [\\ln P(W|\\alpha)] =& \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) (- \n",
      "    \\dfrac{1}{2}) \\int Q(\\alpha_i) \n",
      "        \\left( \\ln(\\alpha_i) - \\alpha_i( \\text{Tr}(\\alpha_i I) \\right) d \\alpha_i \\\\\n",
      "= & \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) (- \n",
      "    \\dfrac{1}{2}) \\int Q(\\alpha_i)      \n",
      "        \\left( \\ln(\\alpha_i) - d \\alpha_i^{2} \\right)\n",
      "        d \\alpha_i \\\\\n",
      "= & \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi)( - \n",
      "    \\dfrac{1}{2})\n",
      "    \\left( \n",
      "        \\int Q(\\alpha_i)  \\ln(\\alpha_i) d \\alpha_i\n",
      "        -\n",
      "        d \\int Q(\\alpha_i) \\alpha_i^2 d \\alpha_i\n",
      "        \\right)        \\\\\n",
      "\\end{align}\n",
      "\n",
      "We can make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;\n",
      "\n",
      "and we can make use of the fact that $\\mathbb{E}_P[x^2] = \\dfrac{(a+1)a}{b^2}$ if $P(x) = \\Gamma(x|a,b)$:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q [\\ln P(W|\\alpha)] = &  \\sum_{i=1}^q -\\dfrac{d}{2} \\ln( 2 \\pi) (- \n",
      "    \\dfrac{1}{2})\n",
      "        \\left(\n",
      "            (\\psi(\\widetilde{a}_\\alpha) - \\ln( \\widetilde{b}_{\\alpha i} ) )\n",
      "            - d \\dfrac{ ( \\widetilde{a}_\\alpha + 1 ) \\widetilde{a}_\\alpha}\n",
      "                        { \\widetilde{b}_{\\alpha i}^{2} }\n",
      "         \\right)\\\\\n",
      "= &  \\dfrac{d}{4} \\ln( 2 \\pi) \\sum_{i=1}^q \n",
      "            \\left( \n",
      "              (  \\psi(\\widetilde{a}_\\alpha) - \\ln ( \\widetilde{b}_{\\alpha i}) )-\n",
      "                    \\dfrac{d ( \\widetilde{a}_{\\alpha} + 1 ) \\widetilde{a}_\\alpha }\n",
      "                            { \\widetilde{b}^{2}_{\\alpha i} }\n",
      "              \\right)            \\\\\n",
      "= & \\dfrac{d}{4} \\ln(2 \\pi) \\left( q \\cdot \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q \\ln (\\widetilde{b}_{\\alpha i}) \n",
      "                               - \\sum_{i=1}^{q} \\left( \\dfrac{d (\\widetilde{a}_\\alpha^2 + \\widetilde{a}_\\alpha) } {\\widetilde{b}_{\\alpha i}^2 } \\right) \\right)               \n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\alpha) \\right] =& \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma ( \\alpha_i| a_{\\alpha}, b_{\\alpha}) \\right) \\right]\\\\\n",
      "= & \\int Q(\\alpha) \\sum_{i=1}^q \\ln( \\Gamma(\\alpha_i|a_\\alpha, b_\\alpha)) d \\alpha \\\\\n",
      "=& \\sum_{i=1}^q  \\int Q(\\alpha_i) \\ln (\\Gamma (\\alpha_i| a_\\alpha , b_\\alpha)) d\\alpha_i \\\\\n",
      "= & \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left( \\dfrac{1}{\\Gamma(a_\\alpha)} \\right) d \\alpha_i +\n",
      "   \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln (b_{\\alpha}^{a_\\alpha} )d \\alpha_i  + \\\\\n",
      "    & \\sum_{i=1}^q \\int Q(\\alpha_i)  \\ln \\left( \\alpha_{i}^{a_\\alpha - 1} \\right) d \\alpha_i + \n",
      "    \\sum_{i=1}^q \\int Q(\\alpha_i) (- b_\\alpha \\alpha_i ) d \\alpha_i \\\\\n",
      "= & \\text{const} +  \n",
      "         (a_\\alpha - 1) \\sum_{i=1}^q   \\int Q(\\alpha_i)  \\ln \\left( \\alpha_{i} \\right) d \\alpha_i - \n",
      "         b_\\alpha \\sum_{i=1}^q \\int Q(\\alpha_i)  \\alpha_i  d \\alpha_i \\\\\n",
      "\\end{align}\n",
      "\n",
      "We make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;  \n",
      "and \n",
      "of the fact that $\\mathbb{E}_P[x] = \\dfrac{a}{b}$ if $P(x) = \\Gamma(x|a,b)$.\n",
      "Then:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\alpha) \\right] =& \n",
      "     \\cdot  (a_\\alpha - 1) \\cdot  \\sum_{i=1}^q   \\left(\\psi(\\widetilde{a}_\\alpha) - \\ln(\\widetilde{b}_{\\alpha, i})\\right)\n",
      "     -\n",
      "      b_\\alpha \\sum_{i=1}^q  \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha, i}} \\right)\n",
      "     + \\text{const} \\\\\n",
      "     = &    (a_\\alpha - 1) \\left(  q \\cdot  \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q  \\left( \\ln(\\widetilde{b}_{\\alpha, i})\\right) \\right)\n",
      "     -\n",
      "      b_\\alpha \\sum_{i=1}^q  \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha, i}} \\right)\n",
      "     + \\text{const} \n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu|0, \\beta^{-1} I) \\right) \\right]\\\\\n",
      "= & \\mathbb{E}_Q \\left[ - \\dfrac{d}{2}  \\ln ( 2 \\pi)  - \\dfrac{1}{2} \\mu^{T} \\beta \\mu  - \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) \\right]\\\\\n",
      "= & -  \\int Q(\\Theta)  \\dfrac{d}{2} \\ln \\left( {2\\pi} \\right) + \\left( \\dfrac{\\beta}{2} \\mu^{T}\\mu \\right)   + \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) d \\Theta \\\\\n",
      "= & - \\int Q(\\mu)  \\dfrac{d}{2} \\ln \\left( {2\\pi} \\right) + \\left( \\dfrac{\\beta}{2} \\mu^{T}\\mu \\right)   + \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) d \\mu \\\\\n",
      "= & - \\dfrac{d}{2} \\ln \\left( {2\\pi} \\right)  \\int Q(\\mu)  d \\mu - \n",
      "        \\dfrac{1}{2} \\ln(\\text{det}(\\beta^{-1} I) \\int Q(\\mu) d \\mu\n",
      "        - \\int Q(\\mu) \\left( \\dfrac{\\beta}{2} \\mu^{T}\\mu \\right) d \\mu\\\\\n",
      "= &  - \\text{const} - \\dfrac{\\beta}{2} \\int Q(\\mu) \\mu^{T}\\mu d \\mu \\\\\n",
      "= & - \\dfrac{\\beta}{2} \\int Q(\\mu) \\mu^{T}\\mu d \\mu + \\text{const} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Again we make use of rule 355 of the Matrix Cookbook which states that $ \\mathbb{E}_P[x^{T}Ax] = \\text{Tr}(A \\Sigma) + m^{T} A m $ where $m$ and $\\Sigma$ are the mean and covariance of $P$, so\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = & - \\dfrac{\\beta}{2} \\left( \\text{Tr}(\\Sigma_\\mu) + m_\\mu^{T} m_\\mu \\right) +  \\text{const} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Because $\\mu$ is $d$-dimensional and $\\Sigma_\\mu$ is defined as $(\\beta + N\\langle \\tau \\rangle)^{-1} I$\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = &- \\dfrac{\\beta}{2} \\left( \\dfrac{d}{\\beta + N \\langle \\tau \\rangle } + m_\\mu^{T} m_\\mu \\right) +  \\text{const}\\\\\n",
      "\\end{align}\n",
      "\n",
      "Because $Q(\\tau) = \\Gamma (\\tau | \\widetilde{a}_\\tau, \\widetilde{b}_\\tau)$we end up with\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\mu) \\right] = &\n",
      "    - \\dfrac{\\beta}{2} \n",
      "    \\left( \\dfrac{d}{\\beta + N ({ \\widetilde{a}_\\tau } / { \\widetilde{b}_\\tau} ) }\n",
      "+ m_\\mu^{T} m_\\mu  \\right) +  \\text{const}\\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau|c_\\tau, d_\\tau ) \\right) \\right] \\\\\n",
      "= & \\mathbb{E}\\left[ \\ln \\left( \\dfrac{1}{\\Gamma (c_\\tau)} d_\\tau^{c_\\tau} \\tau^{c_\\tau -1} e^{-c_\\tau \\cdot \\tau} \\right) \\right]\\\\\n",
      "= & \\int Q(\\Theta) \\left( - \\ln(\\Gamma (c_\\tau)) + c_\\tau \\ln(d_\\tau) + (c_\\tau - 1) \\ln (\\tau) - d_\\tau \\cdot \\tau \\right) \\hspace{3mm} d \\Theta \\\\\n",
      "= & - \\int Q(\\tau) \\ln(\\Gamma (c_\\tau)) \\hspace{3mm} d \\tau \n",
      "    + \\int Q(\\tau) c_\\tau \\ln(d_\\tau)  \\hspace{3mm} d \\tau  \n",
      "    + \\int Q(\\tau) (c_\\tau - 1) \\ln (\\tau) \\hspace{3mm} d \\tau  \n",
      "    - \\int Q(\\tau) d_\\tau \\cdot \\tau \\hspace{3mm} d \\tau    \\\\\n",
      "= &   (c_\\tau - 1) \\int Q(\\tau)  \\ln (\\tau) \\hspace{3mm} d \\tau  \n",
      "    -   d_\\tau \\int Q(\\tau)  \\tau \\hspace{3mm} d \\tau   \n",
      "     + \\text{const} \\\\   \n",
      "\\end{align}\n",
      "\n",
      "Again make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;  \n",
      "and \n",
      "of the fact that $\\mathbb{E}_P[x] = \\dfrac{a}{b}$ if $P(x) = \\Gamma(x|a,b)$.\n",
      "Then:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] \n",
      "= &   (c_\\tau - 1) \\left( \\psi(\\widetilde{a}_\\tau) - \\ln(\\widetilde{b}_\\tau )\\right)\n",
      "    -   d_\\tau  \\dfrac{\\widetilde{a}_\\tau }{\\widetilde{b}_\\tau}  \n",
      "     + \\text{const} \\\\   \n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "6"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln Q(X) \\right] = &  \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(x_n| m_x^{(n)}, \\Sigma_x) \\right) \\right] \\\\\n",
      "= & \\sum_{n=1}^N \\mathbb{E}_Q \\left[ \n",
      "                                    \\ln \\left( \\dfrac{1}{(2 \\pi)^{(q/2)}} \\dfrac{1}{\\text{det}(\\Sigma_x)^{(1/2)}} \\right)\n",
      "                                  + \\left( -\\dfrac{1}{2} (x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) \\right) \n",
      "                                  \\right] \\\\\n",
      "= & N \\cdot  \\ln \\left( \\dfrac{1}{(2 \\pi)^{(q/2)}} \\right)\n",
      "        +  N \\cdot \\left( \\dfrac{1}{\\text{det}(\\Sigma_x)^{(1/2)}} \\right)\n",
      "        - \\sum_{n=1}^N \\dfrac{1}{2} \\int Q(x_n)  (x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) \\hspace{3mm} d x_n\\\\\n",
      "= &  - \\sum_{n=1}^N \\dfrac{1}{2} \\int Q(x_n)  (x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) \\hspace{3mm} d x_n + \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "We recognize $(x_n - m_x^{(n)} )^{T}\\Sigma_x^{-1} (x_n - m_x^{(n)}) $ as the square of the Mahalanobis distance $\\vartriangle$.\n",
      "\n",
      "As stated  for example <a href=\"http://stat.ethz.ch/education/semesters/ss2012/ams/slides/v7.2.pdf\"> here</a> , the expectation of the squared Mahalanobis distance pertaining to $x$ and $m$ under a multivariate Gaussian  $\\mathcal{N}(x|m, \\Sigma)$  corresponds to a Chi-square distribution $\\chi^2_{n}$ with the dimensionality of the space of the distribution as the degrees of freedom $n$. This $\\chi^2_n$ distribution has as expected value also $n$. \n",
      "\n",
      "So: $\\mathbb{E}_P[\\vartriangle^2] =  \\text{dimensionality}(x \\sim P(x))$\n",
      "\n",
      "\n",
      "This can also be derived by using rule 357 from the Matrix Cookbook which states that $\\mathbb{E}_P[(x - m')^{T} A (x-m')] = (m-m')^{T} A (m-m') + \\text{Tr}(A \\Sigma)$, if $P(x) = \\mathcal{N}(x | m, \\Sigma)$. We would get $\\text{Tr}(I_q)$.\n",
      "\n",
      "Therefore we end up with\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbb{E}_Q \\left[ \\ln Q(X) \\right] = & -  \\dfrac{N}{2} q + \\text{const}\\\\\n",
      "= & \\text{const}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "7"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q\\left[\\ln Q(W) \\right] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{k=1}^{d}  \\mathcal{N}(\\widetilde{w}_k | m_w^{(k)}, \\Sigma_w) \\right) \\right]\\\\\n",
      "= &  - \\sum_{k=1}^d \\dfrac{1}{2} \\int Q(\\widetilde{w}_k)  (\\widetilde{w}_k - m_w^{(k)} )^{T}\\Sigma_w^{-1} (\\widetilde{w}_k - m_w^{(k)}) \\hspace{3mm} d \\widetilde{w}_k + \\text{const} \\\\\n",
      "= & - \\dfrac{d}{2} q + \\text{const}\\\\\n",
      "= & \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Where we have followed the same reasoning as for $\\mathbb{E}_Q[\\ln Q(X)]$ above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "8"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      " \\mathbb{E}_Q\\left[ \\ln Q(\\alpha) \\right] =&  \\mathbb{E}_Q \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma(\\alpha_i| \\widetilde{a}_\\alpha, \\widetilde{b}_{\\alpha i} ) \\right) \\right] \\\\\n",
      " =& \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left( \n",
      "     \\dfrac{1}{\\Gamma(\\widetilde{a}_\\alpha)} \\cdot\n",
      "     \\widetilde{b}^{\\widetilde{a}_\\alpha}_{\\alpha i} \\cdot\n",
      "     \\alpha_i^{(\\widetilde{a}_\\alpha - 1) } \\cdot\n",
      "     e^{- \\widetilde{b}_{\\alpha i} \\alpha_i} \n",
      "     \\right) \\hspace{3mm} d \\alpha_i \\\\\n",
      "=& \\sum_{i=1}^q \\int Q(\\alpha_i)  \\dfrac{1}{\\Gamma(\\widetilde{a}_\\alpha)} \\hspace{3mm} d \\alpha_i +\n",
      "   \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left(   \\widetilde{b}^{\\widetilde{a}_\\alpha}_{\\alpha i} \\right) \\hspace{3mm} d \\alpha_i  +\n",
      "   \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln \\left(   \\alpha_i^{(\\widetilde{a}_\\alpha - 1) }   \\right) \\hspace{3mm} d \\alpha_i  -\n",
      "    \\sum_{i=1}^q \\int Q(\\alpha_i) \\widetilde{b}_{\\alpha i} \\alpha_i \\hspace{3mm} d \\alpha_i \\\\\n",
      "=& (\\widetilde{a}_\\alpha - 1)  \\sum_{i=1}^q \\int Q(\\alpha_i) \\ln   \\alpha_i    \\hspace{3mm} d \\alpha_i -\n",
      "    \\sum_{i=1}^q \\widetilde{b}_{\\alpha i}  \\int Q(\\alpha_i) \\alpha_i \\hspace{3mm} d \\alpha_i \n",
      "    + \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Again we make use of the fact that $\\mathbb{E}_P[\\ln x ] = \\psi(a) - \\ln(b)$ if $P(x) = \\Gamma(x|a,b)$, where $\\psi(\\cdot)$ is the digamma function;  \n",
      "and \n",
      "of the fact that $\\mathbb{E}_P[x] = \\dfrac{a}{b}$ if $P(x) = \\Gamma(x|a,b)$.\n",
      "Then:\n",
      "\n",
      "\\begin{align}\n",
      " \\mathbb{E}_Q\\left[ \\ln Q(\\alpha) \\right] \n",
      "=&    (\\widetilde{a}_\\alpha - 1)  \\sum_{i=1}^q \\left( \\psi(\\widetilde{a}_\\alpha) - \\ln(\\widetilde{b}_{\\alpha i}) \\right) -\n",
      "      \\sum_{i=1}^q  \\widetilde{b}_{\\alpha i} \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha i}} \\right)\n",
      "      + \\text{const} \\\\\n",
      "=&    (\\widetilde{a}_\\alpha - 1)   \\left( q  \\cdot \\psi(\\widetilde{a}_\\alpha) - \\left(  \\sum_{i=1}^q \\ln(\\widetilde{b}_{\\alpha i}) \\right) \\right)  -\n",
      "        q \\cdot \\widetilde{a}_\\alpha\n",
      "         + \\text{const} \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "9"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_q [ \\ln Q(\\mu)] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\mathcal{N}(\\mu | m_\\mu, \\Sigma_\\mu) \\right) \\right] \\\\\n",
      "=& \\int Q(\\mu) \\left[ \\left( -\\dfrac{1}{2} (\\mu - m_\\mu)^{T} \\Sigma_\\mu (\\mu - m_\\mu) \\right) -\n",
      "                    \\left( \\dfrac{d}{2} \\ln (2 \\pi) \\right) -\n",
      "                    \\left( \\dfrac{1}{2} \\ln (\\text{det}(\\Sigma_\\mu)) \\right)\n",
      "                \\right] d \\mu\\\\\n",
      "=& - \\dfrac{1}{2} \\int Q(\\mu) (\\mu - m_\\mu)^{T} \\Sigma_\\mu (\\mu - m_\\mu) d \\mu + \\text{const} \\\\\n",
      "=& - \\dfrac{d}{2} + \\text{const} \\\\\n",
      "=& \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Where we have followed the same reasoning as for $\\mathbb{E}_Q[\\ln Q(X)]$ above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "10"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbb{E}_Q[\\ln Q(\\tau)] = & \\mathbb{E}_Q \\left[ \\ln \\left( \\Gamma(\\tau | \\widetilde{a}_\\tau, \\widetilde{b}_\\tau ) \\right) \\right]\\\\\n",
      "= & \\left(\\psi(\\widetilde{a}_\\tau) - \\ln( \\widetilde{b}_\\tau) \\right) - \\widetilde{b}_\\tau \\left( \\dfrac{ \\widetilde{a}_\\tau}{\\widetilde{b}_\\tau} \\right) + \\text{const}\\\\\n",
      "= & \\left(\\psi(\\widetilde{a}_\\tau) - \\ln( \\widetilde{b}_\\tau) \\right) - \\widetilde{a}_\\tau + \\text{const}\n",
      "\\end{align}\n",
      "\n",
      "Where we have followed the same reasoning as for e.g.  $\\mathbb{E}_Q \\left[ \\ln P(\\tau) \\right] $ above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The whole lower bound $\\mathcal{L}(Q)$ can now be expressed as:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& -\\dfrac{N}{2} \\text{Tr}(\\Sigma_x) -\\dfrac{1}{2}  \\sum_{n=1}^N {m_x^{(n)}}^{T}  {m_x^{(n)}} \\\\\n",
      "               & + \\dfrac{d}{4} \\ln(2 \\pi) \\left( q \\cdot \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q \\ln (\\widetilde{b}_{\\alpha i}) \n",
      "                               - \\sum_{i=1}^{q} \\left( \\dfrac{d (\\widetilde{a}_\\alpha^2 + \\widetilde{a}_\\alpha) } {\\widetilde{b}_{\\alpha i}^2 } \\right) \\right) \\\\\n",
      "              & +    (a_\\alpha - 1) \\left(  q \\cdot  \\psi(\\widetilde{a}_\\alpha) - \\sum_{i=1}^q  \\left( \\ln(\\widetilde{b}_{\\alpha, i})\\right) \\right)\n",
      "                    - b_\\alpha \\sum_{i=1}^q  \\left( \\dfrac{\\widetilde{a}_\\alpha}{\\widetilde{b}_{\\alpha, i}} \\right) \\\\\n",
      "              & - \\dfrac{\\beta}{2}     \\left( \\dfrac{d}{\\beta + N ({ \\widetilde{a}_\\tau } / { \\widetilde{b}_\\tau} ) } + m_\\mu^{T} m_\\mu  \\right)\\\\\n",
      "              & + (c_\\tau - 1) \\left( \\psi(\\widetilde{a}_\\tau) - \\ln(\\widetilde{b}_\\tau )\\right)     -   d_\\tau  \\dfrac{\\widetilde{a}_\\tau }{\\widetilde{b}_\\tau} \\\\\n",
      "              & - (\\widetilde{a}_\\alpha - 1)   \\left( q  \\cdot \\psi(\\widetilde{a}_\\alpha) - \\left(  \\sum_{i=1}^q \\ln(\\widetilde{b}_{\\alpha i}) \\right) \\right)  +\n",
      "        q \\cdot \\widetilde{a}_\\alpha \\\\\n",
      "              & - \\left(\\psi(\\widetilde{a}_\\tau) - \\ln( \\widetilde{b}_\\tau) \\right) + \\widetilde{a}_\\tau\\\\\n",
      "              & + \\text{const}\n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 5. Optimize variational parameters (50 points)\n",
      "Implement the update equations for the Q-distributions, in the __update_XXX methods. Each update function should re-estimate the variational parameters of the Q-distribution corresponding to one group of variables (i.e. either $Z$, $\\mu$, $W$, $\\alpha$ or $\\tau$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6. Learning algorithm (10 points)\n",
      "Implement the learning algorithm described in [Bishop99], i.e. iteratively optimize each of the Q-distributions holding the others fixed.\n",
      "\n",
      "What would be a good way to track convergence of the algorithm? Implement your suggestion.\n",
      "\n",
      "Test the algorithm on some test data drawn from a Gaussian with different variances in orthogonal directions. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 7. PCA Representation of MNIST (10 points)\n",
      "\n",
      "Download the MNIST dataset from here http://deeplearning.net/tutorial/gettingstarted.html (the page contains python code for loading the data). Run your algorithm on (part of) this dataset, and visualize the results.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle, gzip, numpy\n",
      "\n",
      "# Load the dataset\n",
      "f = gzip.open('../mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print shape(valid_set)\n",
      "print shape(train_set)\n",
      "print shape(test_set)\n",
      "\n",
      "print shape( test_set[0][1]) # image\n",
      "print shape(test_set[1][1]) # class label image\n",
      "\n",
      "#An image is represented as numpy 1-dimensional array of 784 (28 x 28) float values between 0 and 1 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(2, 10000)\n",
        "(2, 50000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 10000)\n",
        "(784,)\n",
        "()\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "im = test_set[0][1]\n",
      "im2 = im.reshape((28, 28))\n",
      "\n",
      "matshow(im2,cmap=cm.gray)\n",
      "\n",
      "W = np.random.randn(28*28,700)\n",
      "\n",
      "#W[:,3]=0\n",
      "#W[:,5]=0\n",
      "\n",
      "imdist = np.dot( W.T  , im)\n",
      "\n",
      "print shape(imdist)\n",
      "print type(imdist)\n",
      "\n",
      "Wpi = np.linalg.pinv(W)\n",
      "\n",
      "print shape(W)\n",
      "print shape(Wpi)\n",
      "\n",
      "imrecov = np.dot(Wpi.T, imdist)\n",
      "\n",
      "print shape(imrecov)\n",
      "\n",
      "imrecov2 = imrecov.reshape((28, 28))\n",
      "matshow(imrecov2,cmap=cm.gray)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(700,)\n",
        "<type 'numpy.ndarray'>\n",
        "(784, 700)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(700, 784)\n",
        "(784,)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "<matplotlib.image.AxesImage at 0x42c02d0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwAMOIfCAQ/4Xn2UOhFWqHz0AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x3b3ed90>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD5CAYAAAADZljUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDMKCJzLI/wB4rmVnbuD1tgAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x2f070d0>"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}